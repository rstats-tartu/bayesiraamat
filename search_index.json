[
["index.html", "Bayesi statistika kasutades R keelt Saateks", " Bayesi statistika kasutades R keelt Taavi Päll Ülo Maiväli Saateks See õpik soovib anda praktilisi andmeanalüüsi oskusi töötamiseks reaalsete andmetega. See puudutab laias laastus kolme teemat: Kuidas summeerida andmeid: keskmise, varieeruvuse ja kovarieeruvuse näitajad. Kuidas graafiliste meetodite abil kontrollida andmete kvaliteeti ja püstitada uusi hüpoteese. Kuidas teha andmete põhjal järeldusi protsessi kohta, mis neid andmeid genereerib, ühtlasi kirjeldades adekvaatselt neid järeldusi ümbritsevat ebakindlust. Kuna me püüame anda eeskätt praktilisi oskusi andmeanalüüsiks, mitte matemaatilist ega muidu teoreetilist haridust, siis keskendume moodsatele bayesi meetoditele. Need on küll arvuti jaoks töömahukamad kui klassikalised statistilisel olulisusel põhinevad testid, aga inimese jaoks kergemini õpitavad ning tõlgendatavad. Klassikalist ja bayesi statistikat võrdleme lisas 1. Bayesiaanliku lähenemise hind on, et kasutaja peab omandama vähemalt algtasemel R keele, mis võimaldab käsurealt anmdeid manipuleerida. R-i õppimine nõuab kahtlemata lisapingutust, aga me usume, et see tasub ära igaühele, kes töötab vähemalt keskmise suurusega andmekogudega. Me teeme sissejuhatuse R-i peatükkides …., keskendudes R-i tidyverse ökosüsteemile, mis on optimeeritud olema kergesti kasutatav ja õpitav inimestele, kelle põhitöö ei ole seotud koodi kirjutamisega. Pingutus R õppimiseks tasub teile mitmel erineval viisil. R võimaldab palju kiiremini andmetabeleid analüüsiks sobivasse vormi ajada kui spreadsheet programmid. R-i graafikasüsteem, eriti ggplot2, on võimas ning paindlik tööriist väga erinevate graafikute koostamiseks. R-s jooksevad praktiliselt kõik statistilised testid, mida inimmõistus on loonud – R on levinuim statistikaprogramm maailmas, mis on eriti hästi sobiv statistiliseks modelleerimiseks. See tähendab ka, et üle maailma on suur seltskond inimesi, kes R-i arendab ja on nõus vastama ka teie küsimustele. Lisaboonusena on juba kord salvestatud R-i koodi taaskasutades palju lihtsam oma analüüsi korrata ja vastavalt vajadustele muuta, kui spreadsheet programmide puhul. Õpiku kasutamise eeldused: - Arvuti. - Matemaatikaoskused, mis hõlmavad liitmist, lahutamist, korrutamist, jagamist, logaritmimist ja astendamist. - Kõrgemat matemaatikat me ei vaja. Õpiku struktuur: xxx "],
["sissejuhatus-maailm-teooria-ja-mudel.html", "1 Sissejuhatus: maailm, teooria ja mudel Suur ja väike maailm Mudeli väike maailm", " 1 Sissejuhatus: maailm, teooria ja mudel Suur ja väike maailm Kuna maailmas on kõik kõigega seotud, on seda raske otse uurida. Teadus töötab tänu sellele, et teadlased lõikavad reaalsuse väikesteks tükkideks, kasutades tordilabidana teaduslike hüpoteese, ning uurivad seda tükikaupa lootuses, et kui kõik tükid on korralikult läbi nätsutatud, saab sellest taas tordi kokku panna. Tüüpiline bioloogiline hüpotees pakub välja tavakeelse (mitte matemaatilise) seletuse mõnele piiritletud loodusnähtusele. Näiteks antibiootikume uuritakse keemilise sideme tasemel kasutades orgaanilise keemia meetodeid. Antibiootikumide molekulaarseid märklaudu uuritakse molekulaarbioloogiliste meetoditega, nende toimet uuritakse rakubioloogia ja füsioloogia meetoditega, aga kaasajal on väga olulised ka ökoloogilised, evolutsioonilised, meditsiinilised, põllumajanduslikud, majanduslikud ja psühholoogilised aspektid. Kõigil neil tasanditel on loodud palju hüpoteese, millest kokku moodustub meie teadmine antibiootikumide kohta. Neid väga erinevaid asju, mida me kutsume hüpoteesideks, ühendab see, et neist igaüht võib võrrelda empiiriliste andmetega. Samuti, enamust neist saab omakorda jagada osadeks, mida saab omakorda osaliselt kirjeldada matemaatiliste formalismide ehk mudelite abil. Ja neid mudeleid saab võrrelda andmetega. Kuigi erinevate tasemete hüpoteesid on tavakeeles üksteisest väga erinevad, on neid kirjeldavad mudelid sageli matemaatiliselt sarnased. Kui mudel on teooria lihtsustus, siis teooria on maailma lihtsustus. Mudeliteks nimetatakse bioloogias väga erinevaid asju: skeeme, diagramme, füüsikalisi mudeleid (näit Watsoni ja Cricki poolt kasutatud nukleotiidimudelid), mudelorganisme, katsesüsteeme, matemaatilisi mudeleid jms. Üldiselt, mudelid asendavad selle, mida uuritakse millegagi, mida on lihtsam kui päris maailma mõista, manipuleerida või uurida. Meie räägime edaspidi ainult matemaatilisest mudelist ja eriti selle erijuhust, statistilisest e stohhastilisest mudelist. Mis juhtub, kui teie hüpotees on andmetega kooskõlas? Kas see tähendab, et see hüpotees vastab tõele? Või, et see on tõenäoliselt tõene? Kahjuks on vastus mõlemale küsimusele eitav. Põhjuseks on asjaolu, et enamasti leiab iga nähtuse seletamiseks rohkem kui ühe alternatiivse teadusliku hüpoteesi ning rohkem kui üks üksteist välistav hüpotees võib olla olemasolevate andmetega võrdses kooskõlas. Asja teeb veelgi hullemaks, et teoreetiliselt on võimalik sõnastada lõpmata palju erinevaid teooriaid, mis kõik pakuvad alternatiivseid ja üksteist välistavaid seletusi samale nähtusele. Kuna hüpoteese on lõpmatu hulk, aga andmeid on alalti lõplik hulk, siis saab igas teaduslikus faktis kahelda. Kunagi ei või kindel olla, et parimad teooriad ei ole täiesti tähelepanuta jäänud ning, et meie poolt kogutud vähesed andmed kajastavad hästi kõiki võimalikke andmeid. Ca. 1910 mõtlesid Bertrand Russell ja G.E. Moore välja tõe vastavusteooria, mille kohaselt tõest lausungit eristab väärast vastavus füüsikalisele maailmale. Seega on tõesed ainult need laused, mis vastavad asjadele. Ehkki keegi ei oska siiani öelda, mida vastavus selles kontekstis tähendab või kuidas seda saavutada, on vastavusteooria senini kõige populaarsem tõeteooria filosoofide hulgas (mis on kõnekas alternatiivide kohta). Samamoodi, kui lausete vastavusest maailmaga, võime rääkida ka võrrandite (ehk mudelite) vastavusest lausetega. Vastavusest lausetaga sellepärast, et mudelid on loodud kirjeldama teaduslikke teooriaid, mitte otse maailma. Seega ei pea me muretsema mudelite tõeväärtuse pärast. Võib isegi väita, et mudeli tõeväärtusest rääkimine on kohatu. Näide: politoloogia Meil on hüpotees (H1), mille kohaselt demokraatlikus süsteemis käituvad valijad ratsionaalselt ehk lähtuvalt endi huvidest (Achen and Bartels 2016). Alternatiiv (H2) ütleb, et valijad ei vali poliitikuid lähtuvalt oma tegelikest huvidest. Kuna H1 on liiga lai, et seda otse andmetega võrrelda, tuletame sellest kitsama alamhüpoteesi (H1.1), mille kohaselt valijad eelistavad tagasi valida kandidaate, kes on ennast tõestanud sellega, et saavad hakkama majanduse edendamisega. Seega, poliitikud, kes on võimekad majanduse vallas, valitakse tagasi suurema tõenäosusega kui need, kes seda ei ole. Sellest hüpoteesist tuletati kaks andemete vastu testitavat järelmit: H1.1.1 – majandusel läheb keskeltläbi paremini juba tagasi valitud poliitikute all kui esimest korda valitud poliitkute all, kelle ridu ei ole veel elektoraadi poolt harvendatud ja H1.1.2 – majandusnäitajate varieeruvus on esimesel juhul väiksem, sest kehvemad poliitikud on juba valimist eemaldatud. Esimese järelmi testimiseks kasutati statistilise mudelina (m1) aritmeetilist keskmist koos standardveaga ja teise järelmi jaoks (m2) standardhälvet. Tulemused olid paraku vastupidised H1.1.1 ja H1.1.2 poolt ennustatuga, millest autorid tegid järelduse, et olemasolevad andmed ei toeta hüpoteesi H1.1 (andmete vähesuse tõttu nad ei arvanud, et nad oleksid H1.1-e ümber lükanud). Seega, andmed fititi mudelitesse m1 ja m2, nende fittide põhjal tehti järeldused H1.1.1 ja H1.1.2 kohta (et m1 ja H1.1.1 ning m2 ja H1.1.2 vahel puudub kooskõla), mille põhjal omakorda tehti järeldus H1.1 kohta (et H1.1-e ei õnnestunud kinnitada), mille põhjal üksi ei tehtud formaalset järeldust H1 kohta. H1 vs. H2 kohta tehakse järeldus alles raamatu lõpus, lähtudes H1.1, H1.2, …, H1.n kohta tehtud järeldustest. Näide: populatsioonigeneetika Populatsioonigeneetikas on evolutsioon defineeritud kui alleelide sageduste muutumine põlvkonnast põlvkonda. Kõigepealt defineeriti tingimused, milliste kehtimisel alleelide sagedus EI muutu. Need on juhuslik sigimine populatsioonis, lõpmata suur populatsioon, mis koosneb diploidsetest organismidest, kellel on 1 geneetiline lookus ja 2 alleeli. See on Hardy-Weinbergi printsiip, millel põhineb enamus klassikalisest populatsioonigeneetikast ja mida kirjeldab võrrand \\[p^2 + 2pq + q^2 = 1\\] kus \\(p^2\\), \\(2pq\\) ja \\(q^2\\) on genotüüpide \\(AA\\), \\(Aa\\) ja \\(aa\\) sagedused sugurakkudes ning \\(p\\) ja \\(q\\) on alleelide \\(A\\) ja \\(a\\) sagedused (ning \\(p + q = 1\\)). Populatsioonis, mis on Hardy-Weinbergi tasakaalus, on \\(p\\) ja \\(q\\) põlvkondade vältel muutumatud. Selleks, et tasakaalu lõhkuda, toome mudelisse lisaparameetri \\(w\\), mis iseloomustab valikusurvet ehk kohasust (fitnessi). Kohasus iseloomustab looduliku valiku poolt tingitud genotüüpide sageduste muutust populatsioonis. Nüüd saame deterministliku mudeli (deterministliku, sest mudeli parameetritele väärtused omistades ja mudeli läbi arvutades saame vastuseks vaid ühe arvu): \\[p^2wAA + 2pqwAa + q^2waa = w_{mean}\\] kus \\(w_{mean}\\) on populatsiooni keskmine kohasus, \\(wAA\\) on genotüübi \\(AA\\) kohasus jne. Kui me teame parameetrite \\(p, q, wAA, wAa\\) ja \\(waa\\) väärtusi, saame hõlpsalt arvutada populatsiooni kohasuse. Vaadates maailma mudeli pilgu läbi, juhul kui looduses mõõdetud genotüüpide sageduse muutus erineb mudelist arvutatud \\(w_{mean}\\)-ist, siis on meil tegemist geneetilise triiviga. Geneetiline triiv on genotüübisageduste juhuslik muutus populatsioonis, mis on seda suurem, mida väiksem on populatsioon ja mida väiksem on valikusurve populatsioonile. Seega oleks nagu võimalik geneetilise triivi olemasolu tuvastada alati, kui empiiriline genotüübisageduste muutuse kiirus erineb mudeli punktennustusest \\(w_{mean}\\). Selle deterministliku mudeli järgi on valik ja triiv teineteist välistavad: kui empiiriline kohasus = \\(w_{mean}\\), siis valik; muidu triiv. Samas, kui me eeldame, et populatsiooni suurus ei ole lõpmata suur, tuleb mudelisse sisse juhuslik valimiviga. Mida väiksem on populatsioon, seda suurema tõenäosusega ei anna juhuslik paljunemine ka ilma valikusurveta populatsioonis järgmist põlvkonda, mille genotüübisagedused vastaksid eelmise põlvkonna genotüübisagedustele (ptk xxx simuleerime me juhuslikku valimiviga normaaljaotuse mudelist). Seega muutub meie deterministlik mudel stohhastiliseks mudeliks, mille väljund ei ole enam punktväärtus \\(w_{mean}\\)-le vaid rida tõenäosusi erinevatele \\(w_{mean}\\)-i väärtustele (sellise mudeli kuju vt ptk xxx). Selle mudeli järgi ei ole valik ja triiv enam erinevat tüüpi protsessid, vaid ühe kontiinumi kaks poolust; kontiinumi, mis sõltub populatsiooni suurusest ja valikusurve tugevusest. Kuna puhas looduslik valik saab mudeli järgi toimuda ainult lõpmata suures populatsioonis, milliseid looduses ei leidu, siis on alleeli \\(a\\) sageduse muutus teadlase poolt uuritavas looduslikus populatsioonis x ühtaegu nii loodusliku valiku kui geenitriivi tagajärg. Mis juhtub, kui me ei tee mudeli struktuurist otse järeldusi maailma kohta? Nüüd alustame me eeldusest, et looduslik valik on looduses reaalselt toimuv protsess. Näiteks Darwin nägi valikut loodusliku põhjusliku protsessina, mis on samas stohhastiline (mitte kõik kõrgema kohasusega organismid ei anna järglasi). Selle vaate kohaselt on loodusliku valiku tagajärjeks kallutatud valim genotüüpidest, mille avaldumise poolt põhjustatud erinevused organismides viisid nende erinevale paljunemisedukusele. Seega on valik ja triiv erinevat tüüpi looduslikud protsessid, mitte mudeli väljundid. Niisiis teeme rangelt vahet valikul ja triivil nende põhjuste järgi. Kui tõuseb kasulike genotüüpidega organismide osakaal, siis on tegemist loodusliku valiku poolt tingitud evolutsiooniga. Kui aga genotüüpide sageduste muutumine ei ole põhjustatud indiviidide füüsilistest erinevustest, siis on tegu geneetilise triivi poolt tingitud evolutsiooniga. Nõnda saame evolutsiooniteooriast lähtudes hoopis teistsuguse vaate bioloogiale, kui mudeleid otse tõlgendades. Muidugi ei tähenda see, et me ei vaja mudeleid. Vajame küll, aga me peame neid ettevaatlikult tõlgendama, pidades silmas oma teooriate sisu. Andemetega fititud mudelit tõlgendame teooria kaudu ja seda ei tohiks kunagi teha otse mudelist päris maailmale. Mudeli väike maailm Ülalmainitud teadusliku meetodi puudused tingivad, et meie huvides on oma teaduslikke probleeme veel ühe taseme võrra lihtsustada, taandades need statistilisteks probleemideks. Selleks tuletame tavakeelsest teaduslikust teooriast täpselt formuleeritud matemaatilise mudeli ning seejärel asume uurima oma mudelit lootuses, et mudeli kooskõla andmetega ütleb meile midagi teadusliku hüpoteesi kohta. Enamasti töötab selline lähenemine siis, kui mudeli ehitamisel arvestati võimaliku andmeid genereeriva mehhanismiga – ehk, kui mudeli matemaatiline struktuur koostati teaduslikku hüpoteesi silmas pidades. Mudelid, mis ehitatakse silmas pidades puhtalt matemaatilist sobivust andmetega, ei kipu omama teaduslikku seletusjõudu, kuigi neil võib olla väga hea ennustusjõud. Meil on kaks hüpoteesi, A ja B. Juhul kui A on tõene ja B on väär, kas on võimalik, et B on tõele lähemal kui A? Kui A ja B on teineteist välistavad punkthüpoteesid parameetri väärtuse kohta, siis on vastus eitav. Aga mis juhtub, kui A ja B on statistilised mudelid? Näiteks, kui tõde on, et eesti meeste keskmine pikkus on 178.3 cm ja A ütleb, et keskmine pikkus jääb kuhugi 150 cm ja 220 cm vahele ning B ütleb, et see jääb kuhugi 179 cm ja 182 cm vahele, siis on B “tõele lähemal” selles mõttes, et meil on temast teaduslikus mõttes rohkem kasu. Siit on näha oluline erinevus teadusliku hüpoteesi ja statistilise mudeli vahel: hüpotees on orienteeritud tõele, samal ajal kui mudel on orienteeritud kasule. Mudeli maailm erineb päris maailmast selle poolest, et mudeli maailmas on kõik sündmused, mis põhimõtteliselt võivad juhtuda, juba ette teada ja üles loendatud (seda sündmuste kogu kutsutakse parameetriruumiks). Tehniliselt on mudeli maailmas üllatused võimatud. Lisaks, tõenäosusteooriat, ja eriti Bayesi teoreemi, kasutades on meil garantii, et me suudame mudelis leiduva informatsiooniga ümber käia parimal võimalikul viisil. Kõik see rõõm jääb siiski mudeli piiridesse. Mudeli eeliseks teooria ees on, et hästi konstrueeritud mudel on lihtsamini mõistetav — erinevalt vähegi keerulisemast teaduslikust hüpoteesist on mudeli eeldused ja ennustused läbinähtavad ja täpselt formuleeritavad. Mudeli puuduseks on aga, et erinevalt teooriast ei ole mingit võimalust, et mudel vastaks tegelikkusele. Seda sellepärast, et mudel on taotluslikult lihtsustav (erandiks on puhtalt ennustuslikud mudelid, mis on aga enamasti läbinähtamatu struktuuriga). Mudel on kas kasulik või kasutu; teooria on kas tõene või väär. Mudeli ja maailma vahel võib olla kaudne peegeldus, aga mitte kunagi otsene side. Seega, ükski number, mis arvutatakse mudeli raames, ei kandu sama numbrina üle teaduslikku ega päris maailma. Ja kogu statistika (ka mitteparameetriline) toimub mudeli väikses maailmas. Arvud, mida statistika teile pakub, elavad mudeli maailmas; samas kui teie teaduslik huvi on suunatud päris maailmale. Näiteks 95% usaldusintervall ei tähenda, et te peaksite olema 95% kindel, et tõde asub selles intervallis – sageli ei tohiks te seda nii julgelt tõlgendada isegi kitsas mudeli maailmas. Näide: Aristoteles, Ptolemaios ja Kopernikus Aristoteles (384–322 BC) lõi teooria maailma toimimise kohta, mis domineeris haritud eurooplase maailmapilti enam kui 1200 aasta vältel. Tema ühendteooria põhines maailmapildil, mis oli üldtunnustatud juba sajandeid enne Aristotelest ja järgneva 1500 aasta jooksul kahtlesid selles vähesed mõistlikud inimesed. Selle kohaselt asub universumi keskpunktis statsionaarne maakera ning kõik, mida siin leida võib, on tehtud neljast elemendist: maa, vesi, õhk ja tuli. Samas, kogu maailmaruum alates kuu sfäärist on tehtud viiendast elemendist (eeter), mida aga ei leidu maal (nagu nelja elementi ei leidu kuu peal ja sealt edasi). Taevakehad (kuu, päike, planeedid ja kinnistähed) tiirlevad ümber maa kontsentrilistes sfäärides, mille vahel pole vaba ruumi. Seega on kogu liikumine eetri sfäärides ühtlane ja ringikujuline ja see liikumine põhjustab pika põhjus-tagajärg ahela kaudu kõiki liikumisi, mida maapeal kohtame. Kaasa arvatud sündimine, elukäik ja surm. Kõik, mis maapeal huvitavat, ehk kogu liikumine, on algselt põhjustatud esimese liikumise poolt, mille käivitab kõige välimises sfääris paiknev meie jaoks mõistetamatu intellektiga “olend”. Joonis 1.1: Keskaegne aristotellik maailm. Aristotelese suur teooria ühendab kogu maailmapildi alates meie mõistes keemiast ja kosmoloogiast kuni bioloogia, maateaduse ja isegi geograafiani. Sellist ühendteooriat on erakordselt raske ümber lükata, sest seal on kõik kõigega seotud. Aristarchus (c. 310 – c. 230 BC) proovis seda siiski, väites, et tegelikult tiirleb maakera ümber statsionaarse päikese. Ta uskus ka, et kinnistähed on teised päikesed, et universum on palju suurem kui arvati (ehkki kaasaegne seisukoht oli, et universumi mastaabis ei ole maakera suurem kui liivatera) ning, et maakera pöörleb ümber oma telje. Paraku ei suutnud Aristarchuse geotsentriline teooria toetajaid leida, kuna see ei pidanud vastu vaatluslikule testile. Geotsentrilisest teooriast tuleneb nimelt loogilise paratametusena, et tähtedel esineb maalt vaadates parallaks. See tähendab, et kui maakera koos astronoomiga teeb poolringi ümber päikese, siis kinnistähe näiv asukoht taevavõlvil muutub, sest astronoom vaatleb teda teise nurga alt. Pange oma nimetissõrm näost u 10 cm kaugusele, sulgege parem silm, seejärel avage see ning sulgege vasak silm ja te näete oma sõrme parallaksi selle näiva asukoha muutusena. Mõõtmised ei näidanud aga parallaksi olemasolu (sest maa trajektoori diameeter on palju lühem maa kaugusest tähtedest). Parallaksi suudeti esimest korda mõõta alles 1838, siis kui juba iga koolijüts uskus, et maakera tiirleb ümber päikese! Ühte Aristotelese kosmoloogia olulist puudust nähti siiski kohe. Nimelt ei suuda Aristoteles seletada, miks osad planeedid teavavõlvil vahest suunda muudavad ja mõnda aega lausa vastupidises suunas liiguvad (retrogressioon). Kuna astronoomiat kasutasid põhiliselt astroloogid, siis põõrati planeetide liikumisele suurt tähelepanu. Lahenduseks ei olnud aga mitte suure teooria ümbertegemine või ümberlükkamine, vaid uue teaduse nõudmine, mis “päästaks fenomenid”. Siin tuli appi Ptolemaios (c. AD 100 – c. 170), kes lõi matemaatilise mudeli, kus planeedid mitte lihtsalt ei liigu ringtrajektoori mõõda, vaid samal ajal teevad ka väiksemaid ringe ümber esimese suure ringjoone. Neid väiksemaid ringe kutsutakse epitsükliteks. See mudel suutis planeetide liikumist taevavõlvil piisavalt hästi ennustada, et astroloogide seltskond sellega rahule jäi. Ptolemaiosel ja tema järgijatel oli tegelikult mitu erinevat mudelit. Osad neist ei sisaldanud epitsükleid ja maakera ei asunud tema mudelites universumi keskel, vaid oli sellest punktist eemale nihutatud — nii et päike ei teinud ringe ümber maakera vaid ümber tühja punkti. Kuna leidus epitsüklitega mudel ja ilma epitsükliteta mudel, mis andsid identseid ennustusi, on selge, et Aristotelese teooria ja fenomenide päästmise mudelid on põhimõtteliselt erinevad asjad. Samal ajal, kui Aritoteles seletas maailma põhiolemust põhjuslike seoste jadana (mitte matemaatiliselt), kirjeldas/ennustas Ptolemaios sellesama maailma käitumist matemaatiliste (mitte põhjuslike) struktuuride abil. Joonis 1.2: Ilma epitsükliteta ptolemailine mudel. Nii tekkis olukord, kus maailma mõistmiseks kasutati Aristotelese ühendteooriat, aga selle kirjeldamiseks ja tuleviku ennustamiseks hoopis ptolemailisi mudeleid, mida keegi päriselt tõeks ei pidanud ja mida hinnati selle järgi, kui hästi need “päästsid fenomene”. See toob meid Kopernikuse (1473 – 1543) juurde, kes teadusajaloolaste arvates vallandas 17. sajandi teadusliku revolutsiooni, avaldades raamatu, kus ta asetab päikese universumi keskele ja paneb maa selle ümber ringtrajektooril tiirlema. Kas Kopernikus tõrjus sellega kõrvale Aristotelese, Ptolemaiose või mõlemad? Tubdub, et Kopernikus soovis kolmandat, suutis esimest, ning et tolleaegsete lugejate arvates üritas ta teha teist — ehk välja pakkuda alternatiivi ptolemailistele mudelitele, mis selleks ajaks olid muutunud väga keerukaks (aga ka samavõrra ennustustäpseks). Kuna Kopernikuse raamat läks trükki ajal, mil selle autor oli juba oma surivoodil, kirjutas sellele eessõna üks tema vaimulikust sõber, kes püüdis oodatavat kiriklikku pahameelt leevendada vihjates, et päikese keskele viimine on vaid mudeldamise trikk, millest ei tasu järeldada, et maakera ka tegelikult ümber päikese tiirleb (piibel räägib, kuidas jumal peatas taevavõlvil päikese, mitte maa). Ja kuna eessõna oli anonüümne, eeldasid lugejad muidugi, et selle kirjutas autor. Lisaks, kuigi Kopernikus tõstis päikese keskele, jäi ta planeetide ringikujuliste trajektooride juurde, mis tähendas, et selleks, et tema teooria fenomenide päästmisel hätta ei jääks, oli ta sunnitud maad ja planeete liigutama ümber päikese mõõda epitsükleid. Kokkuvõttes oli Kopernikuse mudel umbes sama keeruline kui Ptolemailikud mudelid ja selle abil tehtud ennustused planeetide liikumise kohta olid väiksema täpsusega. Seega, ennustava mudelina ei olnud sel suuri eeliseid. Joonis 1.3: Ptolemaiose ja Kopernikuse mudelid on üllatavalt sarnased. Kopernikuse mudel suutis siiski ennustada mõningaid nähtusi (planeetide näiv heledus jõuab maksimumi nende lähimas asukohas maale), mida Ptolemaiose mudel ei ennustanud. See ei tähenda, et need fenomenid oleksid olnud vastuolus Ptolemaiose mudeliga. Lihtsalt, nende Ptolemaiose mudelisse sobitamiseks oli vaja osad mudeli parameetrid fikseerida nii-öelda suvalistele väärtustele. Seega Koperniku mudel töötas sellisel kujul, nagu see oli, samas kui Ptolemaiose mudel vajas ad hoc tuunimimst. Kui vaadata Koperniku produkti teooriana, mitte mudelina, siis oli sellel küll selgeid eeliseid Aristotelese maailmateooria ees. Juba ammu oli nähtud komeete üle taevavõlvi lendamas (mis Aristotelese järgi asusid kinnistähtede muutumatus sfääris), nagu ka supernoova tekkimist ja kadu, ning enam ei olnud kaugel aeg, mil Galileo joonistas oma teleskoobist kraatreid kuu pinnal, näidates, et kuu ei saanud koosneda täiuslikust viiendast elemendist ja et sellel toimusid ilmselt sarnased füüsikalised protsessid kui maal. On usutav, et kui Kopernikus oleks jõudnud oma raamatule ise essõna kirjutada, oleks tema teooria vastuvõtt olnud palju kiirem (ja valulisem). "],
["kusimused-mida-statistika-kusib.html", "2 Küsimused, mida statistika küsib Jäta meelde", " 2 Küsimused, mida statistika küsib Statistika abil saab vastuseid järgmisetele küsimustele: kuidas näevad välja teie andmed ehk milline on just teie andmete jaotus, keskväärtus, varieeruvus ja koos-varieeruvus? Näiteks, mõõdetud pikkuste ja kaalude koos-varieeruvust saab mõõta korrelatsioonikordaja abil. mida me peaksime teie valimi andmete põhjal uskuma populatsiooni parameetri tegeliku väärtuse kohta? Näiteks, kui meie andmete põhjal arvutatud keskmine pikkus on 178 cm, siis kui palju on meil põhjust arvata, et tegelik populatsiooni keskmine pikkus &gt; 185 cm? mida ütleb statistilise mudeli struktuur teadusliku hüpoteesi kohta? Näiteks, kui meie poolt mõõdetud pikkuste ja kaalude koos-varieeruvust saab hästi kirjeldada kindlat tüüpi lineaarse regressioonimudeliga, siis on meil ehk tõendusmaterjali, et pikkus ja kaal on omavahel sellisel viisil seotud ja eelistatud peaks olema teaduslik teooria, mis just sellise seose tekkimisele bioloogilise mehhanismi annab. mida ennustab mudel tuleviku kohta? Näiteks, meie lineaarne pikkuse-kaalu mudel suudab ennustada tulevikus kogutavaid pikkuse andmeid. Aga kui hästi? statistika peamine ülesanne on kvantifitseerida kõhedust, mida peaksime tundma vastates eeltoodud küsimustele. Statistika ei vasta otse teaduslikele küsimustele ega küsimustele päris maailma kohta. Statistilised vastused jäävad alati kasutatud andmete ja mudelite piiridesse. Sellega seoses peaksime eelistama hästi kogutud rikkalikke andmeid ja paindlikke mudeleid. Siis on lootust, et hüpe mudeli koefitsientidest päris maailma kirjeldamisse tuleb üle kitsama kuristiku. Bayesil on siin eelis, sest osav statistik suudab koostöös teadlastega priori mudelisse küllalt palju kasulikku infot koguda. Teisalt, mida paindlikum on meetod, seda vähem automaatne on selle mõistlik kasutamine. Jäta meelde Statistika jagatakse kolme ossa: kirjeldav (summary), uuriv (exploratory) ja järeldav (inferential). Kirjeldav statistika kirjeldab teie andmeid summaarsete statistikute abil. Uuriv statistika püstitab valimi põhjal uusi teaduslikke hüpoteese, kasutades selleks põhiliselt graafilisi meetodeid. Järeldav statistika kasutab formaalseid mudeleid, et kontrollida uuriva statistika abil püsitatud hüpoteese. Järeldav statistika teeb valimi põhjal järeldusi statistilise populatsiooni kohta, millest see valim pärineb. Statistika põhjal tehtud järeldused on alati ebakindlad; ka siis kui need esitatakse punkthinnanguna parameetriväärtusele. Nii punkthinnangud kui intervall-hinnangud on lihtsustused: tegelik ebakindluse määr on n-dimensionaalne tõenäosuspilv, kus n on mudeli parameetrite arv. Statistika põhiline ülesanne on kvantifitseerida ebakindlust, mis ümbritseb järeldava statistika abil saadud hinnanguid. Selle ebakindluse numbriline mõõt on tõenäosus, mis jääb 0 ja 1 vahele. Tõenäosus omistab numbrilise väärtuse sellele, kui palju me usuksime hüpoteesi x kehtimisse, juhul kui me usuksime, et selle tõenäosuse arvutamiseks kasutatud statistilised mudelid vastavad tegelikkusele. Ükski statistiline mudel ei vasta tegelikkusele. Mudelivaba statistikat ei ole olemas (ka kirjeldav statistika kasutab vähemalt kaudselt mudeleid). "],
["kuidas-naevad-valja-teie-andmed.html", "3 Kuidas näevad välja teie andmed Summaarsed statistikud Keskväärtused Muutuja sisene varieeruvus Logaritmi andmed Iseloomusta andmeid algses skaalas: mediaan (MAD) Muutujate koosvarieeruvus", " 3 Kuidas näevad välja teie andmed library(tidyverse) library(stringr) library(car) Summaarsed statistikud Summaarne statistik püüab iseloomustada teie valimit ühe numbri abil. Milliseid summaarseid statistikuid arvutada ja milliseid vältida, sõltub statistilisest mudelist, mis omakorda sõltub teie andmetest ja teie uskumustest andmeid genereeriva protsessi kohta. Summaarse statistika abil iseloomustame tüüpilist valimi liiget (keskmise näitajad), muutuja sisest varieeruvust (standardhälve, mad jms), erinevate muutujate koos-varieeruvust (korrelatsioonikordaja) Keskväärtused Keskväärtust saab mõõta paaril tosinal erineval viisil, millest järgnevalt kasutame kolme või nelja. Enne kui te arvutama kukute, mõelge järele, miks te soovite keskväärtust teada. Kas teid huvitab valimi tüüpiline liige? Kuidas te sooviksite seda tüüpilisust defineerida? Kas valimi keskmise liikmena või valimi kõige arvukama liikmena? või veel kuidagi? See, millist keskväärtust kasutada, sõltub sageli andmejaotuse kujust. Sümmeetrilisi jaotusi on lihtsam iseloomustada ja mitmetipulised jaotused on selles osas kõige kehvemad. Järgnevad nõuanded on rangelt soovituslikud: Kui valim on normaaljaotusega (histogramm on sümmeetriline), hinda tüüpilist liiget läbi aritmeetilise keskmise (mean). Muidu kasuta mediaani (median). Kui valim on liiga väike, et jaotust hinnata (aga &gt; 4), eelista mediaani. Mediaani saamiseks järjestatakse mõõdetud väärtused suuruse järgi ja võetakse selle rea keskmine liige. Mediaan on vähem tundlik ekstreemsete väärtuste (outlierite) suhtes kui mean. Valimi kõige levinumat esindajat iseloomustab mood ehk jaotuse tipp. Seda on aga raskem täpselt määrata ja mitmetipulisel jaotusel on mitu moodi. Töötamisel posterioorsete jaotustega on mood sageli parim lahendus. Joonis 3.1: Simuleeritud lognormaaljaotusega andmed. Punane joon - mood; sinine joon - mediaan; must joon - aritmeetiline keskmine (mean). Milline neist vastab parimini teie intuitsiooniga nende andmete “keskväärtusest”? Miks? Muutuja sisene varieeruvus Aritmeetilise keskmisega (mean) käib kokku standardhälve (SD). SD on sama ühikuga, mis andmed (ja andmete keskväärtus). Statistikute hulgas eelistatud formaat on mean (SD), mitte mean (+/- SD). 1 SD katab 68% normaaljaotusest, 2 SD – 96% ja 3 SD – 99%. Normaaljaotus langeb servades kiiresti, mis tähendab, et tal on peenikesed sabad ja näiteks 5 SD kaugusel keskmisest paikneb vaid üks punkt miljonist. Näiteks: inimeste IQ on normaaljaotusega, mean = 100, sd = 15. See tähendab, et kui sinu IQ = 115 (ülikooli astujate keskmine IQ), siis on tõenäosus, et juhuslikult kohatud inimene on sinust nutikam, 18% ((100% - 68%) / 2 = 18%). Kui aga “tegelikul” andmejaotusel on “paks saba” (nagu eelmisel joonisel kujutatud andmetel) või esinevad outlierid, siis normaaljaotust eeldav mudel tagab ülehinnatud SD ja seega ülehinnatud varieeruvuse. Kui andmed saavad olla ainult positiivsed, siis SD &gt; mean/2 viitab, et andmed ei sobi normaaljaotuse mudeliga (sest mudel ennustab negatiivsete andmete esinemist küllalt suure sagedusega). Standardhälve on defineeritud ka mõnede teiste jaotuste jaoks peale normaaljaotuse (Poissioni jaotus, binoomjaotus). Funktsioon sd() ja selle taga olev võrrand sd = sqrt((mean(x) - x)**2/n - 1) on loodud normaaljaotuse tarbeks ja neid alternatiivseid standardhälbeid ei arvuta. Veelgi enam, igale jaotusele, mida me oskame integreerida, saab ka integraali abil õige katvusega standardhälbe arvutada. Seega tasub meeles pidada, et tavapärane viis standardhälbe arvutamiseks sd() abil kehtib normaaljaotuse mudeli piirides ja ei kusagil mujal! Siiski, kui arvutada standardhälbe sd()-ga, võib olla kindel, jaotusest sõltumata hõlvavad 2 SD-d vähemalt 75% andmejaotusest. Kui andmed ei sobi normaaljaotusesse ja te ei ole rahul tulemusega, mille tõlgendus on nii ebakindel kui 75 protsenti kuni 96+ protsenti, võib pakkuda kahte alternatiivset lahendust: Logaritmi andmed Kui kõik andmeväärtused on positiivsed ja andmed on lognormaaljaotusega, siis logaritmimine muudab andmed normaalseks. Logaritmitud andmetest tuleks arvutada aritmeetiline keskmine ja SD ning seejärel mõlemad anti-logaritmida (näiteks, kui log2(10) = 3.32, siis antilogaritm sellest on 2**3.32 = 10). Sellisel juhul avaldatakse lõpuks geomeetriline keskmine ja multiplikatiivne SD algses lineaarses skaalas (multiplikatiivne SD = geom mean x SD; geom mean/SD). Geomeetriline keskmine on alati väiksem kui aritmeetiline keskmine. Lisaks on SD intervall nüüd asümmeetriline ja SD on alati &gt; 0. See protseduur tagab, et 68% lognormaalsetest andmetest jääb 1 SD vahemikku ning 96% andmetest jääb 2 SD vahemikku. Kui lognormaalsetele andmetele arvutada tavaline sd lineaarses skaalas kasutades sd() funktsiooni, siis tuleb SD sageli palju laiem kui peaks ja hõlmab ka negatiivseid väärtusi (pea meeles, et SD definitsiooni järgi jääb 96% populatsioonist 2 SD vahemikku). Sageli on aga negatiivsed muutuja väärtused võimatud (näiteks nädalas suitsetatud sigarettide arv). See on näide halvast mudelist! logaritmimise kaudu avaldatud multiplikatsiivse SD arvutamiseks kasutame enda kirjutatud funktsiooni mulitplicative_sd(). Esiteks arvutame multiplikatiivse ja aditiivse sd lognormaalsetele andmetele, mida kujutasime eelmisel joonisel: SD MEAN lower upper multiplicative_SD 1.02 0.403 2.56 multiplicative_2_SD 1.02 0.160 6.45 additive_SD 1.60 -0.256 3.46 additive_2_SD 1.60 -2.116 5.32 Tavalise aritmeetitilise keskmise asemel on meil nüüd geomeetriline keskmine. Võrdluseks on antud ka tavaline (aritmeetiline) keskmine ja (aditiivne) SD. Additiivne SD on selle jaotuse kirjeldamiseks selgelt ebaadekvaatne (vt jaotuse pilti ülalpool ja võrdle mulitplikatiivse SD-ga). Kuidas aga töötab multiplikatiivne standardhälve normaaljaotusest pärit andmetega (N=3, mean=100, sd=20)? Kui multiplikatiivse sd rakendamine normaalsete andmete peal viiks katastroofini, siis poleks sel statistikul suurt kasutusruumi. SD MEAN lower upper multiplicative_SD 108 92.8 126 multiplicative_2_SD 108 79.7 147 additive_SD 109 92.1 126 additive_2_SD 109 75.2 143 Nagu näha, on multiplikatiivse sd kasutamine normaalsete andmetega pigem ohutu (kui andmed on positiivsed). Arvestades, et additiivne SD on lognormaalsete andmete korral kõike muud kui ohutu ning et lognormaaljaotus on bioloogias üsna tavaline (eriti ensüümreaktsioonide ja kasvuprotsesside juures), on mõistlik alati kasutada multiplicative_sd() funktsiooni. Kui mõlema SD väärtused on sarnased, siis võib loota, et andmed on normaalsed ning saab refereede rõõmuks avaldada tavapärase additiivse SD. kui n &lt; 10, siis mõlemad SD-d alahindavad süstemaatiliselt tegelikku sd-d.Et tevaatust väikeste valimitega! Vahest tekkib teil vajadus empiiriliselt määrata, kas teie andmed on normaaljaotusega. Enne kui seda tegema asute, peaksite mõistma, et see, et teie valim ei ole normaalne, ei tähenda automaatselt, et populatsioon, millest see valim tõmmati, ei oleks normaaljaotusega. Igal juhul, valimiandmete normaalsuse määramiseks on kõige mõistlikum kasutada qq-plotti. QQ-plot (kvantiil-kvantiil plot) võrdleb andmete jaotust ideaalse normaaljaotusega andmepunkti haaval. Kui empiiriline jaotus kattub referentsjaotusega, siis on tulemuseks sirgel paiknevad punktid. Järgneval qq plotil on näha, mis juhtub, kui plottida lognormaalseid andmeid normaaljaotuse vastu: qqPlot(andmed) #&gt; [1] 23 37 Joonis 3.2: QQ-plot lognormaalsetele andmetele. Plotil võrreldakse lognormaalsete andmete jaotust referentsjaotusega, milleks on antud juhul normaaljaotus. Punased katkendjooned annavad standardveapõhise usaldusvahemiku (arvutatud simuleeritud juhuvalimist normaaljaotusega referentspopulatsioonist), millesse peaks jääma enamus andmepunkte juhul kui andmepunktid pärineksid normaaljaotusest. Nüüd joonistame qq-ploti logaritmitud andmetele. qqPlot(log(andmed)) #&gt; [1] 23 37 Joonis 3.3: QQ-plot normaalsetele andmetele (logaritmitud lognormaalsed andmed). Pole kahtlust, andmed on logaritmitud kujul normaaljaotusega. qqPlot() võimaldab võrrelda teie andmeid ükskõik millise R-is defineeritud jaotusega (?car::qqPlot). Normaaljaotuse kindlakstegemiseks on loodud ka peotäis sageduslikke teste, mis annavad väljundina p väärtuse. Nende kasutamisest soovitame siiski hoiduda, sest tulemused on sageli ebakindlad, eriti väikestel ja suurtel valimitel. Mõistlikum on vaadata kõikide andmepunktide plotti normaaljaotuse vastu, kui jõllitada ühte numbrit (p), mille väärtus, muuseas, monotooniliselt langeb koos valimi suuruse kasvuga. Iseloomusta andmeid algses skaalas: mediaan (MAD) MAD –– median absolute deviation — on vähem tundlik outlierite suhtes ja ei eelda normaaljaotust. Puuduseks on, et MAD ei oma tõlgendust, mille kohaselt ta hõlmaks kindlat protsenti populatsiooni või valimi andmejaotusest. Seevastu sd puhul võime olla kindlad, et isegi kõige hullema jaotuse korral jäävad vähemalt 75% andmetest 2 SD piiridesse. Lognormaalsete andmetega: mad(andmed, constant = 1); sd(andmed); mad(andmed) #&gt; [1] 0.512 #&gt; [1] 1.86 #&gt; [1] 0.759 mad(log10(andmed), constant = 1); sd(log10(andmed)); mad(log10(andmed)) #&gt; [1] 0.274 #&gt; [1] 0.401 #&gt; [1] 0.406 mad = median(abs(median(x) - x)), mida on väga lihtne mõista. Samas R-i funktsioon mad() korrutab default-ina mad-i läbi konstandiga 1.4826, mis muudab mad()-i tulemuse võrreldavaks sd-ga, tehes sellest sd robustse analoogi. Robustse sellepärast, et mad-i arvutuskäik, mis sõltub mediaanist, mitte aritmeetilisest keskmisest, ei ole tundlik outlierite suhtes. Seega, kui tahate arvutada mad-i, siis fikseerige mad() funktsioonis argument constant ühele. Ära kunagi avalda andmeid vormis: mean (MAD) või median (SD). Korrektne vorm on mean (SD) või median (MAD). Veel üks viis andmejaotuse summeerimiseks on kasutada kvantiile. Siin saame me tüüpiliselt rohkem kui ühe numbri, aga sageli on selline viis informatiivsem, kui ühenumbrilised summaarsed statistikud. Funktsioon quantile võimaldab valida, milliseid kvantiile soovite näha. Järgnevas koodist saame teada, millisest vektori “andmed” väärtusest allapoole jääb 2.5%, 25%, 50%, 75% ja 95% väärtusi. quantile(andmed, c(0.025, 0.25, 0.5, 0.75, 0.95)) #&gt; 2.5% 25% 50% 75% 95% #&gt; 0.230 0.517 0.991 1.737 5.015 Muutujate koosvarieeruvus Andmete koos-varieeruvust mõõdetakse korrelatsiooni abil. Tulemuseks on üks number - korrelatsioonikordaja r, mis varieerub -1 ja 1 vahel. r = 0 – kahte tüüpi mõõtmised (x=pikkus, y=kaal) samadest mõõteobjektidest varieeruvad üksteisest sõltumatult. r = 1: kui ühe muutuja väärtus kasvab, kasvab ka teise muutuja väärtus alati täpselt samas proportsioonis. r = -1: kui ühe muutuja väärtus kasvab, kahaneb teise muutuja väärtus alati täpselt samas proportsioonis. Kui r on -1 või 1, saame me x väärtust teades täpselt ennustada y väärtuse (ja vastupidi, teades y väärrust saame täpselt ennustada x väärtuse). Kuidas tõlgendame aga tulemust r = 0.9? Mitte kuidagi. Selle asemel tõlgendame r2 = 0.9**2 = 0.81 – mis tähendab, et x-i varieeruvus suudab seletada 81% y varieeruvusest ja vastupidi, et Y-i varieeruvus suudab seletada 81% X-i varieeruvusest. Korrelatsiooni saab mõõta mitmel viisil (?cor.test, method=). Kõige levinum on Pearsoni korrelatsioonikoefitsient, mis eeldab, (i) et me mõõdame pidevaid muutujaid, (ii) et valim on esinduslik populatsiooni suhtes, (iii) et populatsiooniandmed on normaaljaotusega ja (iv) et igal mõõteobjektil on mõõdetud 2 omadust (pikkus ja kaal, näiteks). Tuntuim alternatiiv on mitteparameetriline Spearmani korrelatsioon, mis ei eelda andmete normaaljaotust ega seda, et mõõdetakse pidevaid suurusi (ordinaalsed andmed käivad kah). Kui kõik Pearsoni korrelatsiooni eeldused on täidetud ja te kasutate siiski Spearmani korrelatsiooni, siis on teie arvutus ca. 10% vähemefektiivne. cor(iris$Sepal.Length, iris$Sepal.Width, use = &quot;complete.obs&quot;) #&gt; [1] -0.118 Korrelatsioonikordaja väärtus sõltub mitte ainult andmete koosvarieeruvusest vaid ka andmete ulatusest. Suurema ulatusega andmed X ja/või Y teljel annavad keskeltläbi 0-st kaugemal oleva korrelatsioonikordaja. Selle pärast sobib korrelatsioon halvasti näiteks korduskatsete kooskõla mõõtmiseks. Lisaks, korrelatsioonikordaja mõõdab vaid andmete lineaarset koos-varieeruvust: kui andmed koos-varieeruvad mitte-lineaarselt, siis võivad ka väga tugevad koos-varieeruvused jääda märkamatuks. Joonis 3.4: Anscombe’i kvartett illustreerib korrelatsioonikordaja lineaarset olemust: neli andmestikku annavad identse korrelatsioonikordaja (Pearsons’r), ehkki tegelikud seosed andmete vahel on täiesti erinevad. Moraal seisneb selles, et enne korrelatsioonikordaja arvutamist tasub alati plottida andmed, et veenduda võimaliku seose lineaarsuses. Lineaarsuse puudumine andmete koosvarieeruvuse mustris tähendab, et korrelatsioonikordaja tuleb eksitav. Korrelatsioonikordaja mõõdab pelgalt määra, mil üks muutuja muutub siis, kui teine muutuja muutub. Seega ei ole suurt mõtet arvutada korrelatsioonikordajat juhul kui me teame ette seose olemasolust kahe muutuja vahel. Näiteks, kui sama entiteeti mõõdetakse kahel erineval viisil, või kahes korduses, või kui esimene muutuja arvutatakse teise muutuja kaudu. Kõik summaarsed statistikud kaotavad enamuse teie andmetes leiduvast infost – see kaotus on õigustatud ainult siis, kui teie poolt valitud statistik iseloomustab hästi andmete sügavamat olemust (näiteks tüüpilist mõõtmistulemust või andmete varieeruvust). Korrelatsioonimaatriksi saab niimoodi: # numeric columns only! # the following gives cor matrix with # frequentist correction for mutliple testing: # print(psych::corr.test(iris[-5])) # only numeric cols allowed! Hence -Species knitr::kable(cor(iris[,-5])) Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length 1.000 -0.118 0.872 0.818 Sepal.Width -0.118 1.000 -0.428 -0.366 Petal.Length 0.872 -0.428 1.000 0.963 Petal.Width 0.818 -0.366 0.963 1.000 "],
["lineaarsed-mudelid.html", "4 Lineaarsed mudelid 4.1 Sirge võrrand Ennustus lineaarsest mudelist Neli mõistet Mudeli fittimine Üle- ja alafittimine 4.2 Regressioonimudelite eeldused 4.3 Andmete transformeerimine 4.4 Üldised printsiibid", " 4 Lineaarsed mudelid library(tidyverse) library(broom) library(modelr) library(viridis) 4.1 Sirge võrrand Oletame, et me mõõtsime N inimese pikkuse cm-s ja kaalu kg-s ning meid huvitab, kuidas inimeste pikkus sõltub nende kaalust. Lihtsaim mudel pikkuse sõltuvusest kaalust on pikkus = kaal (formaliseeritult: y = x) ja see mudel ennustab, et kui Juhani kaal = 80 kg, siis Juhan on 80 cm pikkune. Siin on pikkus muutuja, mille väärtust ennustatakse (y) ja kaal muutuja x, mille väärtuste põhjal ennustatakse pikkusi. Muidugi, sama hästi võiksime ennustada kaalu pikkuste põhjal, ja kumma ennustuse valime sõltub siinkohal eeskätt meie teaduslikest huvidest. Veelgi enam, sama mudel ennustab, et kui Juhani kaal = 80 tonni, siis on Juhan samuti 80 cm pikkune (eeldusel, et me avaldame x muutuja tonnides ja y muutuja sentimeetrites). Seega, mudeli ennustuse täpsus sõltub ühikutest, milles me andmed mudelisse sisse anname. #Genereerime andmed: x = pikkus ja y = kaal: x &lt;- 0:100 y &lt;- x Selle mudeli saame graafiliselt kujutada nii: plot(y ~ x, type = &quot;l&quot;, xlab = &quot;Weight in kg&quot;, ylab = &quot;Heigth in cm&quot;, main = bquote(y == x)) Joonis 4.1: Lihtne mudel y ~ x, mille lõikepunkt = 0 ja tõus = 1. Üldistatult, mudeli keeles tähistame me seda muutujat, mille väärtusi me ennustame, Y-ga ja seda muutujat, mille väärtuse põhjal me ennustame, X-ga. Sirge mudeli lihtsaim matemaatiline formalism on Y = X. See on äärmiselt jäik mudel: sirge, mille asukoht on rangelt fikseeritud. Sirge lõikab y telge alati 0-s (mudeli keeles: sirge intercept ehk lõikepunkt Y teljel = 0) ja tema tõusunurk saab olla ainult 45 kraadi (mudeli keeles: mudeli slope ehk tõus = 1). Selle mudeli jäikus tuleneb sellest, et temas ei ole parameetreid, mille väärtusi me saaksime vabalt muuta ehk tuunida. Mis juhtub, kui me lisame mudelisse konstandi, mille liidame x-i väärtustele? \\[y = a + x\\] See konstant on mudeli parameeter, mille väärtuse võime vabalt valida. Järgnevalt anname talle väärtuse 30 (ilma konkreetse põhjuseta). x &lt;- 0:100 a &lt;- 30 y &lt;- a + x plot(y ~ x, xlim = c(0, 100), ylim = c(0, 150), type = &quot;l&quot;, main = bquote(y == a + x)) abline(c(0, 1), lty = 2) Joonis 4.2: Lineaarne mudel, mille lõikepunkt = 30 ja tõus = 1. Katkendjoon, lõikepunkt = 0. Pidevjoon, lõikepunkt = 30 Meie konstant a määrab \\(y\\) väärtuse, kui \\(x = 0\\), ehk sirge lõikepunkti \\(y\\) teljel. Teisisõnu, a = mudeli lõikepunkt (intercept). Mis juhtub, kui me mitte ei liida, vaid korrutame x-i konstandiga? \\[y = b \\times x\\] Jällegi, me anname mudeli parameetrile b suvalise väärtuse, 3. x &lt;- 0:200 b &lt;- 3 y &lt;- b * x plot(y ~ x, xlim = c(0, 100), ylim = c(0, 100), type = &quot;l&quot;, main = bquote(y == b %*% x)) abline(c(0, 1), lty = 2) Joonis 4.3: Lineaarne mudel, mille lõikepunkt = 0 ja tõus = 3. Katkendjoon, tõus = 1. Pidevjoon, tõus = 3. Nüüd muutub sirge tõusunurk, ehk kui palju me ootame y-t muutumas, kui x muutub näiteks ühe ühiku võrra. Kui b = 3, siis x-i tõustes ühe ühiku võrra suureneb y kolme ühiku võrra. Proovi järgi, mis juhtub, kui b = -3. Selleks, et sirget kahes dimensioonis vabalt liigutada, piisab kui me kombineerime eelnevad näited ühte: \\[y = a + b \\times x\\] Selleks lisame mudelisse kaks parameetrit, lõikepunkt (a) ja tõus (b). Kui \\(a = 0\\) ja \\(b = 1\\), saame me eelpool kirjeldatud mudeli \\(y = x\\). Kui \\(a = 102\\), siis sirge lõikab y-telge väärtusel 102. Kui \\(b = 0.8\\), siis x-i tõustes 1 ühiku võrra tõuseb y-i väärtus 0.8 ühiku võrra. Kui \\(a = 100\\) ja \\(b = 0\\), siis saame sirge, mis on paraleelne x-teljega ja lõikab y-telge väärtusel 100. Seega, teades a ja b väärtusi ning omistades x-le suvalise meid huvitava väärtuse, saab ennustada y-i keskmist väärtust sellel x-i väärtusel. Näiteks, olgu andmete vastu fititud mudel pikkus(cm) = 102 + 0.8 * kaal(kg) ehk \\[y = 102 + 0.8 \\times x\\] Omistades nüüd kaalule väärtuse 80 kg, tuleb mudeli poolt ennustatud keskmine pikkus 102 + 0.8 * 80 = 166 cm. Iga kg lisakaalu ennustab mudeli kohaselt 0.8 cm võrra suuremat pikkust. a &lt;- 102 b &lt;- 0.8 x &lt;- 0:100 y &lt;- a + b * x plot(y ~ x, xlab = &quot;Weight in kg&quot;, ylab = &quot;Heigth in cm&quot;, ylim = c(50, 200), type = &quot;l&quot;, main = bquote(y == 102 + 0.8 %*% x)) Joonis 4.4: Lineaarne mudel, millel on tuunitud nii lõikepunkt kui tõus. See mudel ennustab, et 0 kaalu juures on pikku 102 cm, mis on rumal, aga mudelite puhul tavaline olukord. Sellel olukorral on mitmeid põhjusi: Me tuunime mudelit andmete peal, mis ei sisalda 0-kaalu. Meie valimiandmed ei peegelda täpselt inimpopulatsiooni. Sirge mudel ei peegelda täpselt pikkuse-kaalu suhteid vahemikus, kus meil on reaalseid kaaluandmeid; ja ta teeb seda veelgi vähem seal, kus meil mõõdetud kaalusid ei ole. Seega pole mõtet imestada, miks mudeli intercept meie üle irvitab. Kahe parameetriga sirge mudel ongi see, mida me fitime kahedimensiooniliste andmetega. Näiteks nii, kasutame R-i “iris” andmesetti: # Fit a linear model and name the model object as m m &lt;- lm(Sepal.Length ~ Petal.Length, data = iris) # Make a scatter plot, colored by the var called &quot;Species&quot; # Draw the fitted regression line from m augment(m, iris) %&gt;% ggplot(aes(Petal.Length, Sepal.Length, color = Species)) + geom_point() + geom_line(aes(y = .fitted), color = 1) + labs(title = &quot;Sepal.Length ~ Petal.Length&quot;) + scale_color_viridis(discrete = TRUE) Joonis 4.5: Fititud mudel, kus muutuja Petal.Length järgi ennustatakse muutuja Sepal.Length väärtusi. Mudeli fittimine tähendab siin lihtsalt, et sirge on 2D ruumi asetatud nii, et see oleks võimalikult lähedal kõikidele punktidele. Oletame, et meil on n andmepunkti ja et me fitime neile sirge. Nüüd plotime fititud sirge koos punktidega ja tõmbame igast punktist mudelsirgeni joone, mis on paraleelne y-teljega. Seejärel mõõdame nende n joone pikkused. Olgu need pikkused a, b, … i. lm() funktsioon fitib sirge niimoodi, et summa \\(a^2 + b^2 + ... + i^2\\) oleks minimaalne. Seda kutsutakse vähimruutude meetodiks. Mudeli koefitsientide väärtused saame kasutades funktsiooni coef(): coef(m) #&gt; (Intercept) Petal.Length #&gt; 4.307 0.409 Siin a = (Intercept) ja b = Petal.Length ehk 0.41. Ennustus lineaarsest mudelist Anname x-le rea väärtusi, et ennustada y keskmisi väärtusi nendel x-i väärtustel. Siin me ennustame y (Sepal_length) keskväärtusi erinevatel x-i (Petal_length) väärtustel, mitte individuaalseid Sepal_length väärtusi. Me kasutame selleks deterministlikku mudelit kujul Sepal_length = a + b*Petal_length. Hiljem õpime ka bayesiaanlike meetoditega individuaalseid Sepal_length-e ennustama. Järgnev kood on sisuliselt sama, millega me üle-eelmisel plotil joonistasime mudeli y = a + bx. Me fikseerime mudeli koefitsiendid fititud irise mudeli omadega ja anname Petal_length muutujale 10 erinevat väärtust originaalse muutuja mõõtmisvahemikus. Aga sama hästi võiksime ekstrapoleerida ja küsida, mis on oodatav Sepal_length, kui Petal_length on 100 cm? Sellele küsimusele on ebareaalne vastus, aga mudel ei tea seda. Proovi, mis vastus tuleb. ## Genereerime uued andmed Petal.Length vahemikus Petal_length &lt;- seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out = 10) ## Võtame mudeli koefitsendid a &lt;- coef(m)[1] b &lt;- coef(m)[2] ## Kasutades mudeli koefitsente genereerime Sepal_length väärtused Sepal_length &lt;- a + b * Petal_length plot(Sepal_length ~ Petal_length, type = &quot;b&quot;) Joonis 4.6: Siin ennustasime kümme y väärtust x väärtuste põhjal.. Mudelist saab kahte tüüpi ennustusi: interpolatsioone (ennustus on samas skaalas, mis andmed, mille peal mudel fititi) ja ekstrapoleerimine (ennustus jääb väljaspoole andmeid). Sõltuvalt erialast me kas vaatame ekstrapoleerimisele viltu või leiame, et selline tegevus on otsesõnu keelatud. Mudelist saab kahte tüüpi ennustusi: (1) saame ennustada Y keskmist väärtust X-i konkreetsel väärtusel ja (2) saame ennustada individuaalseid Y väärtusi X-i konkreetsel väärtusel. Viimase kohta vt ptk… Neli mõistet Mudelis \\(y = a + bx\\) on \\(x\\) ja \\(y\\) muutujad, ning \\(a\\) ja \\(b\\) on parameetrid. Muutujate väärtused fikseeritakse andmetega, parameetrid fititakse andmete põhjal. Fititud mudel valib kõikide võimalike seda tüüpi mudelite hulgast välja täpselt ühe unikaalse mudeli ja ennustab igale \\(x\\)-i väärtusele vastava kõige tõenäolisema \\(y\\) väärtuse (\\(y\\) keskväärtuse sellel \\(x\\)-i väärtusel). Y — mida me ennustame (dependent variable, predicted variable). X — mille põhjal me ennustame (independent variable, predictor). Muutuja (variable) — iga asi, mida me valimis mõõdame (X ja Y on kaks muutujat). Muutujal on sama palju fikseeritud väärtusi kui meil on selle muutuja kohta mõõtmisandmeid. Parameeter (parameter) — mudeli koefitsient, millele võib omistada suvalisi väärtusi. Parameetreid tuunides fitime mudeli võimalikult hästi sobituma andmetega. Mudel on matemaatilise formalism, mis püüab kirjeldada füüsikalist protsessi. Statistilise mudeli struktuuris on komponent, mis kirjeldab ideaalseid ennustusi (nn protsessi mudel) ja eraldi veakomponent (ehk veamudel), mis kirjeldab looduse varieeruvust nende ideaalsete ennustuste ümber. Mudeli koostisosad on (i) muutuja, mille väärtusi ennustatakse, (ii), muutuja(d), mille väärtuste põhjal ennustatakse, (iii) parameetrid, mille väärtused fititakse ii põhjal ja (iv) konstandid. Mudeli fittimine Mudelid sisaldavad nii (1) matemaatilisi struktuure, mis määravad mudeli tüübi, kui (2) parameetreid, mida saab andmete põhjal tuunida, niiviisi täpsustades mudeli kuju ehk paiknemist matemaatlises ruumis. Näiteks võrrand \\(y = a + bx\\) määrab mudeli, kus \\(y = x\\) on see struktuur, mis tagab, et mudeli tüüp on sirge, ning \\(a\\) ja \\(b\\) on parameetrid, mis määravad sirge asendi. Seevastu struktuur \\(y = x + x^2\\) tagab, et mudeli \\(y = a + b_1x + b_2x^2\\) tüüp on parabool, ning parameetrite \\(a\\), \\(b_1\\) ja \\(b_2\\) väärtused määravad selle parabooli täpse kuju. Ja nii edasi. Mudeli parameetrite tuunimist nimetatakse mudeli fittimiseks. Mudelit fittides on eesmärk saavutada antud tüüpi mudeli maksimaalne sobivus andmetega (kus “andmed” hõlmavad nii valimiandmeid kui taustateadmisi). Sellele tegevusele annab mõtte meie lootus, et mudeli tüüp kajastab mingit looduses toimuvat protsessi, mis meile teaduslikku huvi pakub. Ning, kuigi mudeli fit maksimeeritakse mudeli tüübi kohta, püüab see andmete vaatenurgast vaadatuna olla optimaalne, mitte maksimaalne (vt järgmine peatükk mudeli üle- ja alafittimisest). Kahjuks ei ole selline optimaalsus kuigi hästi matemaatilisse vormi valatav, ega ka mingi (pool)automaatse meetodiga empiiriliselt kontrollitav. Siin on tegu pigem teadlase sooviga, mille filosoofiline eeldus on, et meie andmetes on peidus nii andmeid genereeriva loodusliku protsessi üldine olemus (essents), kui juhuslik müra ehk valimiviga, ning et mudeli üldine kuju (sirge, parabool, jms) on juhtumisi sobiv just selleks, et neid kahte omavahel lahku ajada. Lineraarse mudeli parima sobivuse andmetega saab tagada kahel erineval viisil: (i) vähimruutude meetod (Legendre, 1805; Gauss, 1809) mõõdab y telje suunaliselt iga andmepunkti kauguse mudeli ennustusest, võtab selle kauguse ruutu, summeerib kauguste ruudud ning leiab sirge asendi, mille korral see summa on minimaalne; (ii) Bayesi teoreem (Laplace, 1774) annab väheinformatiivse priori korral praktiliselt sama fiti. Olulise erinevusena võtab vähimruutude meetod arvesse ainult valimiandmed, samas kui Bayesi teoreemi kasutades fitime mudeli koefitsiente nii valimiandmete kui taustateadmiste peal (vt 8. ptk). Hea mudel on Võimalikult lihtsa struktuuriga, mille põhjal on veel võimalik teha järeldusi protsessi kohta, mis genereeris mudeli fittimiseks kasutatud andmeid; Sobitub piisavalt hästi andmetega (eriti uute andmetega, mida ei kasutatud selle mudeli fittimiseks), et olla relevantne andmeid genereeriva protsessi kirjeldus; Genereerib usutavaid simuleeritud andmeid. Sageli fititkse samade andmetega mitu erinevat tüüpi mudelit ja püütakse otsustada, milline neist vastab kõige paremini eeltoodud tingimustele. Näiteks, kui sirge suudab kaalu järgi pikkust ennustada paremini kui parabool, siis on sirge mudel paremas kooskõlas teadusliku hüpoteesiga, mis annaks mehhanismi protsessile, mille käigus kilode lisandumine viiks laias kaaluvahemikus inimeste pikkuse kasvule ilma, et pikkuse kasvu tempo kaalu tõustes langeks. Samas, see et me oleme oma andmeid fittinud n mudeliga ja otsustanud, et mõned neist on paremad kui teised, ei tähenda, et mõni meie mudelitest oleks hea ka võrdluses tegeliku looduses valitseva olukorraga. Mudelid on pelgalt matemmatilised formalismid, mis võivad, aga kindlasti ei pea, kajastama füüsikalist maailma, ja meie mudelitevalik sõltub meile jõukohasest matemaatikast. Siinkohal ei tasu unustada, et matemaatika kirjeldab eelkõige abstraktseid mustreid, mitte otse füüsikalist maailma. See, et teie andmed sobivad hästi mingi mudeliga, ei tähenda automaatselt, et see fakt oleks teaduslikult huvitav. Mudeli parameetrid on mõtekad mudeli matemaatilise kirjelduse kontekstis, aga mitte tingimata suure maailma põhjusliku seletamise kontekstis. Siiski, kui mudeli matemaatiline struktuur loodi andmeid genreeeriva loodusliku protsessi olemust silmas pidades, võib mudeli koefitsientide uurimisest selguda olulisi tõsiasju suure maailma kohta. Mudeli fittimine: X ja Y saavad oma väärtused otse andmetest; parameetrid võivad omandada ükskõik millise väärtuse. Fititud mudelist ennustamine: X-le saab omistada ükskõik millise väärtuse; parameetrite väärtused on fikseeritud; Y väärtus arvutatakse mudelist. Üle- ja alafittimine Osad mudelite tüübid on vähem paindlikud kui teised (parameetreid tuunides on neil vähem liikumisruumi). Kuigi sellised mudelid sobituvad halvemini andmetega, võivad need ikkagi paremini kui mõni paindlikum mudel välja tuua andmete peidetud olemuse. Statistiline mudeldamine eeldab, et me usume, et meie andmetes leidub nii müra (mida mudel võiks ignoreerida), kui signaal (mida mudel püüab tabada). Ilma signaalita süsteemi poleks arusaadavatel põhjustel mõtekas mudeldada ja ilma mürata süsteemi mudel tuleks ilma varieeruvuse (vea) komponendita, ehk deterministlik. Kuna mudeli jaoks näeb müra samamoodi välja kui signaal, on iga mudel kompromiss üle- ja alafittimise vahel. Me lihtsalt loodame, et meie mudel on piisavalt jäik, et mitte liiga palju müra modelleerida ja samas piisavalt paindlik, et piisaval määral signaali tabada. Üks kõige jäigemaid mudeleid on sirge, mis tähendab, et sirge mudel on suure tõenäosusega alafittitud. Keera sirget kuipalju tahad, ikka ei sobitu ta enamiku andmekogudega. Ja need vähesed andmekogud, mis sirge mudeliga sobivad, on genereeritud teatud tüüpi lineaarsete protsesside poolt. Sirge on seega üks kõige paremini tõlgendatavaid mudeleid. Teises äärmuses on polünoomsed mudelid, mis on väga paindlikud, mida on väga raske tõlgendada ja mille puhul esineb suur mudeli ülefittimise oht. Ülefititud mudel järgib nii täpselt valimiandmeid, et sobitub hästi valimis leiduva juhusliku müraga ning seetõttu sobitub halvasti järgmise valimiga samast populatsioonist (igal valimil on oma juhuslik müra). Üldiselt, mida rohkem on mudelis tuunitavaid parameetreid, seda paindlikum on mudel, seda kergem on seda valimiandmetega sobitada ja seda raskem on seda tõlgendada. Veelgi enam, alati on võimalik konstrueerida mudel, mis sobitub täiuslikult kõikide andmepunktidega (selle mudeli parameetrite arv = N). Selline mudel on täpselt sama informatiivne kui andmed, mille põhjal see fititi — ja täiesti kasutu. Joonis 4.7: Kasvava paindlikusega polünoomsed mudelid. Vähimruutude meetodil fititud mudeleid saame võrrelda AIC-i näitaja järgi. AIC - Akaike Informatsiooni Kriteerium - vaatab mudeli sobivust andmetega ja mudeli parameetrite arvu. Väikseim AIC tähitab parimat fitti väikseima parameetrite arvu juures (kompromissi) ja väikseima AIC-ga mudel on eelistatuim mudel. Aga seda ainult võrreldud mudelite hulgas. AIC-i absoluutväärtus ei loe - see on suhteline näitaja. model_formula aic y ~ x 35.0 y ~ poly(x, 2) 37.0 y ~ poly(x, 3) 36.1 y ~ poly(x, 4) 32.5 y ~ poly(x, 5) -Inf AIC näitab, et parim mudel on mod_e4. Aga kas see on ka kõige kasulikum mudel? Mis siis, kui 3-s andmepunkt on andmesisestaja näpuviga? Ülefittimise vältimiseks kasutavad Bayesi mudelid informatiivseid prioreid, mis välistavad ekstreemsed parameetriväärtused. Vt http://elevanth.org/blog/2017/08/22/there-is-always-prior-information/ 4.2 Regressioonimudelite eeldused Tähtsuse järjekorras: valiidsus – sa mõõdad asju, mis on teaduslikult olulised ja relevantsed teaduslikule küsimusele. Näiteks, kui soovite mõõta kolesterooli alandava ravimi mõju, on kaasaegne soovitus mõõta suremust, mitte pelgalt kolesterooli taset veres. esinduslikkus – andmed peaksid olema esinduslikud laiema populatsiooni suhtes. Väikesed ja kallutatud valimid ei ole sageli esinduslikud. aditiivsus ja lineaarsus. Väga tähtis on, et lineaarse regressiooniga mõõdetavad seosed oleks ka tõesti lineaarsed. Y-i deterministlik komponent peab olema lineaarne funktsioon prediktoritest \\(y = β_1x_1 + β_2x_2 +···\\). Kui sellega on probleeme, siis võib aidata prediktorite transformeerimine (log(x) või 1/x) või uute prediktorite mudelisse lisamine. Samuti on võimalik prediktoritena lisada nii \\(x\\) kui \\(x^2\\). Näiteks kui me paneme mudelisse nii muutuja \\(vanus\\) kui \\(vanus^2\\), siis saame modelleerida seost kus y vanuse kasvades alguses kasvab ja siis kahaneb (aga ka U kujulist seost vanusega). Sellisel juhul võib olla mõistlik rekodeerida vanus kategooriliseks muutujaks (näit 4 vanuseklassi), mille tasemeid saab siis ükshaaval vaadata. vigade sõltumatus vigade võrdne varieeruvus (homoskedastilisus) ja vigade normaalsus on vähemtähtsad. Neid eeldusi ei tasu tavaliselt kontrollida (kuigi meetodid selleks on olemas). 4.3 Andmete transformeerimine Lineaarsed transformatsioonid ei mõjuta mudeli fitti. Küll aga võivad nad hõlbustada mudeli koefitsientide tõlgendamist (kas x on millimeetrites, meetrites vms). Mittelineaarsed transformatsioonid muudavad mudeli fitti ja võivad olla kasulikud mudeli aditiivsuse/lineaarsuse oarandamisel. 4.3.1 Logaritmimine Kui x-l saavad olla ainult positiivsed väärtused, siis on logaritmimine sageli hea mõte. Samas, andmetest peab kaotama nullid (või asendama mingi nullilähedase positiivse väärtusega). Näiteks seost sissetuleku ja oodatava eluea vahel on lihtsam ette kujutada kui eluiga ~ log(sissetulek). Log skaalas on beeta koefitsiendid peaaegu alati &lt; 1. Kui naturaallogaritmitud (aga mitte kümnend või kahendlogaritm) andmete peal fititud beta on väike (nullile lähedal), siis saab seda otse tõlgendada kui suhtelist erinevust. Näiteks, kui beta = 0.06, siis võime seda tõlgendada nii: ühe ühikuline x-i muutus viib 6%-sele y muutusele. Sedamõõda kui beeta kaugeneb nullist (näiteks beta = 0.4), hakkab selline kiire hinnang tõsiselt alahindama tegelikku x-i mõju y väärtusele. Erinevus naturaallogaritmi (log, põõrdlogaritm = exp(log(x))) ja log10 või log2 vahel on, et esimesel juhul on kergesti tõlgendatav beeta (aga mitte logaritmitud x-i väärtused) ja teisel juhul (log10 v log2) on kergesti tõlgendatavad logaritmitud x-i väärtused (aga mitte beeta). logaritmimine ja antilogaritmimine: log(100) = 4.60517 exp(4.60517) = 100 log10(100) = 2 10**2 = 100 log2(100) = 6.643856 2**6.643856 = 100 Kui logaritmime nii x-i kui y-i, siis saab beetat tõlgendada oodatava y suhtelise muutusena vastusena x-i suhtelisele muutusele. Näiteks kui b = 1.4, siis x-i muutus 1% võrra viib muutuseni y-s 1.4% võrra. 4.3.2 Standardiseerimine \\(x.z = (x - mean(x))/sd(x)\\) Sellisel viisil standardiseeritud andmete keskväärtus on 0 ja sd = 1. Seega on kõik predikorid samas skaalas ja me mõõdame efekte sd ühikutes. See muudab lihtsaks algeslt erinevas skaalas prediktorite võrdlemise. Intecept tähendab nüüd keskmist ennustust juhul kui kõik prediktorid on fikseeritud oma keskväärtustel. Kui meie mudel sisaldab binaarseid muutujaid, siis on kasulikum standardiseerida üle 2xSD, jättes binaarsed muutujad muutmata. \\(x.z2 = (x - mean(x))/(2 * sd(x))\\) Nüüd tähendab 1 ühikuline muutus efekti -1 SD-st kuni 1 SD-ni üle keskväärtuse. 4.3.3 tsentreerimine x.c1 = x - mean(x), mis annab keskväärtuseks nulli aga jätab varieeruvused algsesse skaalasse Teine võimalus on tsentreerida mõnele teaduslikult mõistlikule väärtusele. Näiteks IQ-d saab tsentreerida 100-le (x - 100). 4.3.4 mudeli koefitsientide transformeerimine Ilma interaktsioonideta mudeli korral saab sama tulemuse, mis prediktoreid tsentreerides, kui me ei tee midagi enne kui saame fititud keofitsiendid. Nüüd reskaleerime need korrutades iga beta oma prediktori kahekordse sd-ga (\\(β.x = β * 2* sd(x)\\)). Nende β.x-de pealt näeb hästi iga muutuja suhtelist tähtsust mudelis. 4.3.5 korrelatsioonikoefitsiendi arvutamine regressioonikoefitsientidest Kui standardiseerime nii y kui x-i x &lt;- (x-mean(x))/sd(x) y &lt;- (y-mean(y))/sd(y) siis y~x regressiooni intercept = 0 ja tõus on sama, mis x ja y vaheline korrelatsioonikoefitsient r. Seega jääb tõus alati -1 ja 1 vahele (tähtis: tõus on pea alati &lt; 1). Siit tuleb ka seletus nähtusele, mida kutsutakse regressiooniks keskmisele (regression to the mean). Fakti, et y on sellises mudelis alati 0-le lähemal kui x, kutsutaksegi regressiooniks keskmisele. Näiteks, kui olete 20 cm keskmisest pikem ja pikkuse päritavus on 0.5, siis on oodata, et teie järglased on keskeltläbi 10 cm võrra keskmisest pikemad (ja teist lühemad). Selle pseudo-põhjusliku nähtuse avastas Francis Galton. 4.3.6 pidev või diskreetne muutuja? Tavaliselt on mõistlik fittida mudel pidevale y muutujale ka siis, kui tahame lõpuks tõlgenada tulemusi diskreetsel skaalal. Pidev muutuja sisaldab lihtsalt rohkem informatsiooni ja seetöttu on meil lootust saada parem fit. Erandiks on pidevad x muutujad, mille mõju y-le on mittelineaarne (näiteks vanuse mõju suremusele). Siin on vahest mõistlik vastupidi konverteerida pidev muutuja faktormuutujaks ja saada hinnang näiteks igale vanuseklassile eraldi kasutades skeemi \\(eluiga = vanus + vanus^2\\). 4.4 Üldised printsiibid võta sisse kõik teaduslikku huvi pakkuvad muutujad ja viska välja muutujad, mille kohta sul pole põhust arvata, et nad võiksid y väärtusi mõjutada. kontrolli, ega muutujate vahel ei esine väga tugevaid korrelatsioone (kollineaarsus). Kui jah, siis kombineeri kollineaarsed muutujad üheks või transformeeri neid või viska mõni muutuja välja. muutujad, mis ei varieeru, ei oma ka regressioonis mõju. tugeva mõjuga muutujate puhul võib olla vajalik sisse tuua nende muutujate interkatsioond (vt ptk …). muutujad, mida sa reaalselt mõõtsid ei pruugi olla need muutujad, mis mudelisse lähevad – näiteks arvuta kehamassiindeks mõõdetud muutujate põhjal. kui pidevad x-id transformeerida log skaalasse, siis on lineaarsesse mudelisse pandud efektid multiplikatiivsed, mitte aditiivsed. "],
["kaks-lineaarse-mudeli-laiendust.html", "5 Kaks lineaarse mudeli laiendust Mitme sõltumatu prediktoriga mudel Interaktsioonimudel", " 5 Kaks lineaarse mudeli laiendust library(tidyverse) library(scatterplot3d) library(viridis) library(ggeffects) library(broom) library(car) Mitme sõltumatu prediktoriga mudel Esiteks vaatame mudelit, kus on mitu prediktorit \\(x_1\\), \\(x_2\\), … \\(x_n\\), mis on aditiivse mõjuga. See tähendab, et me liidame nende mõjud, mis omakorda tähendab, et me usume, et \\(x_1\\) … \\(x_n\\) mõjud y-i väärtusele on üksteisest sõltumatud. Mudel on siis kujul \\[y = a + b_1x_1~ + b_2x_2~ +~ ... +~ b_nx_n\\] Mitme prediktoriga mudeli iga prediktori tõus (beta koefitsient) ütleb, mitme ühiku võrra ennustab mudel y muutumist juhul kui see prediktor muutub ühe ühiku võrra ja kõik teised prediktorid ei muutu üldse (Yule, 1899). Kui meie andmed on kolmedimensionaalsed (me mõõdame igal mõõteobjektil kolme muutujat) ja me tahame ennnustada ühe muutuja väärtust kahe teise muutuja väärtuste põhjal (meil on kaks prediktorit), siis tuleb meie kolme parameetriga lineaarne regressioonimudel tasapinna kujul. Kui meil on kolme prediktoriga mudel, siis me liigume juba neljamõõtmelisse ruumi. Joonis 5.1: Regressioonitasand 3D andmetele. Kahe prediktoriga mudel, kus Sepal.Length ja Petal.Length on prediktorid ja Sepal.Width ennustatav muutuja. Seda mudelit saab kaeda 2D ruumis, kui kollapseerida kolmas mõõde konstandile. p &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + xlim(4, 8) + scale_color_viridis(discrete = TRUE) + theme(title = element_text(size = 8)) p1 &lt;- p + geom_abline(intercept = coef(m2)[1], slope = coef(m2)[2]) + labs(title = deparse(formula(m2))) m1 &lt;- lm(Sepal.Width ~ Sepal.Length, data = iris) p2 &lt;- p + geom_abline(intercept = coef(m1)[1], slope = coef(m1)[2]) + labs(title = deparse(formula(m1))) devtools::source_gist(&quot;8b4d6ab6a333ef1cd14e8067c3badbae&quot;, filename = &quot;grid_arrange_shared_legend.R&quot;) grid_arrange_shared_legend(p1, p2) Joonis 5.2: 2D-le kollapseeritud graafiline kujutus 3D andmete põhjal fititud mudelist. Vasemal, muutuja Petal.Length on kollapseeritud konstandile. Siin on regressioonijoon hoopis teises kohas, kui lihtsas ühe prediktoriga mudelis (paremal). Võrreldes mudelite m1 (üks prediktor) ja m2 (kaks prediktorit) Sepal.Length (\\(b_1\\)) koefitsienti on näha, et need erinevad oluliselt. coef(m1) #&gt; (Intercept) Sepal.Length #&gt; 3.4189 -0.0619 coef(m2) #&gt; (Intercept) Sepal.Length Petal.Length #&gt; 1.038 0.561 -0.335 Kumb mudel on siis parem? AIC-i järgi on m2 kõvasti parem kui m1, lisakoefitsendi (Petal.Length) kaasamisel mudelisse paranes oluliselt selle ennustusvõime. AIC(m1, m2) #&gt; df AIC #&gt; m1 3 179.5 #&gt; m2 4 92.1 Ennustused sõltumatute prediktoritega mudelist Siin on idee kasutada fititud mudeli struktuuri ennustamaks y keskmisi väärtusi erinevatel \\(x_1\\) ja \\(x_2\\) väärtustel. Kuna mudel on fititud, on parameetrite väärtused fikseeritud. ## New sepal length values Sepal_length &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), length.out = 10) ## Keep new petal length constant Petal_length &lt;- mean(iris$Petal.Length) ## Extract model coeficents a &lt;- coef(m2)[&quot;(Intercept)&quot;] b1 &lt;- coef(m2)[&quot;Sepal.Length&quot;] b2 &lt;- coef(m2)[&quot;Petal.Length&quot;] ## Predict new sepal width values Sepal_width_predicted &lt;- a + b1 * Sepal_length + b2 * Petal_length plot(Sepal_width_predicted ~ Sepal_length, type = &quot;b&quot;, ylim = c(0, 5), col = &quot;red&quot;) # Prediction from the single predictor model abline(m1, lty = &quot;dashed&quot;) Joonis 5.3: Ennustatud y väärtused erinevatel \\(x_1\\) väärtustel kui \\(x_2\\) on konstantne, punane joon. Katkendjoon, ühe prediktoriga mudeli ennustus. Nüüd joonistame 3D pildi olukorrast, kus nii x1 kui x2 omandavad rea väärtusi. Mudeli ennustus on ikkagi sirge kujul – mis sest, et 3D ruumis. Joonis 5.4: Kahe prediktoriga mudeli ennustus 3D ruumis. Interaktsioonimudel Interaktsioonimudelis sõltub ühe prediktori mõju teise prediktori väärtusest: \\[y = a + b_1x_1 + b_2x_2 + b_3x_1x_2\\] Sageli on nii, et prediktoreid, mille mõju y-le on suur, tasub mudeldada ka interaktsioonimudelis (näiteks suitsetamise mõju vähimudelites kipub olema interaktsiooniga). Interaktsioonimudeli koefitsientide tõlgendamine on keerulisem. b1 on otse tõlgendatav ainult siis, kui x2 = 0 (ja b2 ainult siis, kui x1 = 0). Samas, kui interaktsioonimudel fititakse standardiseeritud x-muutujate peal, mille keskväärtus = 0, siis muutub koefitsientide tõlgendamine lihtsamaks - b1 tõlgendame x2 keskväärtusel (ja vastupidi, b2 x1 keskväärtusel). Edaspidi õpime selliseid mudeleid graafiliselt tõlgendama, kuna koefitsientide otse tõlgendamine ei ole siin sageli perspektiivikas. Interaktsioonimudelis sõltub x1 mõju tugevus y-le x2 väärtusest. Selle sõltuvuse määra kirjeldab b3 (x1 ja x2 interaktsiooni tugevus). Samamoodi ja sümmeetriliselt erineb ka x2 mõju erinevatel x1 väärtustel. Ainult siis, kui x2 = 0, ennustab x1 tõus 1 ühiku võrra y muutust b1 ühiku võrra. Interaktsioonimudeli 2D avaldus on kurvatuuriga tasapind, kusjuures kurvatuuri määrab b3. Interaktsiooniga mudel on AIC-i järgi pisut vähem eelistatud võrreldes kahe prediktoriga mudeliga m2. Seega, eriti lihtsuse huvides, eelistame m2-e. m3 &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Length + Sepal.Length * Petal.Length, data = iris) AIC(m1, m2, m3) #&gt; df AIC #&gt; m1 3 179.5 #&gt; m2 4 92.1 #&gt; m3 5 93.4 Ennustused interaktsioonimudelist Kõigepealt anname rea väärtusi x1-le ja hoiame x2 konstantsena. Petal_length &lt;- mean(iris$Petal.Length) a &lt;- coef(m3)[&quot;(Intercept)&quot;] b1 &lt;- coef(m3)[&quot;Sepal.Length&quot;] b2 &lt;- coef(m3)[&quot;Petal.Length&quot;] b3 &lt;- coef(m3)[&quot;Sepal.Length:Petal.Length&quot;] Sepal_width_predicted &lt;- a + b1 * Sepal_length + b2 * Petal_length + b3 * Sepal_length * Petal_length plot(Sepal_width_predicted ~ Sepal_length, type = &quot;l&quot;, ylim = c(2, 6)) abline(coef(m2)[c(&quot;(Intercept)&quot;, &quot;Sepal.Length&quot;)], lty = &quot;dashed&quot;) Joonis 5.5: Ennustus interaktsioonimudelist, kus x1 (Sepal_Length) on antud rida väärtusi ja x2 (Petal_length) hoitakse konstantsena (pidevjoon). Interaktsioonimudeli regressioonijoon on paraleelne ilma interaktsioonita mudeli ennustusele (katkendjoon). Nagu näha viib korrutamistehe selleni, et interaktsioonimudeli tõus erineb ilma interaktsioonita mudeli tõusust. Kui aga interaktsioonimudel plottida välja 3D-s üle paljude x1 ja x2 väärtuste, saame me regressioonikurvi (mitte sirge), kus b3 annab kurvatuuri. Joonis 5.6: Ennustused 3D interaktsioonimudelist üle paljude x1 (Sepal_Length) ja x2 (Petal_length) väärtuste. Vau! See on alles ennustus! "],
["vahimruutude-meetodiga-fititud-mudelite-toovoog-lm.html", "6 Vähimruutude meetodiga fititud mudelite töövoog – lm() 6.1 1. vaatame mudeli koefitsiente 6.2 2. Testime mudeli eeldusi 6.3 Residuaalid y ja x muutujate vastu 6.4 Teeme mudeli põhjal ennustusi (marginal plots)", " 6 Vähimruutude meetodiga fititud mudelite töövoog – lm() Kuna lm() funktsiooniga ja bayesi meetodil fititud mudeliobjektidega töötamine on mõnevõrra erinev, õpetame seda eraldi. Siinkohal anname põhilise töövoo lm() mudelobjektide inspekteerimiseks. Töötame m3 mudeliobjektiga, mis on interaktsioonimudel: Sepal.Width ~ Sepal.Length * Species ehk \\[Speal.Width = a + b_1*Sepal.Length + b_2*Species + b_3*Sepal.Length*Species\\] library(ggeffects) m3 &lt;- lm(Sepal.Width ~ Sepal.Length * Species, data = iris) 6.1 1. vaatame mudeli koefitsiente tidy(m3) #&gt; # A tibble: 6 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -0.569 0.554 -1.03 3.06e- 1 #&gt; 2 Sepal.Length 0.799 0.110 7.23 2.55e-11 #&gt; 3 Speciesversicolor 1.44 0.713 2.02 4.51e- 2 #&gt; 4 Speciesvirginica 2.02 0.686 2.94 3.85e- 3 #&gt; 5 Sepal.Length:Speciesversicolor -0.479 0.134 -3.58 4.65e- 4 #&gt; 6 Sepal.Length:Speciesvirginica -0.567 0.126 -4.49 1.45e- 5 Interaktsioonimudeli koefitsientide jõllitamine on sageli tühi töö ja vaimu närimine. Õnneks on meil muid meeotodeid, kuidas lm() mudelitega töötada. Võrdluseks - nii fitime eraldi mudeli igale irise liigile. Tulemus on tegelikult identne interaktsioonimudeliga kategoorilisele muutujale (Species), aga koefitsiendid on otse tõlgendatavad. Samas, interaktsioonimudelit saab fittida ka pidevale muutujale! iris %&gt;% split(.$Species) %&gt;% map(~ lm(Sepal.Width ~ Sepal.Length, data = .)) %&gt;% map(summary) %&gt;% map_dfr(~ broom::tidy(.), .id = &quot;Species&quot;) #&gt; # A tibble: 6 x 6 #&gt; Species term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 setosa (Intercept) -0.569 0.522 -1.09 2.81e- 1 #&gt; 2 setosa Sepal.Length 0.799 0.104 7.68 6.71e-10 #&gt; 3 versicolor (Intercept) 0.872 0.445 1.96 5.56e- 2 #&gt; 4 versicolor Sepal.Length 0.320 0.0746 4.28 8.77e- 5 #&gt; 5 virginica (Intercept) 1.45 0.431 3.36 1.55e- 3 #&gt; 6 virginica Sepal.Length 0.232 0.0651 3.56 8.43e- 4 Adjusteeritud r2 tasub eraldi üle vaadata. summary(m3)$adj.r.squared #&gt; [1] 0.61 0.61 tähendab, et mudel suudab seletada mitte rohkem kui 61% y-muutuja (Sepal.Width) varieeruvusest. 6.2 2. Testime mudeli eeldusi Nii saab fititud väärtused (.fitted), residuaalid (.resid), fittitud väätruste standardvead (.se.fit). Residuaal = Y data value - fitted value. Seega positiivne residuaal näitab, et mudeli ennustus keskmisele y väärtusele mingil x-muutujate väärtusel on madalam kui juhutb olema tegelik y-i andmepunkti väärtus. See võib olla tingitud y-muutuja normaalsest bioloogilisest varieeruvusest, aga ka sellest, et mudel ei kirjelda täiuslikult x-ide ja y tegelikku seost. (a_m3 &lt;- augment(m3)) #&gt; # A tibble: 150 x 10 #&gt; Sepal.Width Sepal.Length Species .fitted .se.fit .resid .hat .sigma #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3.5 5.1 setosa 3.50 0.0399 -0.00306 0.0215 0.273 #&gt; 2 3 4.9 setosa 3.34 0.0403 -0.343 0.0218 0.272 #&gt; 3 3.2 4.7 setosa 3.18 0.0512 0.0163 0.0354 0.273 #&gt; 4 3.1 4.6 setosa 3.10 0.0591 -0.00380 0.0471 0.273 #&gt; 5 3.6 5 setosa 3.42 0.0385 0.177 0.0200 0.273 #&gt; 6 3.9 5.4 setosa 3.74 0.0581 0.157 0.0455 0.273 #&gt; # ... with 144 more rows, and 2 more variables: .cooksd &lt;dbl&gt;, #&gt; # .std.resid &lt;dbl&gt; .hat &gt;1 sugereerib high leverage andmepunkte .std.resid on studentiseeritud residuaal, mis on sd ühikutes (.resid/sd(.resid)) 6.2.1 Lineaarsus - residuaalid~fitted plot Residuals vs fitted plot testib lineaarsuse eeldust - kui .resid punktid jaotuvad ühtlaselt nulli ümber, siis mudel püüab kinni kogu süstemaatilise varieeruvuse teie andmetest ja see mis üle jääb on juhuslik varieeruvus. ggplot(a_m3, aes(`.fitted`, `.resid`)) + geom_point(aes(color=Species), alpha=0.5) + geom_smooth() 6.2.2 Cooki kaugus - outlierid .cooksd on Cook-i kaugus, mis näitab võimalikke outliereid. rusikareeglina tähendab cooksd &gt; 3 cooksd keskväärtust, et tegu võiks olla outlieriga. Teine võimalus on pidada igat punkti, mis on kõrgem kui 4/n, outlieriks. Kolmanadad arvavad jälle, et outlierina võiks vaadelda iga teistest väga erinevat väärtust, või et .cooksd &gt; 1 v .cooksd &gt; 0.5 on indikatsiooniks nn mõjukast väärtusest (influencial value). ggplot(data = NULL, aes(x = 1:150, y = a_m3$`.cooksd`)) + geom_col() + geom_hline(yintercept = 4/150)+ geom_hline(yintercept = 3*mean(a_m3$`.cooksd`), lty = 2) 6.2.3 Mõjukuse plot outlierid - studentideeritud residuaalid &gt; 2 või &lt; -2 hat &gt; 1 - sugereerib high leverage andmepunkte library(car) influencePlot(m3, id.method=&quot;identify&quot;, main=&quot;Influence Plot&quot;, sub=&quot;Circle size is proportional to Cook&#39;s distance&quot;) #&gt; StudRes Hat CookD #&gt; 15 -0.243 0.1236 0.00139 #&gt; 42 -2.810 0.0621 0.08307 #&gt; 69 -2.477 0.0253 0.02567 #&gt; 107 -0.331 0.1638 0.00359 #&gt; 119 -2.464 0.0824 0.08782 Mõjukad punktid (Influential observations) omavad suurt mõju mudeli ennustustele. High Leverage andmepunktid on x-muutujate ekstreemsed punktid, mille lähedal ei ole n-mõõtmelises ruumis (kui teil on n x-muutujat) teisi punkte. Seetõttu läheb fititud mudel nende punktide lähedalt mõõda. Mõjukad punktid on tüüpiliselt ka high leverage punktid, kuid vastupidine ei kehti! 6.2.4 Residuaalide normaalsus - qq plot Kas residuaalid on normaaljaotusega? car::qqPlot(a_m3$`.std.resid`, distribution = &quot;norm&quot;) #&gt; [1] 42 69 6.2.5 Homoskedastilisus - Scale-location plot Scale-location plot - homoskedastilisuse eeldust ehk seda, et varieeruvus ei sõltuks prediktormuutuja väärtusest. Y-teljel on ruutjuur studentiseeritud residuaalide absoluutväärtusest ggplot(a_m3, aes(`.fitted`, `.resid` %&gt;% abs %&gt;% sqrt)) + geom_point(aes(color=Species), alpha=0.5) + ylab(&quot;square root of absolute residual&quot;)+ geom_smooth(se = FALSE) 6.3 Residuaalid y ja x muutujate vastu Kõigepealt residuaalid y-muutja vastu ggplot(a_m3, aes(Sepal.Width, `.std.resid`)) + geom_point(aes(color=Species)) + geom_hline(yintercept = 0, lty =2, color =&quot;darkgrey&quot;) + geom_smooth( se=F, color=&quot;black&quot;, size=0.5) Mudel paistab süstemaatiliselt alahindama Sepal Width-i seal kus Sepal Length on kõrge, ja vastupidi. Horisontaalne punktiirjoon näitab, kus mudel vastab täpselt andmetele. Studentiseeritud residuaalid sd ühikutes Ja nüüd residuaalid x-muutuja vastu. ggplot(a_m3, aes(Sepal.Length, `.std.resid`, color=Species)) + geom_point() + geom_hline(yintercept = 0, lty =2, color =&quot;darkgrey&quot;)+ geom_smooth(se=F, color=&quot;black&quot;, size=0.5) Ideaalsed residuaalid! 6.4 Teeme mudeli põhjal ennustusi (marginal plots) Me ennustame Y-i keskmisi väärtuseid etteantud X-i väärtustel. ggpredict() ennustab y-muutuja väärtusi ühe x-muutuja väärtuste järgi, hoides kõiki teisi x-muutujaid konstantsena. Kõigepealt võrdleme lihtsa 1 prediktoriga mudeli ennustust kahe prediktoriga mudeli ennustusega lm1 &lt;- lm(Sepal.Width ~ Sepal.Length, data = iris) lm2 &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Length, data = iris) mydf &lt;- ggpredict(lm1, terms = &quot;Sepal.Length&quot;) mydf2 &lt;- ggpredict(lm2, terms = &quot;Sepal.Length&quot;) ggplot(mydf, aes(x, predicted)) + geom_line() + geom_ribbon(data = mydf, aes(ymin = conf.low, ymax = conf.high), alpha = 0.5, fill=&quot;lightgrey&quot;) + geom_line(data = mydf2, aes(x, predicted), lty=2)+ geom_ribbon(data = mydf2, aes(ymin = conf.low, ymax = conf.high), alpha = 0.5, fill=&quot;lightgrey&quot;) + geom_point(data=iris, aes(Sepal.Length, Sepal.Width, color=Species)) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;predicted sepal width&quot;)+ theme_classic() terms argument võtab kuni 3 muutujat, neist 2 peavad olema faktormuutujad ja 3 muutuja korral tekib tabelisse veerg nimega facet, mille abil saab tulemused facet_wrap()-ga välja plottida. mydf &lt;- ggpredict(m3, terms = c(&quot;Sepal.Length&quot;, &quot;Species&quot;)) ggplot(mydf, aes(x, predicted)) + geom_line(aes(color=group)) + geom_point(data=iris, aes(Sepal.Length, Sepal.Width, color=Species)) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;predicted sepal width&quot;) Nii saab sisestada üksikuid parameetriväärtusi ja neile ennustusi teha: (mydf1 &lt;- ggpredict(m3, terms = c(&quot;Sepal.Length [5, 22]&quot;, &quot;Species [setosa, versicolor]&quot;))) #&gt; #&gt; # Predicted values for Sepal.Width #&gt; # x = Sepal.Length #&gt; #&gt; # setosa #&gt; x predicted conf.low conf.high #&gt; 5 3.42 3.35 3.5 #&gt; 22 17.00 13.32 20.7 #&gt; #&gt; # versicolor #&gt; x predicted conf.low conf.high #&gt; 5 2.47 2.31 2.63 #&gt; 22 7.91 5.53 10.28 6.4.1 4. Võrdleme mudeleid Eeldus - kõik võrreldavad mudelid on fititud täpselt samade andmete peal. Eeldus (ei ole vajalik AIC meetodi puhul) - tegemist on nn nested mudelitega. Nested mudel tähendab, et kõik väiksema mudeli liikmed on olemas ka suuremas mudelis. Mudelite võrdlus ANOVA-ga (ainult nested mudelid) tidy(anova(lm1, lm2, m3)) #&gt; Warning: Unknown or uninitialised column: &#39;term&#39;. #&gt; # A tibble: 3 x 6 #&gt; res.df rss df sumsq statistic p.value #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 148 27.9 NA NA NA NA #&gt; 2 147 15.4 1 12.5 169. 4.83e-26 #&gt; 3 144 10.7 3 4.71 21.2 2.06e-11 Mudelite võrdlus AIC-ga AIC(lm1, lm2, m3) #&gt; df AIC #&gt; lm1 3 179.5 #&gt; lm2 4 92.1 #&gt; m3 7 43.3 AIC (Akaike Informatsiooni Kriteerium) on number, mis püüab tabada mõistlikku tasakaalu mudeli fiti valimiandmetega ja parsinoomia vahel. Väiksema AIC-ga mudel on eelistatud suurema AIC-ga mudeli ees (samas, AIC-l kui ühel arvul puudub tõlgendus). Probleem AIC-i taga on selles, et parem fit valimiandmetega võib tähendada mudeli ülefittimist (ja seega halvemat mudelit). Kuna ülefittimise tõenäosus kasvab koos mudeli keerukusega (parameetrite arvuga), eelistame võimalikult lihtsat mudelit, mis samas seletaks võimalikult suure osa valimiandmete varieeruvusest. "],
["veamudel.html", "7 Veamudel 7.1 Lihtne varieeruvuse mudel 7.2 protsessimudel ja veamudel lineaarses regressioonis Enimkasutatud veamudel on normaaljaotus Normaaljaotuse ja lognormaaljaotuse erilisus 7.3 Teised veamudelid", " 7 Veamudel library(tidyverse) library(brms) library(broom) 7.1 Lihtne varieeruvuse mudel Oletame, et me oleme mõõtnud nelja patsienti ja saanud tulemuseks 1.2, 2.12, 1.4 ja 8.34. Kuidas me oma valimit iseloomustame ja kas me peaksime 4. tulemuse kahtlasena välja viskama? Arvatavasti tahaksime saada hinnangut kõige tõenäolisemale mõõtetulemusele patsientide populatsioonis ehk siis keskmise või tüüpilise patsiendi väärtusele, mis on kõik sisuliselt sama. Ja lisaks ka hinnangut patsientide vahelise varieeruvuse määrale (meid võib huvitada võrrelda varieeruvust patsientide ja tervete inimeste vahel). Esmapilgul tundub see lihtsa ülesandena, mis ei vaja mudeldamist – lihtsalt arvutame aritmeetilise keskmise ja standardhälbe ja meil on mõlemad hinnangud olemas. Aga tegelikult oleme probleemi ees, millele pole ühte õiget lahendust. Kui me viskame 4. tulemuse välja, siis tuleb meie keskmine kuhugi 1.5 kanti, muidu aga läheb see piirkonda, mille lähedal meil ei ole ühtegi andmepunkti. Samuti annaks sd arvutus üsna erinevad tulemused. Kumb võimalus siis valida? Selleks peame ikkagi otsustama, kuidas modelleerida oma andmed. Arvestades looduslikku protsessi, mis need andmed genereeris (ja mille ma jätsin lahtiseks), võiks andmete mudel olla näiteks normaaljaotus, lognormaaljaotus, cauchy jaotus vms. Kui valime normaaljaotuse, millise õlad laskuvad väga kiiresti, siis on vaid väike tõenäosus kohata tervelt veerandit oma andmepunktidest nõnda kaugel teistest, mis omakorda annab põhjust selle punkti eemaldamiseks. Aga näiteks lognormaaljaotuse korral, mille õlg laskub palju aeglasemalt, on tõenäosus 4. mõõtmisest isegi kaugemal olevaid andmeid kohata palju suurem ja seega peaksime selle andmepunkti sisse jätma. Erinevat tüüpi mudelitel on erinevad parameetrid, millele andmete põhjal peaksime väärtusi otsima. See, et normaaljaotuse parameetrit \\(\\mu\\) saab meie näites arvutada aritmeetilise keskmise kaudu, ei tähenda, et ka teiste mudelite korral peaksime sama lokatsiooniparameetrit fittima (või et neil mudelitel üldse oleks lokatsiooniparameeter, mida fittida). Sarnased lood on muidugi ka varieeruvust iseloomustava parameetriga. Statistilist mudelit saab kasutada mitmel moel. Mudel toob sisse lisainformatsiooni andmete jaotuse kohta, mida valimiandmetes endis ei pruugi sisalduda, ja mis tõstab meie järelduste kvaliteeti (või langetab seda, kui valisime kehva mudeli). Võrreldes erinevat tüüpi mudelite sobivust andmetega ning omades aimu protsesside kohta, mida üks või teine mudel võiks adekvaatselt kirjeldada, on vahest võimalik teha järeldusi loodusliku mehhanismi kohta, mis genereeris andmed, mille põhjal mudelid fititi. Me võime fititud mudeli põhjal teha ennustusi, ehk genereerida uusi andmeid in silico. Niisiis lihtne mudel andmetele: \\(\\mu\\) ehk aritmeetiline keskmine kui hinnang kõige tõenäosemale väärtusele. See on deterministlik nn protsessimudel, kus samad valimiväärtused annavad alati sama ja ühese tulemuse. Statistiline mudel sisaldab endas nii protsessimudelit kui tõenäosuslikku nn varieeruvuse mudelit (ajaloolistel põhjustel kutsutakse seda sageli veamudeliks), mis tuleb sisse tõenäosusjaotuse kujul \\[dnorm(\\mu, \\sigma)\\] Selle mudeli on võimalik ümber sõnastada (seda seeläbi üldistades) lihtsa regressioonivõrrandina \\(y = b_0\\), kusjuures \\(\\mu = b_0\\) ehk andmete keskväärtus võrdub regressioonisirge interceptiga. Asendades saame \\[y \\sim dnorm(b_0, \\sigma)\\] Tilde \\(\\sim\\) tähistab seose tõenäosuslikkust, ehk seda, et y muutuja ennustuslikd väärtused tõmmatakse juhuvalimina normaaljaotusest, mis omakorda on fititud empiiriliste väärtuste (ehk valimi) põhjal. Seega on meil normaaljaotuse keskväärtus võimalik leida aritmeetilise keskmisena või samaväärselt vähimruutude meetodiga, mis paneb keskväärtuse kohta, kus keskväärtuse ja iga andmepunkti vahelise kauguste ruutude summa tuleb minimaalne. Vähimruutude meetod on üldisem, sest töötab ka järgmises peatükis, kus me asendame \\(\\mu\\) terve regressioonivõrrandiga kujul \\(y = b_0 + b_1x_1 + b_2x_2 + ... + b_ix_i\\) (protsessimudel). Ja kui meie regressioonivõrrandid lähevad mittelineaarseks ja vähimruutude meetod nende fittimisel enam ei tööta, siis veel üldisem meetod, Bayesi teoreem, töötab ikka. Kuigi aritmeetiline keskmine ja vähimruutude meetod annavad sama hinnangu lokatsiooniparameetrile, ei ütle need midagi sigma kohta. Samas Bayesi meetod annab hinnangu (koos usaldusintervalliga) mõlemale parameetrile. Normaaljaotus mudeldab lokalisatsiooniparameetrit mu populatsiooni tüüpilise või keskmise liikme hinnanguna ja varieeruvusparameetrit sigma populatsiooni liikmete vaheliste erinevuste määra hinnanguna. Arvutame lihtsa mudeli läbi vähimruutude meetodiga ja Bayesi meetodiga set.seed(1234321) andmed &lt;- tibble(a= rnorm(4)) plot(andmed) mean(andmed$a); sd(andmed$a) #&gt; [1] 1.24 #&gt; [1] 0.662 Vähimruutude meetodit rakendab lm() funktsioon lm(a~1, data = andmed) %&gt;% broom::tidy() #&gt; # A tibble: 1 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 1.24 0.331 3.74 0.0333 Ja Bayesi brms::brm() (Bayes_mudel &lt;- brm(a~1, data = andmed) %&gt;% broom::tidy()) #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error lower upper #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 b_Intercept 1.24 0.684 0.196 2.25 #&gt; 2 sigma 1.27 1.24 0.461 3.02 Nagu näete, lm() fitib ainult mu parameetri, samas kui me Bayesi meetodit kasutades saame hinnangu (koos usalduspiiridega) kahele parameetrile: mu ehk intercept ja sigma ehk sd. Meie poolt simuleeritud andmed tulevad normaaljaotusega populatsioonist, mille mu = 0 ja sd = 1. Kumbki meetod ei luba meile null-intercepti sest andmeid on vähe ja need on juhusliku valimivea tõttu kallutatud. See-eest sigma hinnang, mille Bayes meile annab on küll laiavõitu (ikka sellepärast, et meil on vähe andmeid), aga vähemalt hõlmab endas õiget väärtust. 7.2 protsessimudel ja veamudel lineaarses regressioonis Kui mudel \\(kaal = b_0 + b_1 ~pikkus\\) ennustab, et 160 cm inimene kaalub keskmiselt 80 kg, siis protsessi mudel ei ütle, kui suurt pikkusest sõltumatut kaalude varieeruvust võime oodata 160 cm-ste inimeste hulgas. Selle hinnangu andmiseks tuleb mudelile lisada varieeruvusekomponent, sageli normaaljaotuse kujul, mis modelleerib üksikute inimeste kaalude varieeruvust (mitte keskmise kaalu varieeruvust) igal mõeldaval ja mittemõeldaval pikkusel. Bioloogid, erinevalt füüsikutest, usuvad, et valimisisene andmete varieeruvus on tingitud pigem bioloogilisest varieeruvusest kui mõõtmisveast. Aga loomulikult sisaldub selles ka mõõtmisviga. Lihtsuse huvides räägime edaspidi siiski veamudelist, selle asemel, et öelda “varieeruvuse ja veamudel”. Kuidas veakomponent lineaarsesse mudelisse sisse tuua? Ilma veakomponendita mudel: \\[y = b_0 + bx\\] ennustab y-i keskväärtust erinevatel x-i väärtustel. Veakomponent: \\[y\\sim dnorm(\\mu,~\\sigma)\\] kus \\(\\mu\\) (mu) on mudeli poolt ennustatud keskväärtus ja \\(\\sigma\\) (sigma) on mudeli poolt ennustatud standardhälve ehk varieeruvus andmepunktide tasemel. Veamudelis on keskväärtuse ehk mu ennustus endiselt deterministlik ja sigma töötab originaalsel andmetasemel, mitte keskväärtuste tasemel. See võimaldab protsessimudeli veamudelisse sisse kirjutada lihtsalt mu ümber defineerides: \\[\\mu = b_0 + bx\\] mis tähendab, et \\[y \\sim dnorm(b_0 + b_1x, ~\\sigma)\\] See ongi sirge mudel koos veakomponendiga. Seega on sellel lineaarsel regressioonimudelil kolm parameetrit: intercept \\(b_0\\), tõus \\(b_1\\) ja “veaparameeter” \\(\\sigma\\). Sellist mudelit on mõistlik fittida Bayesi teoreemi abil. Bayesi meetodiga fititud mudel, mida kutsutakse posteerioriks, näitab, millised kombinatsioonid nendest kolmest parameetrist usutavalt koos esinevad, ja millised mitte. Seega on fititud 3 parameetriga bayesi mudel 3-dimensionaalne tõenäosusjaotus (3D posteerior). Muidugi saame ka ükshaaval välja plottida kolm 1D posteeriori, millest igaüks iseloomustab üht parameetrit ning on kollapseeritud üle kahe ülejäänud parameetri. Edaspidi õpime selliste mudelitega töötama. Kõik statistilised mudelid on tõenäosusmudelid ning sisaldavad veakomponenti. Kuna erinevalt lokatsiooniparameetrist, ei aja me mudelis sigmat lahku vastavalt x-i väärtustele, siis veamudel (ja enamus veamudeleid, millega me edaspidi töötame) modelleerivad igale x-i väärtusele (kaalule) samasuure y-i suunalise varieeruvuse (pikkuste sd). Suurem osa statistikast kasutab eeldusi, mida keegi päriselt tõe pähe ei võta, aga millega on arvutuslikus mõttes lihtsam elada. Enimkasutatud veamudel on normaaljaotus Alustuseks simuleerime lihtsate vahenditega looduslikku protsessi, mille tulemusel tekib normaaljaotus. Oletame, et bakteri kasvukiirust mõjutavad 12 geeni, mille mõjud võivad olla väga erineva tugevusega, kuid mille mõjude suurused ei sõltu üksteisest. Seega nende 12 geeni mõjud kasvukiirusele liituvad. Järgnevas koodis võtame 12 juhuslikku arvu 1 ja 100 vahel (kasutades runif() funktsiooni). Need 12 arvu näitavad 12 erineva geeni individuaalsete mõjude suurusi bakteritüve kasvukiirusele. Meil on seega kuni 100-kordsed erinevused erinevate geenide mõjude suuruste vahel. Seejärel liidame need 12 arvu. Nüüd võtame uue 12-se valimi ja kordame eelnevat. Me teeme seda 10 000 korda järjest ja plotime saadud 10 000 arvu (10 000 liitmistehte tulemust) tihedusfuntksioonina. kasv &lt;- replicate(10000, sum(runif(12, 1, 100))) p &lt;- ggplot(tibble(kasv), aes(kasv)) + geom_density() p Joonis 7.1: Normaaljaotus tekib sõltumatutest efektidest. Kümne tuhande N = 12 suuruse juhuvalimi summa tihedusdiagramm. Selles näites võrdub iga andmepunkt 10 000st ühe bakteritüve kasvukiiruse mõõtmisega. Seega, antud eelduste korral on bakteritüvede kasvukiirused normaaljaotusega. Nüüd vaatame, mis juhtub, kui 12 geeni mõjud ei ole üksteisest sõltumatud. Kui 12 geeni on omavahel vastasmõjudes, siis nende geenide mõjud korrutuvad, mitte ei liitu. (Korrutamine pole ainus viis, kuidas vastasmõjusid modeleerida, küll aga kõige levinum.) Kõigepealt vaatleme juhtu, kus 12 geeni on kõik väikeste mõjudega ning seega mitte ühegi geeni mõju ei domineeri teiste üle. Seekord genreerime 12 juhuslikku arvu 1 ja 1.1 vahel. Siin tähendab arv 1.1 kasvu tõusu 10% võrra. Seejärel korrutame need 12 arvu, misjärel kordame eelnevat 10 000 korda. kasv &lt;- replicate(10000, prod(runif(12, 1, 1.1))) p %+% tibble(kasv) Joonis 7.2: Normaaljaotus tekib väikestest sõltuvatest efektidest. Kümne tuhande N = 12 suuruse juhuvalimi korrutiste tihedusdiagramm. Ühegi geeni mõju ei domineeri teiste üle. Tulemuseks on jällegi normaaljaotus. Selles näites olid üksikud interakteeruvad geenid ükshaaval väikeste mõjudega ja ühegi geeni mõju ei domineerinud teiste üle. Mis juhtub, kui mõnel geenil on kuni 2 korda suurem mõju kui teisel? kasv &lt;- replicate(10000, prod(runif(12, 1, 2))) p %+% tibble(kasv) Joonis 7.3: Lognormaaljaotus tekib suurematest sõltuvatest efektidest. Kümne tuhande N = 12 suuruse juhuvalimi korrutiste tihedusdiagramm. Mõnel geenil on kuni 2 korda suurem mõju kui teisel. Nüüd on tulemuseks log-normaaljaotus. Mis teie arvate, kas teie poolt uuritavat tunnust mõjutavad faktorid, mis omavahel ei interakteeru või kui interakteeruvad, on kõik ühtlaselt väikeste efektidega? Või on tegu vastasmõjudes olevate faktoritega, millest osad on palju suuremate mõjudega, kui teised? Ühel juhul eelistate te normaaljaotust, teisel juhul peate õppima töötama ka lognormaaljaotusega. Kui me vaatame samu andmeid logaritmilises skaalas, avastame, et need andmed on normaaljaotusega. See ongi andmete logaritmimise mõte. kasv &lt;- replicate(10000, log10(prod(runif(12, 1, 2)))) p %+% tibble(kasv) + labs(x = &quot;kasv, log10&quot;) Joonis 7.4: Logaritmilises skaalas lognormaalsed efektid on normaaljaotusega. Kümne tuhande N = 12 suuruse juhuvalimi korrutiste tihedusdiagramm. Mõnel geenil on kuni 2 korda suurem mõju kui teisel. Normaaljatuse avastas Gauss (1809), aga nime andis sellele Francis Galton (1860ndatel), kuna antropoloogilised mõõtmised “normaalselt” järgisid “vigade seadust”, mille ta nimetas “Normaalseks jaotuste kurviks”. Normaaljaotuse mudel väikestel valimitel Oletame, et meil on kolm andmepunkti ning me usume, et need andmed on juhuslikult tõmmatud normaaljaotusest või sellele lähedasest jaotusest. Normaaljaotuse mudelit kasutades me sisuliselt deklareerime, et me usume, et kui me oleksime olnud vähem laisad ja 3 mõõtmise asemel sooritanuks 3000, siis need mõõtmised sobituksid piisavalt hästi meie 3 väärtuse peal fititud normaaljaotusega. Seega, me usume, et omades 3 andmepunkti me teame juba umbkaudu, millised tulemused me oleksime saanud korjates näiteks 3 miljonit andmepunkti. Oma mudelist võime simuleerida ükskõik kui palju andmepunkte. Aga pidage meeles, et selle mudeli fittimiseks kasutame me ainult neid andmeid, mis meil päriselt on — ja kui meil on ainult 3 andmepunkti, on tõenäoline, et fititud mudel ei kajasta hästi tegelikkust. Halvad andmed ei anna kunagi head tulemust. Eelnev ei kehti Bayesi mudelite kohta, mis toovad priorite kaudu sisse lisainfot, mis ei kajastu valimiandmetes ja võib analüüsi päästa. Kuidas panna skeptik uskuma, et statistilised meetodid töötavad halvasti väikestel valimitel? Järgnevalt illustreerime seda ühe võimaliku valimiga paljudest, mis on tõmmatud imaginaarsest populatsioonist, mille parameetreid me teame. Me tõmbame 3-se valimi ning üritame selle valimi põhjal ennustada selleasama populatsiooni struktuuri. Kuna tegemist on simulatsiooniga, teame täpselt, et populatsioon, kust me tõmbame oma kolmese valimi, on normaaljaotusega, et tema keskväärtus = 0 ja et tema sd = 1. Seega saame võrrelda oma ennustust populatsiooni tõeliste parameetriväärtustega. Me fitime oma valimiandmetega 2 erinevat mudelit: normaaljaotuse ja Studenti t jaotuse. Joonis 7.5: Juhuvalim normaaljaotusest, mille keskmine = 0 ja sd = 1 (n=3; andmepunktid on näidatud mustade munadena). Sinine joon - populatsioon, millest tõmmati valim; punane joon - normaaljaotuse mudel, mis on fititud valimi andmetel; must joon - Studenti t jaotuse mudel, mis on fititud samade andmetega. Mustad punktid, valim. Katkendjoon, populatsiooni keskmine, millest valim tõmmati. Siin saame hinnata mudelite fitte jumala positsioonilt, võrreldes fititud mudelite jaotusi “tõese” sinise jaotusega. Mõlemad mudelid on süstemaatiliselt nihutatud väiksemate väärtuste poole ja alahindavad varieeruvust. t jaotuse mudel on oodatult paksemate sabadega ja ennustab 0-st kaugele palju rohkem väärtusi kui normaaljaotuse mudel. Kuna me teame, et populatsioon on normaaljaotusega, pole väga üllatav, et t jaotus modeleerib seda halvemini kui normaaljaotus. Igal juhul, mõni teine juhuvalim annaks meile hoopis teistsugused mudelid, mis rohkem või vähem erinevad algsest populatsioonist. Mis juhtub kui me kasutame oma normaaljaotuse mudelit uute andmete simuleerimiseks? Kui lähedased on need simuleeritud andmed populatsiooni andmetega ja kui lähedased valimi andmetega, millega me normaaljaotuse mudeli fittisime? # tõmbame 3 juhuslikku arvu normaalhaotusest, mille keskväärtus = 0 ja sd = 1. dfr &lt;- tibble(sample_data = rnorm(3)) dfr &lt;- summarise_at(dfr, &quot;sample_data&quot;, c(&quot;mean&quot;, &quot;sd&quot;)) dfr #&gt; # A tibble: 1 x 2 #&gt; mean sd #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0654 0.808 # simuleerime 1000 uut andmepunkti fititud mudelist simulated_data &lt;- rnorm(1000, dfr$mean, dfr$sd) # arvutame simuleeritud andmete keskmise ja sd ning joonistame neist histogrammi ggplot(tibble(simulated_data), aes(simulated_data)) + geom_histogram(bins = 15) Joonis 7.6: Kasutame fititud mudeleid uute andmete simuleerimiseks. Nagu näha, igati ootuspäraselt on uute (simuleeritud) andmete keskväärtus ja SD väga sarnased algsete andmete omale, mida kasutasime mudeli fittimisel. Kahjuks ei ole need aga kaugeltki nii sarnased algsele jaotusele, mille kuju me püüame oma andmete ja mudeli pealt ennustada. Seega on meie mudel üle-fittitud, mis tähendab, et ta kajastab liigselt neid valimi aspekte, mis ei peegelda algse populatsiooni omadusi. Loomulikult ei vasta ükski mudel päriselt tegelikkusele. Küsimus on pigem selles, kas mõni meie mudelitest on piisavalt hea, et olla kasulik. Vastus sellele sõltub, milleks plaanime oma mudelit kasutada. mean(simulated_data &gt; 0) #&gt; [1] 0.535 mean(simulated_data &gt; 1) #&gt; [1] 0.116 Kui populatsiooniväärtustest on 50% suuremad kui 0, siis mudeli järgi vaevalt 32%. Kui populatsiooniväärtustest on 16% suuremad kui 1, siis mudeli järgi vaevalt 4%. See illustreerib hästi mudeli kvaliteeti. sim_t &lt;- rstudent_t(1000, 2, dfr$mean, dfr$sd) mean(sim_t &gt; 0) #&gt; [1] 0.516 mean(sim_t &gt; 1) #&gt; [1] 0.189 Samad ennustused t jaotusest on isegi paremad! Aga kumb on ikkagi parem mudel populatsioonile? Normaaljaotuse ja lognormaaljaotuse erilisus Normaaljaotus ja lognormaaljaotus on erilised sest kesksest piirteoreemist (central limit theorem) tuleneb, et olgu teie valim ükskõik millise jaotusega, paljudest valimitest arvutatud aritmeetilised keskmised on alati enam-vähem normaaljaotusega. See kehtib enamuse andmejaotuste korral, kui n&gt;30. Selle matemaatilise tõe peegeldus füüsikalisse maailma on “elementaarsete vigade hüpotees”, mille kohaselt paljude väikeste üksteisest sõltumatute juhuslike efektide (vigade) summa annab tulemuseks normaaljaotuse. Paraku enamus bioloogilisi mõõtmisi annavad tulemuseks eranditult mitte-negatiivseid väärtusi. Sageli on selliste väärtuste jaotused ebasümmeetrilised (v.a. siis, kui cv = sd/mean on väike), ja kui nii, siis on meil sageli tegu lognormaaljaotusega, mis tekkib log-normaalsete muutujate korrutamisest. Siit tuleb Keskne piirteoreem 2, mille kohaselt suvalise jaotusega muutujate geomeetrilised keskmised on enam-vähem lognormaaljaotusega, ning elementaarsete vigade hüpotees 2: Kui juhuslik varieeruvus tekib paljude juhuslike efektide korrutamisel, on tulemuseks lognormaaljaotus. Lognormaaljaotusega väärtuste logaritmimine annab normaaljaotuse. Nii normaal- kui lognormaaljaotus on maksimaalse entroopiaga jaotused. Entroopiat vaadeldakse siin informatsiooni &amp; müra kaudu — maksimaalse entroopiaga süsteem sisaldab maksimaalselt müra ja minimaalselt informatsiooni (vastavalt Shannoni informatsiooniteooriale). See tähendab, et väljaspool oma parameetrite tuunitud väärtusi on normaal- ja lognormaaljaotused minimaalselt informatiivsed. Normaaljaotusel ja lognormaaljaotusel on kummagil kaks parameetrit, mu ja sigma (ehk keskmine ja standardhälve), mille väärtused fikseerides fikseerime üheselt jaotuse ehk mudeli kuju, lisades sinna minimaalselt muud (sooviamtut) informatsiooni. Teised maksimaalse entroopiaga jaotused on näiteks eksponentsiaalne jaotus, binoomjaotus, bernoulli jaotus, poissoni jaotus. Kui meil on tegu nullist suuremate andmetega, on andmete logaritmimine sageli hea mõte. Logaritmitud andmete pealt arvutatud keskmise ja sd tagasi transformeerimine annab meile geomeetrilise keskmise ja geomeetrilise sd. Maksimaalsel entroopial põhineb normaaljaotuse ja lognormaaljaotuse sage kasutamine Bayesi statistikas prioritena, sest me suudame paremini kontrollida, millist informatsiooni me neisse surume. Esimesel kesksel piirteoreemil seevastu põhineb kogu sageduslik statistika (vt ptk 8.). 7.2.1 Normaaljaotuse ja lognormaaljaotuse võrdlus Normaaljaotus normaalsete juhuslike muutujate liitmine annab normaalse summa. Lineaarsed kombinatsioonid \\(Y= \\alpha + \\beta_1X_1 + \\beta_2X_2\\) jäävad normaalseks. normaalsete muutujate aritmeetilised keskmised on normaaljaotusega Keskne piirteoreem: mitte-normaalsete muutujate aritmeetilised keskmised on enam-vähem normaaljaotusega elementaarsete vigade hüpotees: kui juhuslik varieeruvus on paljude juhuslike mõjude summa, on tulemuseks normaaljaotus lm() regressionimudelid eeldavad normaaljaotusega residuaale. log-normaalsete vigadega lineaarset regresiooni tuleks teha mudeldades log(Y) (vähimruutude meetodil) või lognormaalset tõepäramudelit kasutades (bayesi regressioon, vt ptk 13). additiivne regressioonimudel viib additiivsete vigadeni (residuaalideni), mis omakorda viib konstantsele varieeruvusele ehk konstantsele SD-le. lognormaaljaotus lognormaalsete juhuslike muutujate korrutamine annab lognormaalse korrutise. Longnormaalsete muutujate geomeetrilised keskmised on lognormaaljaotusega. Keskne piirteoreem: mitte-lognormaalsete muutujate geomeetrilised keskmised on enam-vähem lognormaaljaotusega Multiplikatiivne elementaarsete vigade hüpotees: kui juhuslik varieeruvus on paljude juhuslike mõjude korrutis, on tulemuseks lognormaaljaotus multiplikatiivne regressioonimudel viib multiplikatiivsete vigadeni (residuaalideni), mis omakorda viib konstantsele suhtelisele varieeruvusele ehk konstantsele CV-le. Vigade jaotus on nüüd ebasümmeetriline. Seega võime lognormaaljaotust kutsuda ka multiplikatiivseks normaaljaotuseks. 7.3 Teised veamudelid 7.3.1 Lognormaaljaotus x &lt;- seq(0, 10, length.out = 1000) y &lt;- dlnorm(x) plot(x, y, typ = &quot;l&quot;) Seda jaotust, mis ei ulatu kunagi teisele poole nulli, iseloomustab, et x-i logaritmimine annab tulemuseks normaaljaotuse. plot(log(x), y, type = &quot;l&quot;) Lognormaaljaotuse keskväärtus, standardhälve, mood ja mediaan: \\[keskv\\ddot{a}\\ddot{a}rtus = \\exp(\\mu + 1/2 \\times σ^2)\\] \\[sd = \\exp(\\mu + 1/2 \\times \\sigma^2) \\times \\sqrt{\\exp(\\sigma^2) − 1}\\] \\[mood = e^{\\mu - \\sigma^2}\\] \\[mediaan = e^\\mu\\] Siin on siis \\(\\mu\\) ja \\(\\sigma\\) arvutatud logaritmitud andmete pealt. 7.3.2 Binoomjaotus Kui teil on binaarne muutuja (sellel saab olla ainult kaks väärtust, näiteks sees/väljas, 1/0), mis kajastab sõltumatuid sündmusi, siis modelleerib seda binoomjaotus \\(y ∼ Binomial(n, p)\\). Kus n on edukate sündmuste arv ja p on nende suhteline sagedus (p = n / N, kus N on kõikide sündmuste kopguarv). Sõltumatud sündmused on sellised, kus ühe sündmuse esinemise järgi ei saa ennustada teise sündmuse esinemist (st puudub korrelatsioon sündmuste esinemise vahel). Tehniliselt on binoomjaotusel veel omadus, et valim võetakse replacementiga, mis tähendab, et iga sündmus pannakse populatsiooni tagasi, kus seda saab uuesti valimisse tõmmata. Siit tuleb, et binoomjaotuse mudel kehtib päris maailmas mõõndustega ja et seda mudelit on kindlam kasutada siis, kui N &gt;&gt; n. Kui N on suur, siis meenutab binoomjaotus normaaljaotust (läheneb selle kujule). n &lt;- 10 # sündmuste koguarv x &lt;- seq(0, n) # kõik võimalikud õnnestumiste arvud 10st sündmusest p &lt;- 0.3 # 30% õnnestumisi (sagedus) y &lt;- dbinom(x, n, p) plot(x, y) \\[keskv\\ddot{a}\\ddot{a}rtus = N \\times p\\] Kui Np võrdub täisarvuga, siis mediaan = mood = keskväärtus \\[sd = sqrt(N \\times p(1 - p))\\] Standardviga proportsioonile \\(p = \\sqrt{\\frac{p(1 − p)}{N}}\\) See standardviga (standard error) on teiste sõnadega standardhälve meie hinangule proportsiooni väärtusele. Kui n = 0 või N - n = 0, siis on selline SE arvutus eksitav. 7.3.3 Poissoni jaotus See jaotus modelleerib üksikuid haruldasi ja sõltumatuid diskreetseid sündmusi, mille arvu me saame üles lugeda. Näiteks surmi ajaühiku kohta või pommitabamusi pindalaühiku kohta. See on sisuliselt binoomjaotuse erijuht. Lisaeeldused on, et sündmuste toimumise sagedus ei muutu, et kaks sündmust ei saa toimuda täpselt samal ajal/kohas, et sündmuse toimumise tõenäosus on proportsionaalne intervalli pikkusega/suurusega (ajas või ruumis) ja et N &gt;&gt; n. Kui keskmine sündmuste arv intevallis on \\(\\lambda\\) (lambda), siis \\[P(k~events~in~interval) = e^{\\lambda} \\times \\frac{\\lambda ^{k}}{k!}\\] Oodatud väärtus = variance = \\(\\lambda\\) \\(sd = \\sqrt{\\lambda}\\) Millal kasutada Poissoni jaotust, ja millal binoomjaotust? Kui iga andmepunkti saab vaadelda kui edukate katsete arvu suhet kõikide katsete arvule, siis kasuta binoomjaotust/logistilist regressiooni. Kui aga andmepunkti väärtusel pole loomulikku piiri (see on lihtsalt mingit tüüpi sündmuste arv), kasuta Poissoni/logaritmilist regressiooni. "],
["eda-eksploratoorne-andmeanaluus.html", "8 EDA — eksploratoorne andmeanalüüs 8.1 EDA kokkuvõte", " 8 EDA — eksploratoorne andmeanalüüs library(tidyverse) library(corrgram) library(psych) library(skimr) Kui ühenumbriline andmete summeerimine täidab eelkõige kokkuvõtliku kommunikatsiooni eesmärki, siis EDA on suunatud teadlasele endale. EDA eesmärk on andmeid eelkõige graafiliselt vaadata, et saada aimu 1) andmete kvaliteedist ja 2) lasta andmetel kõneleda “sellisena nagu nad on” ja sugereerida uudseid teaduslikke hüpoteese. Neid hüpoteese peaks siis testima formaalse statistilise analüüsi abil (ptk järeldav statistika). Näiteid erinevate graafiliste lahenduste kohta vt graafika peatükist. EDA: mida rohkem graafikuid, seda rohkem võimalusi uute mõtete tekkeks! EDA on rohkem kunst kui teadus selles mõttes, et teil on suur vabadus küsida selle abil erinevaid küsimusi oma andmete kohta. Ja seda nii tehnilisest aspektist lähtuvalt (milline on minu andmete kvaliteet?), kui teaduslikke küsimusi küsides (kas muutuja A võiks põhjustada muutusi muutujas B?). Mõned üldised soovitused võib siiski anda. alusta analüüsi tasemest, kus andmed on kõige inforikkamad — toorandmete plottimisest punktidena. Kui andmehulk ei ole väga massiivne, näitab see hästi nii andmete kvaliteeti, kui ka võimalikke sõltuvussuhteid erinevate muutujate vahel. Millised korrelatsioonid võiksid andmetes esineda? corrgram(iris, order = TRUE, lower.panel = panel.pts, upper.panel = panel.ellipse, diag.panel = panel.density, main = &quot;Correlogram of Iris dataset&quot;) Joonis 8.1: Korrelatstioonimaatriks joonisena. vaata andmeid numbrilise kokkuvõttena. psych::describe(iris) #&gt; vars n mean sd median trimmed mad min max range skew #&gt; Sepal.Length 1 150 5.84 0.83 5.80 5.81 1.04 4.3 7.9 3.6 0.31 #&gt; Sepal.Width 2 150 3.06 0.44 3.00 3.04 0.44 2.0 4.4 2.4 0.31 #&gt; Petal.Length 3 150 3.76 1.77 4.35 3.76 1.85 1.0 6.9 5.9 -0.27 #&gt; Petal.Width 4 150 1.20 0.76 1.30 1.18 1.04 0.1 2.5 2.4 -0.10 #&gt; Species* 5 150 2.00 0.82 2.00 2.00 1.48 1.0 3.0 2.0 0.00 #&gt; kurtosis se #&gt; Sepal.Length -0.61 0.07 #&gt; Sepal.Width 0.14 0.04 #&gt; Petal.Length -1.42 0.14 #&gt; Petal.Width -1.36 0.06 #&gt; Species* -1.52 0.07 skimr::skim(iris) #&gt; Skim summary statistics #&gt; n obs: 150 #&gt; n variables: 5 #&gt; #&gt; ── Variable type:factor ─────────────────────────────────────────────────── #&gt; variable missing complete n n_unique top_counts #&gt; Species 0 150 150 3 set: 50, ver: 50, vir: 50, NA: 0 #&gt; ordered #&gt; FALSE #&gt; #&gt; ── Variable type:numeric ────────────────────────────────────────────────── #&gt; variable missing complete n mean sd p0 p25 p50 p75 p100 #&gt; Petal.Length 0 150 150 3.76 1.77 1 1.6 4.35 5.1 6.9 #&gt; Petal.Width 0 150 150 1.2 0.76 0.1 0.3 1.3 1.8 2.5 #&gt; Sepal.Length 0 150 150 5.84 0.83 4.3 5.1 5.8 6.4 7.9 #&gt; Sepal.Width 0 150 150 3.06 0.44 2 2.8 3 3.3 4.4 #&gt; hist #&gt; ▇▁▁▂▅▅▃▁ #&gt; ▇▁▁▅▃▃▂▂ #&gt; ▂▇▅▇▆▅▂▂ #&gt; ▁▂▅▇▃▂▁▁ Siin pööra kindlasti tähelepanu tulpadele min ja max, mis annavad kiire võimalusi outliereid ära tunda. Kontrolli, kas andmete keskmised (mediaan, mean ja trimmed mean) on üksteisele piisavalt lähedal — kui ei ole, siis on andmete jaotus pika õlaga, ja kindlasti mitte normaalne. Kontrolli, kas erinevate muutujate keskväärtused ja hälbed on teaduslikus mõttes usutavas vahemikus. Ära unusta, et ka väga väike standardhälve võib tähendada, et teie valim ei peegelda bioloogilist varieeruvust populatsioonis, mis teile teaduslikku huvi pakub. NB! selles psych::describe() funktsiooni väljundis on mad läbi korrutatud konstandiga 1.4826, mis toob selle väärtuse lähemale sd-le. Seega on mad siin sd robustne analoog — kui mad on palju väiksem sd-st, siis on karta, et muutujas on outliereid. kontrolli NA-de esinemist oma andmetes VIM paketi abil või käsitsi (vt esimene ptk). Kontrolli, et NA-d ei oleks tähistatud mingil muul viisil (näiteks 0-i või mõne muu numbriga). Kui vaja, rekodeeri NAd. Mõtle selle peale, millised protsessid looduses võiksid genreerida puuduvaid andmeid. Kui NA-d ei jaotu andmetes juhuslikult, võib olla hea mõte andmeid imputeerida (vt hilisemaid ptk, bayesiaanlik imputeerimine). Näiteks, kui ravimiuuringust kukuvad eeskätt välja patsiendid, kellel ravim ei tööta, on ilmselt halb mõte nende patsientide andmed lihtsalt uuringust välja vistata (muidugi, kui te ei esinda kasumit taotleva ettevõtte huve). Kui NA-d jaotuvad juhuslikult, mõtle sellele, kas sa tahad NA-dega read tabelist välja visata, või hoopis osad muutujad, mis sisaldavad liiga palju NA-sid, või mitte midagi välja vistata. NB! NA-dega andmed ei sobi hästi regresiooniks. Kui andmeid on nii palju, et üksikute andmepunktide vaatlemine paneb pea valutama, siis järgmine informatiivsuse tase on histogramm. kui tahame kõrvuti vaadata paljude erinevate muutujate varieeruvust ja keskväärtusi, siis on head valikud joyplot, violin plot, ja vähem hea valik (sest ta kaotab andmetest rohkem infot) on boxplot. Kui meil on vaid 2-4 jaotust, mida võrrelda, siis saab mängida histogramme facetisse või üksteise otsa pannes (vt ptk graphics). Tulpdiagramm on hea valik siis, kui tahate kõrvuti näidata proportsioonide erinevust. Näiteks, kui meil on 3 liiki kalu, millest igas on erinevas proporstioonis parasiidid, võime joonistada 3 tulpa, millest igas on näidatud ühe kalaliigi parasiitide omavaheline proportsioon. Tulpdiagramm on hädaga pooleks kasutuskõlblik, kui iga muutuja kohta on vaid üks number, mida plottida. Kuigi, siin on meil parem võimalus — Cleveland plot. Olukorras, kus te tahate plottida valimi keskväärtust ja usalduspiire või varieeruvusnäitajat (sd, mad), on olemas selgelt paremad meetodid kui tulpdiagramm. Samas, ehki tulpdiagrammide kasutamine teaduskirjanduses on pikas langustrendis, kasutatakse neid ikkagi liiga palju just sellel viisil. Ära piirdu muutuja tasemel varieeruvuse plottimisega. Teaduslikult on sageli huvitavam mimte muutuja koosvarieerumine. Järgmistes peatükkides modelleerime seda formaalselt regresioonanalüüsis aga alati tasub alustada lihtsatest plottidest. Scatterplot on lihtne viis kovarieeruvuse vaatamiseks. Kui erinevad muutujad on mõõdetud erinevates skaalades (ühikutes), siis võib nende koosvarieeruvust olla kergem võrrelda, kui nad eelnevalt normaliseerida (kõigi muutujate keskväärtus = 0, aga varieeruvus jääb algsesse skaalasse) või standardiseerida (kõik keskväärtused = 0-ga ja sd-d = 1-ga). Standardiseerida tohib ainult normaaljaotusega muutujaid (seega võib olla vajalik muutuja kõigepealt logaritmida). Normaliseerimine: arvuta igale valimi väärtusele: mean(x) - x; standardiseerimine: (mean(x) - x) / sd(x). Visualiseeringu valik sõltub valimi suurusest. Väikse valimi korral (N&lt;10) boxploti, histogrammi vms kasutamine on lihtsalt rumal. Ära mängi lolli ja ploti parem punkti kaupa. N &lt; 20 - ploti iga andmepunkt eraldi (stripchart(), plot()) ja keskmine või mediaan. 20 &gt; N &gt; 100: geom_dotplot() histogrammi vaates N &gt; 100: geom_histogram(), geom_density() — nende abil saab ka 2 kuni 6 jaotust võrrelda Mitme jaotuse kõrvuti vaatamiseks, kui N &gt; 15: geom_boxplot(), or geom_violin(), geom_joy() Nii saab plottida multiplikatiivse sd: #&gt; Warning: Ignoring unknown parameters: bins Joonis 8.2: Multiplikatiivse sd joonistamine. 8.1 EDA kokkuvõte Andmepunktide plottimine säilitab maksimaalselt andmetes olevat infot (nii kasulikku infot kui müra). Aitab leida outliereid (valesti sisestatud andmeid, valesti mõõdetud proove jms). Kui valim on väiksem kui 20, piisab täiesti üksikute andmepunktide plotist koos mediaaniga. Dot-plot ruulib. Histogramm – kõigepealt mõõtskaala ja seejärel andmed jagatakse võrdse laiusega binnidesse ja plotitakse binnide kõrgused. Bin, kuhu läks 20 andmepunkti on 2X kõrgem kui bin, kuhu läks 10 andmepunkti. Samas, bini laius/ulatus mõõteskaalal pole teile ette antud – ja sellest võib sõltuda histogrammi kuju. Seega on soovitav proovida erinevaid bini laiusi ja võrrelda saadud histogramme. Histogramm sisaldab vähem infot kui dot plot, aga võimaldab paremini tabada seaduspärasid &amp; andmejaotust &amp; outliereid suurte andmekoguste korral. Density plot. Silutud versioon histogrammist, mis kaotab infot aga toob vahest välja signaali müra arvel. Density plotte on hea kõrvuti vaadelda joy ploti abil. Box-plot –- sisaldab vähem infot kui histogramm, kuid neid on lihtsam kõrvuti võrrelda. Levinuim variant (kuid kahjuks mitte ainus) on Tukey box-plot – mediaan (joon), 50% IQR (box) ja 1,5x IQR (vuntsid), pluss outlierid eraldi punktidena. Violin plot –- informatiivsuselt box-ploti ja histogrammi vahepeal – sobib paljude jaotuste kõrvuti võrdlemiseks. Line plot –- kasuta ainult siis kui nii X kui Y teljele on kantud pidev väärtus (pikkus, kaal, kontsentratsioon, aeg jms). Ära kasuta, kui teljele kantud punktide vahel ei ole looduses mõtet omavaid pidevaid väärtusi (näiteks X teljel on katse ja kontroll või erinevad valgumutatsioonid, mille aktiivsust on mõõdetud). Tulpdiagramm – Suhete võrdlemine (bar). Cleveland plot on hea countide võrdlemiseks. Kui Cleveland plot mingil põhjusel ei sobi, kasuta tupldiagrammi. Pie chart on proportsioonide vaatamiseks enam-vähem kõlblik ainult siis, kui teil pole vaja võrrelda proportsioone erinevates objektides. Kõik graafikud, kus lugeja peab võrdlema pindalasid, on inimmõistusele petlikud — lugeja alahindab süstemaatiliselt erinevuste suurusi! Selle pärast on proportsioonide võrdlemiseks palju parem tulpdiagramm, kus võrreldavad tulbad on ühekõrgused. Informatsiooni hulk kahanevalt: iga andmepunkt plotitud —&gt; histogramm —&gt; density plot &amp; violin plot —&gt; box plot —&gt; tulpdiagramm standardhälvetega —&gt; cleveland plot (ilma veapiirideta) "],
["jareldav-statistika.html", "9 Järeldav statistika Järeldav statistika on tõenäosusteooria käepikendus Andmed ei ole sama, mis tegelikkus", " 9 Järeldav statistika Kui EDA määrab graafiliste meetoditega andmete kvaliteeti ja püstitab uusi hüpoteese, siis järeldav statistika püüab formaalsete arvutuste abil vastata kahele lihtsale küsimusele: 1. mis võiks olla kõige usutavam parameetriväärtus? ja 2. kui suur ebakindlus seda hinnangut ümbritseb? Kuna andmed tulevad meile lõpliku suurusega valimina koos mõõtmisveaga ja bioloogilise varieeruvusega, on ebakindlus hinnagusse sisse ehitatud. Hea protseduur kvantifitseerib selle ebakindluse ausalt ja täpselt – siin ei ole eesmärk mitte niivõrd ebakindlust vähendada (seda teeme eelkõige katse planeerimise tasemel), vaid seda kirjeldada. Järeldav statistika püüab, kasutades algoritme ja mudeleid, teha andmete põhjal järeldusi looduse kohta. Ebakindluse allikad on mõõtmisviga (võib olla tsentreeritud õigele väärtusele, või mitte), valimiviga (juhuslik viga, mis sõltub valimi suurusest), bioloogiline varieeruvus, mudeli viga (kas maailm on lineaarne ja normaaljaotusega?), algoritmi viga kus algoritm ei tee seda, mida kasutaja tahab (eriti ohtlik mcmc algoritmide puhul) ja süstemaatiline viga (juhtub, kui te saate valesti aru oma katsesüsteemist, harrastate teaduslikku pettust või teete kõike muud, mis suunaliselt kallutab teie valimit tegelikkusest). Sellisel tegevusel on mõtet ainult siis, kui ühest küljest andmed peegeldavad tegelikkust ja teisest küljest tegelikkus hõlmab enamat, kui lihtsalt meie andmeid. Kui andmed = tegelikkus, siis pole mõtet keerulisi mudeleid kasutada – piisab lihtsast andmete kirjeldusest. Ja kui andmetel pole midagi ühist tegelikkusega, siis on need lihtsalt ebarelevatsed. Seega on järeldava statistika abil tehtud järeldused alati rohkem või vähem ebamäärased ning meil on vaja meetodit selle ebamäärasuse mõõtmiseks. Selle meetodi annab tõenäosusteooria. Järeldav statistika on tõenäosusteooria käepikendus See õpik õpetab Bayesi statistikat, mis põhineb tõenäosusteoorial. Tänu sellele moodustab Bayesi statistika sidusa terviku, mille abil saab teha kõike seda, mida saab teha tõenäosusteooria abil. Bayesi statistika põhineb Bayesi teoreemil, mis on triviaalne tuletus tõenäosusteooria aksioomidest. Tänu Cox-i teoreemile (1961) teame, et klassikaline lausearvutuslik loogika on tõenäosusteooria erijuht ning, et Bayesi teoreem on teoreetiliselt parim viis tõenäosustega töötamiseks. Seega, kui te olete kindel oma väidete tõesuses või vääruses, siis on klassikaline loogika parim viis nendega opereerida; aga kui te ei saa oma järeldustes päris kindel olla, siis on teoreetiliselt parim lahendus tõenäosusteooria ja Bayesi teoreem. Tõenäosusteooria on aksiomaatline süsteem, mille abil saame omistada numbriline väärtuse meie usu määrale mingisse hüpoteesi. Näiteks, kui me planeerime katset, kus me viskame kulli ja kirja ja teeme seda kaks korda, siis saame arvutada, millise tõenäosusega võime oodata katse tulemuseks kaht kirja. Aga seda tingimusel, et me võtame omaks mõned eeldused – näiteks et münt on aus ja et need kaks viset on üksteisest sõltumatud. Sellel katsel on 4 võimalikku tulemust: H-H, H-T, T-H, T-T (H -kull, T - kiri). Tõenäosus saada 2-l mündiviskel 2 kirja, P(2 kirja) = 1/4, P(0 kirja) = 1/4 ja P(1 kiri) = 2/4 = 1/2. Sellega oleme andnud oma katseplaanile täieliku tõenäosusliku kirjelduse (pane tähele, et 1/4 + 1/4 + 1/2 = 1). Ükskõik kui keeruline on teie katseplaan, põhimõtteliselt käib selle analüüs samamoodi. Tõenäosusteooria loomus seisneb kõikide võimalike sündmuste üleslugemises ning senikaua, kui me seda nüri järjekindlusega teeme, on vastus, mille me saame, tõsikindel. Ehkki Bayesi statistika põhineb tõenäosusteoorial ja on sellega kooskõlas, ei ole see sama asi, mis tõenäosusteooria. Statistikas pööratakse tõenäosusteoreetiline ülesanne pea peale ja küsitakse nii: kui me saime 2-l mündiviskel 2 kirja, siis millise tõenäosusega on münt aus (tasakaalus)? Erinevus tõenäosusteoreetilise ja statistilise lähenemise vahel seisneb selles, et kui tõenäosusteoorias me eeldame, et teame, kuidas süsteem on üles ehitatud, ja ennustame sellest lähtuvalt andmete tõenäosusi, siis statistikas me kontrollime neid eeldusi andmete põhjal. Seega annab tõenäosusteooria matemaatiliselt tõsikindlaid vastuseid ideaalmaailmade kohta, samas kui statistika püüab andmete põhjal teha järeldusi päris maailma kohta. Selleks kasutame Bayesi teoreemi (vt allpool). Tõenäosusteooria määrab kõikide võimalike sündmuste esinemise tõenäosused, eeldades, et hüpotees H kehtib (H on siin lihtsalt teine nimi “eeldusele”). Statistika arvutab H-i kehtimise tõenäosuse lähtuvalt kogutud andmetest, matemaatilistest mudelitest ning teaduslikest taustateadmistest. Tõenäosusteooria aksioomid ütlevad tõlkes inimkeelde, et tõenäosused (P) jäävad 0 ja 1 vahele, et P(A) = 1 tähendab, et A on tõene, et P(A) = 0 tähendab, et A on väär, ning kui A ja B on hüpoteesiruumi ammendavad üksteist välistavad hüpoteesid, siis P(A) + P(B) = 1. Need aksioomid peaksid olema iseenesestmõistetavad ja ainult neist on tuletatud kogu tõenäosusteooria. Need aksioomid, mis oma matemaatilises vormis postuleeriti Andrei Kolmogorovi poolt ca 1930, on tuletatavad järgmistest eeldustest: ratsionaalne mõtlemine vastab kvalitatiivselt tervele mõistusele: lisatõendusmaterjal hüpoteesi kasuks tõstab selle hüpoteesi usutavust. mõtlemine peab olema konsistentne: kui me võime järeldusi teha rohkem kui ühel viisil, peame lõpuks ikkagi alati samale lõppjäreldusele jõudma kogu kättesaadav relevantne informatsioon tuleb järelduste tegemisel arvesse võtta (totaalse informatsiooni printsiip) ekvivalentsed teadmised on representeeritud ekvivalentsete numbritega. Kui tõenäosused on 0 või 1, siis taandub tõenäosusteooria matemaatliselt oma erijuhule, milleks on lausearvutuslik loogika. Lausearvutuse oluline erinevus tõenäosusteooriast on, et kui selle abil on saavutatud valiidne tulemus, on see tõsikindel ja uute andmete lisandumisel ei saa me seda tulemust muuta. Seevastu tõenäosusteoorias ja statstikas muudavad uued andmed alati tõenäosusi. Selles mõttes ei saa tõenäosuslik teadus kunagi valmis. Formaalsed tuletised tõenäosusteooria aksioomidest Me anname siin 9 tuletust ilma tõestuskäikudeta, mis on aga lihtsad. Siin võib A ja B vaadelda erinevate sündmustena või hüpoteesidena. Me eeldame, et kummagi hüpoteesi tõenäosus &gt; 0. Sümbolite tähendused: \\(P(A~ \\vert ~B)\\) on tinglik tõenäosus, mida tuleks lugeda: “A tõenäosus tingimusel, et kehtib B”. Pane tähele, et \\(P(vihm~\\vert~pilves~ilm)\\) ei ole sama, mis \\(P(pilves~ilm~\\vert~vihm)\\). \\(A \\land B\\) tähendab “A ja B”, \\(A \\lor B\\) tähendab “A või B”, \\(\\lnot A\\) tähendab mitte-A, ehk A == FALSE. Tõenäosusteooria põhituletised: Kui B sisaldab endas A-d, siis \\(P(B) \\leq P(A)\\) Def: A ja B on üksteisest sõltumatud siis ja ainult siis kui \\(P(A~ \\vert B) = P(A)\\) Kui A ja B on üksteisest sõltumatud, siis \\(P(A \\land B) = P(A)P(B) = P(A~\\vert~B)P(B)\\) Kui A ja B on üksteist välistavad, siis \\(P(A \\lor B) = P(A) + P(B)\\). Kui A ja B ei ole üksteist välistavad, siis \\(P(A \\lor B) = P(A) + P(B) - P(A \\land B)\\) Def: \\(P(A~\\vert~B) = P(A \\land B)/P(B)\\) – Tinglik tõenäosus Totaalne tõenäosus: \\(P(A) = P(A~\\vert~B)P(B) + P(A~\\vert~\\lnot B)P(\\lnot B)\\) – , tuletatud 6. punktist. Bayesi teoreem: \\(P(A~\\vert~B) = P(A)P(B~\\vert~A)/P(B)\\) – tuletatud 6. punktist –, kus \\(P(B) = P(A)P(B~\\vert~A) + P(\\lnot A)P(B~\\vert~\\lnot A)\\) – 7. punktist. Bayesi teoreemi kasutatakse määramaks hüpoteesi tõenäosuse pärast uute faktide (andmete) lisandumist olemasolevatele teadmistele. Selleks peab hüpoteesiruum olema jagatud vähemalt kaheks ammendavaks ja üksteist välistavaks hüpoteesiks. Kui A on H1 ning mitte-A on ammendav ja välistav H2 ja B tähistab andmeid (data), saame Bayesi teoreemi ümber kirjutada \\(P(H_1~\\vert~data) = P(H_1)P(data~\\vert~H_1) /( P(H_1)P(data~\\vert~H_1) + P(H_2)P(data~ \\vert ~H_2) )\\) \\(P(H_1~\\vert~data)\\) on \\(H_1\\) kehtimise tõenäosus meie andmete korral – ehk posteerior, \\(P(H_1)\\) on \\(H_1\\) kehtimise eelnev, ehk meie andmetest sõltumatu, tõenäosus – ehk prior, \\(P(data~\\vert~H_1)\\) on andmete esinemise tõenäosus tingimusel, et H1 kehtib – ehk tõepära. Jagamistehe tehakse ainult selle pärast, et normaliseerida 1-le kõikide hüpoteeside tõenäosuste summa meie andmete korral ja seega viia posteerior vastavusse tõenäosusteooria aksioomidega — kui meil on i ammendavat üksteist välistavat hüpoteesi, siis murrujoone alla läheb \\(\\sum~P(data~\\vert~H_i)P(H_i) = 1\\). Bayesi teoreem on triviaalne tuletus tõenäosusteooria aksioomidest, milles pole midagi maagilist. See ei ole automaatne meetod, mis tagaks inimkonna teadmiste kasvu, vaid lihtsalt parim võimalik viis andmemudeli ja taustateadmiste mudeli ühendamiseks ja normaliseerimiseks tinglikuks tõenäosuseks (hüpoteesi tõenäosus meie andmete ja taustateadmiste korral). Edasi sõltub kõik mudelite, andmete ja taustateadmiste kvaliteedist. Näited tõenäosusteooria tuletiste rakendamisest Järgnevatel näidetel on ühist kaks asja: need on matemaatiliselt triviaalselt lihtsad, aga intuitiivselt lootusetult keerulised. Kõigi nende puhul on inimestel tugev intuitsioon, mis on vale – ja tõenäosusteooria tundmine ei anna meile paremat intuitsiooni. Seega, ainus, mis üle jääb, on iga probleemi taandamine tõenäosusteooria valemitele ja selle tuimalt läbi arvutamine. Punkt 3. Kui me viskame täringut 3 korda, kui suure tõenäosusega saame vähemalt ühe kuue? Naiivselt võiks arvata, et see tõenäosus on 50%. Kuid rakendades tõenäosusteooriat saame teistsuguse vastuse. Lihtsuse huvides defineerime küsimuse ümber: kui suure tõenäosusega ei saa me 3-l viskel ühtegi kuute? Vastus: kui igal viskel on 0 kuue tõenäosus 1/6, siis \\((5/6)*(5/6)*(5/6) = 0.58\\) ja \\(1 - 0.58 = 0.42\\), mis tähendab, et vähemalt 1 kuue (või ükskõik mis numbri ühest kuueni) saame 42% tõenäosusega. Teine näide (NYT 03-12-2017): te ostate maja Texases Hustonis, millele müüja annab garantii, et üleujutuse tõenäosus on 1% aastas. Seadus nimetab seda näidikut “100 aasta suurvee-tasemeks”. 1% näidu puhul ei pea te seaduse järgi ostma üleujutusekindlustust. Kui suure tõenäosusega tabab teie maja üleujutus pangalaenu perioodi vältel (30 aastat)? Vastus: \\(1 - (99/100)^{30} = 0.26\\). Punkt 6. Meil on kolm pannkooki, millest esimesel on mõlemad küljed moosised, teisel on üks külg moosine ja kolmandal pole üldse moosi. Juhtus nii, et meile pandi taldrikule pannkook, mille pealmine külg on moosine. Millise tõenäosusega on moosine ka selle pannkoogi alumine külg? NB! Vastus ei ole 50%. Lahendus: Kui A - moos all, B - moos üleval, siis vastavalt tingliku tõenäosuse definitsioonile \\(P(moos~ all~ \\lvert ~moos~üleval ) = P(moos~all \\land ~moos~üleval)/P(moos~all)\\) Tõenäosus, et moos on all ja üleval on 1/3 (me teame, et 1 pannkook 3st on mõlemalt küljelt moosine) ja tõenäosus, et moos on all, on keskmine kolmest tõenäosusest, millega me kolmel pannkoogil moosise külje saame: mean(c(1, 0.5, 0)) = 1/2. Seega, vastus on \\((1/3)/(1/2) = 2/3\\). Kui me saame moosise ülemise külje, siis on tõenäosus 2/3, et ka all on moos! Punkt 7. Kui A tähistab sündmust “ma sooritan eksami edukalt” ja B tähistab sündmust “ma õpin eksamiks”, ning meil on dihhotoomne valik: õpin / ei õpi, siis \\(P(hea~hinne) = P(õpin)P(hea~hinne~ \\lvert ~õpin) + P(ei~ õpi)P(hea~hinne~ \\lvert ~ei~õpi)\\). Ehk sõnadega kirjutatult: Hea hinde tõenäosus võrdub korrutisega kahest tõenäosusest – tõenäosus, et ma eksamiks õpin, ja tõenäosus, et ma saan hea hinde siis kui ma õpin –, millele tuleb liita teine korrutis kahest tõenäosustest – tõenäosus, et ma ei õpi, ja tõenäosus, et ma saan hea hinde ka ilma õppimata. Siit saad ise enda jaoks välja arvutada ennustuse, millise tõenäosusega just sina selle kursuse edukalt läbid. Punkt8. Bayesi teoreemi rakendamine diskreetsetele hüpoteesidele: Oletame, et 45 aastane naine saab rinnavähi sõeluuringus mammograafias positiivse tulemuse. Millise tõenäosusega on tal rinnavähk? Kõigepealt jagame hüpoteesiruumi kahe diskreetse hüpoteesi vahel: H1 - vähk ja H2 - mitte vähk. Edasi omistame numbrilised väärtused järgmistele parameetritele: H1 tõepära, ehk tõenäosus saada positiivne mammogramm juhul, kui patsiendil on rinnavähk (testi sensitiivsus): \\(P( +~\\vert~H_1) = 0.9\\) H2 tõepära, ehk tõenäosus saada positiivne mammogramm juhul, kui patsiendil ei ole rinnavähki (1 - testi spetsiifilisus): \\(P( +~\\vert~H_2) = 0.08\\). Pane tähele, et 0.9 + 0.08 ei võrdu ühega, mis tähendab, et tõepära pole tõenäosusteooria mõttes päris tõenäosus. Eelnev tõenäosus, et patsiendil on rinnavähk \\(P(H_1) = 0.01\\) (see on rinnavähi sagedus 45 a naiste populatsioonis; kui me teame patsiendi genoomi järjestust või rinnavähijuhte tema lähisugulastel, võib P(H~1) tulla väga erinev). \\(P(H_2) = 1 - P(H_1) = 0.99\\) Nüüd arvutame posterioorse tõenäosuse \\(P(H_1~\\vert~+)\\) likelihood_H1 &lt;- 0.9 likelihood_H2 &lt;- 0.08 prior_H1 &lt;- 0.01 prior_H2 &lt;- 1 - prior_H1 posterior1 &lt;- likelihood_H1*prior_H1/(likelihood_H1*prior_H1 + likelihood_H2*prior_H2) posterior1 #&gt; [1] 0.102 Nagu näha, postiivne tulemus rinnavähi sõeluuringus annab 10% tõenäosuse, et teil on vähk (ja 90% tõenäosuse, et olete terve). Selle mudeli parameetriväärtused vastavad enam-vähem tegelikele mammograafia veasagedustele ja tegelikule populatsiooni vähisagedusele. Mis juhtub, kui me teeme positiivsele patsiendile kordustesti? Nüüd on esimese testi posteerior meile prioriks, sest see kajastab definitsiooni järgi kogu teadmist, mis meil selle patsiendi vähiseisundist on (muidugi eeldusel, et me esimese mudeli kohusetundlikult koostasime). likelihood_H1 &lt;- 0.9 likelihood_H2 &lt;- 0.08 prior_H1 &lt;- posterior1 prior_H2 &lt;- 1 - prior_H1 posterior2 &lt;- likelihood_H1*prior_H1/(likelihood_H1*prior_H1 + likelihood_H2*prior_H2) posterior2 #&gt; [1] 0.561 Patsiendile võib pärast kordustesti positiivset tulemust öelda, et ta on 44% tõenäosusega vähivaba. Eelduseks on, et me ei tea midagi selle patsiendi geneetikast ega keskkonnast põhjustatud vastuvõtlikusest vähile ning, et testi ja kordustesti vead on üksteisest sõltumatud (mitte korreleeritud). Bayesi teoreemi kasutamine pideva suuruse (näiteks keskväärtuse või standardhälbe) hindamiseks on põhimõtteliselt samasugune, ainult et nüüd on meil lõpmata suur arv hüpoteese (iga teoreetiliselt võimalik parameetri väärtus on siin “hüpotees”), mis tähendab, et vastavalt Bayesi teoreemile on meil vaja ka lõpmata hulka tõepärasid ja lõpmata hulka prioreid. Lõpmata hulk tõepärasid ja prioreid tähendab lihtsalt, et me avaldame need kahe pideva funktsioonina, misjärel saame neist kahest funktsioonist arvutada kolmanda pideva funktsiooni, posteeriori. Posteeriorist saab omakorda arvutada iga mõeldava parameetriväärtuste vahemiku tõenäosuse või usalduspiirid, milles mingi meie poolt etteantud tõenäosusega paikneb parameetri tegelik väärtus (vt ptk 10). Ja posterioorse funktsiooni tipp (mood) vastab kõige tõenäolisemale parameetriväärtusele. Mida kitsam on posteerior, seda kitsamad tulevad sellest arvutatud usalduspiirid. Siit tuleneb, et kui võimalik peaksime oma mudelitesse panema parameetreid (statistikuid), mille posteeriorid tulevad maksimaalselt kitsad (vt allpool ptk “ajalooline vahepala” selle kohta, kuidas aritmeetiline keskmine on selline statistik). Tõenäosuse tõlgendus Kolmogorovi aksioomid õpetavad meid tõenäosustega matemaatiliselt ümber käima, aga nad ei anna meile seost matemaatiliste tõenäosuste ja päris maailma vahel, ega ei ütle, mida tõenäosus teaduses tähendab. Need on pigem küsimused teadlastele ja filosoofidele, kui matemaatikutele. Kaasajal eksisteerib kaks põhilist tõenäosuse tülgendust, bayesiaanlik ja sageduslik, millest me siin käsitleme esimest. Sagedisliku tõlgenduse kohta vt lisa 1. Bayesiaanlik statistika opereerib episteemilise tõenäosusega. See tähendab, et tõenäosus annab numbrilise mõõdu meie ebakindluse määrale mõne hüpoteesi ehk parameetriväärtuse kehtimise kohta. Seega mõõdab tõenäosus meie teadmiste kindlust (või ebakindlust). Näiteks, kui Bayesi arvutus väidab, et vihma tõenäosus homme on 60%, siis me oleme 60% kindlad, et homme tuleb vihma. Aga hoolimata sellest, mida me vihma kohta usume, homme kas sajab vihma või mitte, ja seega on objektiivne vihma tõenäosus meie akna taga 0% või 100% – mitte kunagi 60%. Tõenäosuse formaalne tõlgendus tuleb otse kihlveokontorist. Kui sa arvutasid, et vihma tõenäosus homme on 60%, siis see tähendab, et sa oled ratsionaalse olendina nõus maksma mitte rohkem kui 60 senti kihlveo eest, mis võidu korral toob sulle sisse 1 EUR – ehk 40 senti kasumit. Selles mõttes on Bayesi tõenäosus subjektiivne. Kui me teaksime täpselt, mis homme juhtub, siis ei oleks meil selliseid tõenäosusi vaja. Seega, kui te usute, et teadus suudab tõestada väiteid maailma kohta, nagu seda teeb matemaatika formaalsete struktuuride kohta, siis pääsete sellega statistika õppimisest ja kasutamisest. Aga kui te siiski arvutate Bayesi tõenäosusi, siis ei ütle need midagi selle kohta, kas maailm on tõenäosuslik või deterministlik. Inimesed, kes vajavad tõenäosusi maailma seisundite kirjeldamiseks, ei kasuta enamasti Bayesi tõenäosustõlgendust, vaid sagedusliku tõlgendust. Kui me mõõdame pidevat suurust, näiteks inimeste pikkusi, siis saame arvutuse tagajärjel tõenäosused kõigi võimalike parameetriväärtuste kohta, ehk igale mõeldavale pikkuse väärtusele. Kuna pideval suurusel on lõpmata hulk võimalikke väärtusi, avaldame me sellised tõenäosused pideva tõenäosusfunktsioonina, ehk posteeriorina. See näeb sageli välja nagu normaaljaotus ja me võime igast posteeriorist arvutada, kui suur osa summaarsest tõenäosusest, mis on 100%, jääb meid huvitavasse pikkustevahemikku. Kui näiteks 67% posteeriori pindalast jääb pikkuste vahemikku 178 kuni 180 cm, siis me usume 67%-se kindlusega, et tõde asub kuskil selles vahemikus. Tõenäosusteooriast tulenevad statistika põhiprintsiibid statistilise analüüsi kvaliteet sõltub mudeli eeldustest &amp; struktuurist. Kuna maailm ei koosne matemaatikast, teevad matemaatilised mudelid alati eeldusi maailma kohta, mis ei ole päris tõesed ja mida ei saa tingimata empiiriliselt kontrollida. Mündiviske näites eeldasime, et mündivisked olid üksteisest sõltumatud. Kui me sellest eeldusest loobume, läheb meie mudel keerulisemaks, sest me peame mudelisse lisama teavet visetevahelise korrelatsiooni kohta. Aga see keerulisem mudel toob sisse uued eeldused (vähemalt pool tosinat lisaeeldust). Üldiselt peaks mudeli struktuur kajastama katse struktuuri, mis kaasaegses statistikas tähendab sageli hierarhilisi mudeleid. statistilise analüüsi täpsus sõltub andmete hulgast. Kui kahe mündiviske asemel teeksime kakskümmend, siis saaksime samade eelduste põhjal teha oluliselt väiksema ebakindluse määraga järeldusi mündi aususe kohta. statistilise analüüsi kvaliteet sõltub andmete kvaliteedist. Kui münt on aus, aga me viskame seda ebaausalt, siis, mida rohkem arv kordi me seda teeme, seda tugevamalt usub teadusüldsus selle tagajärjel millessegi, mis pole tõsi. statistilise analüüsi kvaliteet sõltub taustateadmiste kvaliteedist. Napid taustateadmised ei võimalda parandada andmete põhjal tehtud järeldusi juhul, kui andmed mingil põhjusel ei vasta tegelikkusele. Adekvaatsete taustateadmiste lisamine mudelisse aitab vältida mudelite üle-fittimist. Järeldused ühe hüpoteesi kohta mõjutavad järeldusi ka kõigi alternatiivsete hüpoteeside kohta. Relevantsete hüpoteeside eiramine viib ekslikele järeldustele kõigi teiste hüpoteeside kohta. Me ei saa põhimõtteliselt rääkida tõendusmaterjali tugevusest ühe hüpoteesi kontekstis – tõendusmaterjal on suhteline ja selle tugevust mõõdab tõepärade suhe \\(P(andmed ~\\vert~ H_1)/P(andmed ~\\vert~ H_2)\\). Andmed ei ole sama, mis tegelikkus Nüüd, kus me saame aru tõenäosusteooriast, on aeg asuda statistika kallale. Me ei kasuta statistikat kunagi vabatahtlikult, vaid teeme seda ainult siis, kui usume kahte asja: ühest küljest, et meie andmed on piisavalt tõetruud, et nende põhjal saaks teha adekvaatseid oletusi päris maailma kohta. Ja teisest küljest, et meie andmed ei ole piisavalt sarnased tõetruud, et neid järeldusi saaks teha lihtsalt ja intuitiivselt. Seega tasub alustada näitega sellest, kuidas andmed ja tegelikkus erinevad. Meie tööriistaks on siin simulatsioon. Simuleerimine on lahe sest simulatsioonid elavad mudeli väikeses maailmas, kus me teame täpselt, mida me teeme ja mida on selle tagajärjel oodata. Simulatsioonidega saame me hõlpsalt kontrollida, kas ja kuidas meie mudelid töötavad ning genereerida olukordi (parameetrite väärtuste kombinatsioone), mida suures maailmas kunagi ette ei tule. Selles mõttes on mudelid korraga nii väiksemad kui suuremad kui päris maailm. Alustuseks simuleerime juhuvalimi n = 3 lõpmata suurest normaaljaotusega populatsioonist, mille keskmine on 100 ja sd on 20. See on põhimõtteliselt sama simulatsioon, millise me tegime eelnevalt peatükis “Normaaljaotuse mudel väikestel valimitel”. Jällegi, tähtis ei ole konkreetne juhuvalim, vaid valimi kui sellise erinevus populatsioonist. Päris elus on korraliku juhuvalimi tõmbamine tehniliselt raske ettevõtmine ja, mis veelgi olulisem, me ei tea kunagi, milline on populatsiooni tõeline jaotus, keskmine ja sd. Seega, elagu simulatsioon! set.seed(1) # makes random number generation reproducible Sample &lt;- rnorm(n = 3, mean = 100, sd = 20) Sample; mean(Sample); sd(Sample) #&gt; [1] 87.5 103.7 83.3 #&gt; [1] 91.5 #&gt; [1] 10.8 Nagu näha on meie konkreetse valimi keskmine 10% väiksem kui peaks ja valimi sd lausa kaks korda väiksem. Seega peegeldab meie valim halvasti populatsiooni — aga me teame seda ainult tänu sellele, et tegu on simulatsiooniga. Kui juba simuleerida, siis robinal: tõmbame ühe juhuvalimi asemel 10 000, arvutame seejärel 10 000 keskmist ja 10 000 sd-d ning vaatame nende statistikute jaotusi ja keskväärtusi. Simulatsioon on nagu tselluliit — see on nii odav, et igaüks võib seda endale lubada. Meie lootus on, et kui meil on palju valimeid, millel kõigil on juhuslik viga, mis neid populatsiooni suhtes ühele või teisele poole kallutab, siis rohkem on valimeid, mis asuvad tõelisele populatsioonile pigem lähemal kui kaugemal. Samuti, kui valimiviga on juhuslik, siis satub umbkaudu sama palju valimeid tõelisest populatsiooniväärtusest ühele poole kui teisele poole ja vigade jaotus tuleb sümmeetriline. N &lt;- 3 N_simulations &lt;- 10000 df &lt;- tibble(a = rnorm(N * N_simulations, 100, 20), b = rep(1:N_simulations, each = N)) Summary &lt;- df %&gt;% group_by(b) %&gt;% summarise(Mean = mean(a), SD = sd(a)) Summary %&gt;% ggplot(aes(Mean)) + geom_histogram(bins = 40) Joonis 9.1: Keskmiste jaotus 10 000 valimist. mean(Summary$Mean) #&gt; [1] 100 mean(Summary$SD) #&gt; [1] 17.8 Oh-hooo. Paljude valimite keskmiste keskmine ennustab väga täpselt populatsiooni keskmist aga sd-de keskmise keskmine alahindab populatsiooni sd-d. Valem, millega sd-d arvutatakse, töötab lihtsalt kallutatult, kui n on väike (&lt;10). Kes ei usu, kordab simulatsiooni valimiga, mille N=30. Ja nüüd 10 000 SD keskväärtused: Summary %&gt;% ggplot(aes(SD)) + geom_histogram(bins = 40) Joonis 9.2: SD-de jaotus 10 000 valimist. mode &lt;- function(x, adjust = 1){ x &lt;- na.omit(x) dx &lt;- density(x, adjust = adjust) dx$x[which.max(dx$y)] } mode(Summary$SD) #&gt; [1] 14.1 SD-de jaotus on ebasümmeetriline ja mood ehk kõige tõenäolisem valimi sd väärtus, mida võiksime oodata, on u 14, samal ajal kui populatsiooni sd = 20. Lisaks on sd-de jaotusel paks saba, mis tagab, et teisest küljest pole ka vähetõenäoline, et meie valimi sd populatsiooni sd-d kõvasti üle hindab. Arvutame, mitu % valimite sd-e keskmistest on &gt; 25 mean(Summary$SD &gt; 25) #&gt; [1] 0.211 Me saame &gt;20% tõenäosusega pahasti ülehinnatud SD. mean(Summary$SD &lt; 15) #&gt; [1] 0.434 Ja me saame &gt;40% tõenäosusega pahasti alahinnatud sd. Selline on väikeste valimite traagika. Aga vähemalt populatsiooni keskmise saame me palju valimeid tõmmates ilusasti kätte — ka väga väikeste valimitega. Kahjuks pole meil ei vahendeid ega kannatust loodusest 10 000 valimi kogumiseks. Enamasti on meil üksainus valim. Õnneks pole sellest väga hullu, sest meil on olemas analoogne meetod, mis töötab üsna hästi ka ühe valimiga. Me teeme lihtsalt ühest valimist mitu, mis meenutab pisut mittemillegist midagi tegemist, aga veidi üllatuslikult töötab selles kontekstis üsna hästi. Seda metoodikat kutsutakse bootstrappimiseks ja selle võttis esimesena kasutusele parun von Münchausen. Too jutukas parun nimelt suutis end soomülkast iseenda patsi pidi välja tõmmata (koos hobusega), mis ongi bootstrappimise põhimõte. (Inglise kultuuriruumis tõmbab bootstrappija ennast mülkast välja oma saapaserva, mitte patsi pidi – siit ka meetodi nimi.) Statistika tõmbas oma saapaid pidi mülkast välja Brad Efron 1979. aastal. Joonis 9.3: Nii nagu parun Münchausen tõmbas ennast patsi pidi mülkast välja, genereeritakse bootstrappimisega algse valimi põhjal teststatistiku jaotus. "],
["bootstrap.html", "10 Bootstrap 10.1 Mõned tava-bootstrapi paketid Bayesi bootstrap Parameetriline bootstrap Bootstrappimine ei ole kogu tõde", " 10 Bootstrap library(tidyverse) library(rethinking) library(bayesboot) Populatsioon on valimile sama, mis on valim bootstrappitud valimile. Nüüd alustame ühestainsast empiirilisest valimist ja genereerime sellest 2000 virtuaalset valimit. Selleks tõmbame me oma valimist virtuaalselt 2000 uut juhuvalimit (bootstrap valimit), millest igaüks on sama suur kui algne valim. Trikk seisneb selles, et bootstrap valimite tõmbamine käib asendusega, st iga empiirilise valimi element, mis bootstrap valimisse tõmmatakse, pannakse kohe algsesse valimisse tagasi. Seega saab seda elementi kohe uuesti samasse bootstrap valimisse tõmmata (kui juhus nii tahab). Seega sisaldab tüüpiline bootstrap valim osasid algse valimi numbreid mitmes korduses ja teisi üldse mitte. Iga bootstrap valimi põhjal arvutatakse meid huvitav statistik (näiteks keskväärtus) ja kõik need 2000 bootstrapitud statistikut plotitakse samamoodi, nagu me ennist tegime valimitega lõpmata suurest populatsioonist. Ainsad erinevused on, et bootstrapis võrdub andmekogu suurus, millest valimeid tõmmatakse, valimi suurusega ning, et iga bootstrapi valim on sama suur kui algne andmekogu (sest meie statistiku varieeruvus sõltub valimi suurusest ja me tahame seda varieeruvust oma bootstrapvalimiga tabada). Tüüpiliselt kasutatakse bootstrapitud statistikuid selleks, et arvutada usaldusintervall statistiku väärtusele. Bootstrap ei muuda meie hinnangut statistiku punktväärtusele. Ta annab hinnangu ebakindluse määrale, mida me oma valimi põhjal peaksime tundma selle punkthinnangu kohta. Bootstrappimine on üldiselt väga hea meetod, mis sõltub väiksemast arvust eeldustest kui statistikas tavaks. Bootstrap ei eelda, et andmed on normaaljaotusega või mõne muu matemaatiliselt lihtsa jaotusega. Tema põhiline eeldus on, et valim peegeldab populatsiooni – mis ei pruugi kehtida väikeste valimite korral ja kallutatud (mitte-juhuslike) valimite korral. Lisaks, tavaline bootstrap ei sobi hierarhiliste andmestruktuuride analüüsiks ega näiteks aegridade analüüsiks. Bootstrappida saab edukalt enamusi statistikuid, mida te võiksite elu jooksul arvutada, aga on siiski erandeid: näiteks valimi maksimum ja miinimumväärtused. Bootstrap empiirilisele valimile suurusega n töötab nii: tõmba (asendusega) empiirilisest valimist B uut virtuaalset valimit (B bootstrap valimit), igaüks suurusega n. arvuta keskmine, sd või mistahes muu statistik igale bootstrapi valimile. Tee seda B korda. joonista oma statistiku väärtustest histogramm või density plot Nende andmete põhjal saab küsida palju toreidaid küsimusi — vt allpool. Mis on USA presidentide keskmine pikkus? Meil on viimase 11 presidendi pikkused. heights &lt;- tibble(value = c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185, 188)) B &lt;- 2000 #the number of bootstrap samples boot_mean &lt;- heights %&gt;% broom::bootstrap(B) %&gt;% do(summarise(., Mean = mean(value))) #&gt; Warning: &#39;broom::bootstrap&#39; is deprecated. #&gt; See help(&quot;Deprecated&quot;) ggplot(boot_mean, aes(Mean)) + geom_density() Joonis 10.1: Bootstrapitud posteerior USA presidentide keskmisele pikkusele. Järgnevas koodis ütleme me kõigepealt, et B = 2000 (et me võtame 2000 bootstrap valimit) kasutades selleks broom paketi käsku boot(), millele on lihtne lisada dplyri funktsioon summarise(). Siiski peame seda tegema dplyr::do() abil. Pane tähele, et “tänu” do() kasutamisele peame me summarise() funktsioonis näitama punktiga koha, kuhu lähevad %&gt;% torust tulnud bootstrap valimid. Aga muidu on kõik tavapärane tidyverse. Mida selline keskväärtuste jaotus tähendab? Me võime seda vaadelda posterioorse tõenäosusjaotusena. Selle tõlgenduse kohaselt iseloomustab see jaotus täpselt meie usku presidentide keskmise pikkuse kohta, niipalju kui see usk põhineb bootstrappimises kasutatud andmetel. Senikaua, kui meil pole muud relevantset teavet, on kõik, mida me usume teadvat USA presidentide keskmise pikkuse kohta, peidus selles jaotuses. Need pikkused, mille kohal jaotus on kõrgem, sisaldavad meie jaoks tõenäolisemalt tegelikku USA presidentide keskmist pikkust kui need pikkused, mille kohal posterioorne jaotus on madalam. Kuidas selle jaotusega edasi töötada? See on lihtne: meil on 2000 arvu (2000 bootstrapitud statistiku väärtust) ja me teeme nendega kõike seda, mida parasjagu tahame. Näiteks me võime arvutada, millisesse pikkuste vahemikku jääb 92% meie usust USA presidentide tõelise keskmise pikkuse kohta. See tähendab, et teades seda vahemikku peaksime olema valmis maksma mitte rohkem kui 92 senti pileti eest, mis juhul kui USA presidentide keskmine pikkus tõesti jääb sinna vahemikku, toob meile võidu suuruses 1 EUR (ja 8 senti kasumit). Selline kihlveokontor on täiesti respektaabel ja akadeemiline tõenäosuse tõlgendus; see on paljude arvates lausa parim tõlgendus, mis meil on. Miks just 92% usaldusinterval? Vastus on, et miks mitte? Meil pole ühtegi universaalset põhjust eelistada üht usaldusvahemiku suurust teisele. Olgu meil usaldusinteval 90%, 92% või 95% — tõlgendus on ikka sama. Nimelt, et me usume, et suure tõenäosusega jääb tegelik keskväärtus meie poolt arvutatud vahemikku. Mudeli ja maailma erinevused tingivad niikuinii selle, et konkreetne number ei kandu mudelist otse üle pärismaailma. Eelnevalt mainitud kihlveokontor töötab mudeli maailmas, mitte teie kodulähedasel hipodroomil. 92% usaldusintervalli arvutamiseks on kaks meetodit, mis enamasti annavad vaid veidi erinevaid tulemusi. HPDI — Highest Density Probability Interval — alustab jaotuse tipust (tippudest) ja katab 92% jaotuse kõrgema(te) osa(de) pindalast HPDI(heights$value, prob = 0.92) #&gt; |0.92 0.92| #&gt; 177 192 PI — Probability Interval — alustab jaotuse servadest ja katab kummagist servast 4% jaotuse pindalast. PI 90%-le on sama, mis arvutada 5% ja 95% kvantiilid (jne). PI(heights$value, prob = 0.90) #&gt; 5% 95% #&gt; 180 190 # quantile(heights$value, probs = c(0.05, 0.95)) teeb sama asja HPDI on üldiselt parem mõõdik kui PI, aga teatud juhtudel on seda raskem arvutada. Kui HPDI ja PI tugevalt erinevad, on hea mõte piirduda jaotuse enda avaldamisega — jaotus ise sisaldab kogu informatsiooni, mis meil on oma statistiku väärtuse kohta. Intervallid on lihtsalt summaarsed statistikud andmete kokkuvõtlikuks esitamiseks. Kui suure tõenäosusega on USA presidentide keskmine pikkus suurem kui USA populatsiooni meeste keskmine pikkus (178.3 cm mediaan)? mean(heights$value &gt; 178.3) #&gt; [1] 0.909 Ligikaudu 100% tõenäosusega (valimis on 1 mees alla 182 cm, ja tema on 177 cm). Lühikesed jupatsid ei saa Ameerikamaal presidendiks! Kuidas lahendada bootstrap, kui mei tahame usaldusintervalle kahe ebavõrdse grupi erinevusele? Näiteks kui meil on katsegrupis N = 25 ja kontrollgrupis N = 20, ja me tahame arvutada statistikut ES = katsegrupi keskmine - kontrollgrupi keskmine. tõbma katsegrupist N = 25 bootstrapvalim tõmba kontrollgrupist N = 20 bootsrapvalim lahuta kontrollgrupi bootstrapvalimi mediaan katsegrupi omast (või aritmeetriline keskmine või ükskõik mis muu keskmise näitaja, mida hing ihaldab) korda punkte 1-3 B korda ja tööta edasi bootstrapjaptusega, nagu eespool näidatud. 10.1 Mõned tava-bootstrapi paketid Professionaalid kasutavad boot paketti, mis on suhteliselt ebameeldiva süntaksiga, aga väga laialt rakendatav. Boot paketi peale on ehitatud tavainimesele hästi kasutatav pakett bootES (Kirby and Gelranc, 2013, Behav Res 45:905–927), mis teeb lihtsaks usalduspiiride leidmise erinevat tüüpi efekti suurustele, kaasa arvatud lihtsad hierarhilised ja ühefaktorilise ANOVA tüüpi katseskeemid. Nendes pakettides tasub üldjuhul kasutada meetodit nimega BCa (bias-corrected-and-accelerated) usalduspiiride arvutamiseks. See meetod püüab parandada bootstrap-valimite võimalikku kallutatust (esineb sedavõrd, kui bootstrap-jaotuse tipp ei ole samas kohas kui oleks paljude päris-valimite pealt arvutatud statistikute jaotuse tipp) ja olukorda, kus statistiku väärtuse varieeruvuse määr sõltub statistiku väärtusest. BCa edukaks arvutamiseks peab bootstrap valimite arv tuntavalt ületama valimi suurust. Simulatsioonidega on näidatud, et BCa (ja teisi) usalduspiire saab mõistlikult arvutada valimitelt, mille suurus on &gt; 15. Sellest väiksemate valimite korral peate eeldama, et teie usaldusinetvallid valetavad. Aegridade, kus esineb järjestikuste ajapunktide vahelisi sõltuvusi, tuleks kasutada nn block bootstrappi, mida implementerrib näiteks boot::tsboot(). Bayesi bootstrap Kui klassikalise bootstrap meetodi pakkus välja B. Efron aastal 1979, siis selle Bayesi versioon avaldati D.B. Rubini poolt 1981. a. Bayesi versioon bootstrapist on implementeeritud “bayesboot” paketis funktsioonis bayesboot(). Hea lihtsa seletuse Bayesi bootstrapi kohta saab siit https://www.youtube.com/watch?v=WMAgzr99PKE ja lihtsa r koodi selle meetodi rakendamiseks saab siit https://www.r-bloggers.com/simple-bayesian-bootstrap/. Lühidalt, erinevalt eelkirjeldatud tava-bootstrapist simuleeritakse Bayesi bootstrapis posterioorjaotused, näiteks arvutatakse kaalutud keskmine, kus ühtlasest jaotusest pärit kaalud on prioriks. Näited sellest, kuidas kasutada bayesbooti standardhälbe, korrelatsioonikoefitsiendi ja lineaarse mudeli koefitsientide usalduspiiride arvutamiseks leiate ?bayesboot käsuga. heights_bb &lt;- bayesboot(heights$value, mean) plot(heights_bb, compVal = 185) HPDI(heights_bb$V1, prob = 0.95) #&gt; |0.95 0.95| #&gt; 183 187 Joonis 10.2: Bayesi bootstrapi posteerior USA presidentide keskmisele pikkusele. Vaikimisi pannakse bayesboot() funktsioonis statistiku arvutamisel kaalud (prior) valimi indeksile, mis annab erineva tulemuse kui näiteks kaalutud keskmise arvutamisel, kus kaalud (prior) pannakse valimi väärtustele. Aritmeetilise keskmise Bayesi bootstrap väärtused kasutades kaalutud keskmise funktsiooni weighted.mean saab niimoodi: heights_bb_w &lt;- bayesboot(heights$value, weighted.mean, use.weights = TRUE) Tõenäosus, et keskmine on suurem kui 182 cm mean(heights_bb[, 1] &gt; 182) #&gt; [1] 0.993 Kahe keskväärtuse erinevus (ES = keskmine1 - keskmine2): set.seed(1) ## Simulate two random normal distributions with mean 0. ## True difference is 0. dfr &lt;- tibble(a = rnorm(10, 0, 1), b = rnorm(10, 0, 1), c = a - b) dfr_bb &lt;- bayesboot(dfr$c, weighted.mean, use.weights = TRUE ) plot(dfr_bb, compVal = 0) Joonis 10.3: Bayesi bootstrap ES-le. BayesianFirstAid raamatukogu funktsioon bayes.t.test() annab kasutades t-jaotuse tõepäramudelit üsna täpselt sama vastuse. See raamatukogu eeldab JAGS mcmc sämpleri installeerimist. Abi saab siit https://github.com/rasmusab/bayesian_first_aid ja siit https://faculty.washington.edu/jmiyamot/p548/installing.jags.pdf. Parameetriline bootstrap Kui me arvame, et me teame, mis jaotusega on meie andmed, ja meil on suhteliselt vähe andmepunkte, võib olla mõistlik lisada bootstrapile andmete jaotuse mudel. Näiteks, meie USA presidentide pikkused võiksid olla umbkaudu normaaljaotusega (sest me teame, et USA meeste pikkused on seda). Seega fitime kõigepealt presidentide pikkusandmetega normaaljaotuse ja seejärel tõmbame bootsrap valimid sellest normaaljaotuse mudelist. Normaaljaotuse mudelil on 2 parameetrit: keskmine (mu) ja standardhälve (sigma), mida saame fittida valimiandmete põhjal: mu &lt;- mean(heights$value) sigma &lt;- sd(heights$value) N &lt;- length(heights$value) sample_means &lt;- tibble(value = rnorm(N * 1000, mu, sigma), indeks = rep(1:1000, each = N)) sample_means_sum &lt;- sample_means %&gt;% group_by(indeks) %&gt;% summarise(Mean = mean(value)) ggplot(sample_means_sum, aes(x = Mean)) + geom_histogram(color = &quot;white&quot;, bins = 20) HPDI(sample_means_sum$Mean) #&gt; |0.89 0.89| #&gt; 183 187 Joonis 10.4: Parameetrilise bootstrapi posteerior USA presidentide keskmisele pikkusele. Üldiselt ei soovita me parameetrilist bootstrappi väga soojalt, sest täisbayesiaanlik alternatiiv, mida me kohe õppima asume, on sellest paindlikum. Joonis 10.5: Bootstrappimise meetodid. Bootstrappimine ei ole kogu tõde Bootstrappimine on võimas ja väga laia kasutusalaga meetodite kogum. Sellel on siiski üks oluline puudus. Nimelt arvestab bootstrap ainult andmetega ja ignoreerib taustateadmisi (parameetriline bootstrap küll eeldab taustateadmisetele tuginevalt jaotusmudelit, kuid ignoreerib kogu muud taustateadmist). Miks on see probleem? Mõtleme hetkeks sellele teadusliku meetodi osale, millel põhineb suuresti näiteks Darwini liikide tekkimise argument. See on nn inference to the best explanation, mille kohaselt on eelistatud see teooria, mis on parimas kooskõlas faktidega, ehk mille kehtimise korral on meie andmete esinemine kõige tõenäolisem. Kui mõni hüpotees omistab andmete esinemisele suure tõenäosuse, siis me ütleme tehnilises keeles, et see hüpotees on tõepärane (has high likelihood). Esmapilgul tundub see kõik igati mõistlik, kuid proovime lihtsat mõtteeksperimenti. Selles juhtub nii, et loteriil võidab peaauhinna meile tundmatu kodanik Franz K. Meil on selle fakti (ehk nende andmete) seletamiseks kaks teooriat: 1. Franz K. võit oli juhuslik (loterii oli aus ja keegi peab ju võitma) ja 2. Franz K. noorem õde võltsis loterii tulemusi oma venna kasuks. Teine teooria sobib andmetega palju paremini kui esimene (sest kuigi keegi peab võitma, Franz K. võiduvõimalus oli väga väike); aga ometi eelistab enamus mõistlikke inimesi esimest teooriat. Põhjus on selles, et meil pole iseenesest mingit alust arvata, et Franz K.-l üldse on noorem õde, või et see õde omaks ligipääsu loteriile. Kui me aga saame teada, et Franz K. noorem õde tõesti korraldab loteriid, siis leiame kohe, et asi on kahtlane. Siit näeme, et lisaks tõepärale on selleks, et me usuksime mõne teooria kehtimisse, vaja veel, et see teooria oleks piisavalt tõenäoline meie taustateadmiste valguses. Bayesi teoreem ei tee muud, kui arvutab teooria kehtimise posterioorse tõenäosuse (järeltõenäosuse), kasutades selleks meie eelteadmiste ja tõepära kvantitatiivseid mudeleid. Seega, Bayesi paradigmas ei arvesta me mitte ainult andmetega, vaid ka taustateadmistega, sünteesides need kokku üheks posterioorseks jaotuseks ehk järeljaotuseks. Selle jaotuse arvutamine erineb bootstrapist, kuid tema tõlgendus ja praktiline töö sellega on sarnane. Erinevalt tavapärasest bootstrapist on Bayes parameetriline meetod, mis sõltub andmete modelleerimisest modeleerija poolt ette antud jaotustesse (normaaljaotus, t jaotus jne). Tegelikult peame me Bayesi arvutuseks modelleerima vähemalt kaks erinevat jaotust: andmete jaotus, mida me kutsume likelihoodiks ehk tõepäraks, ning eelneva teadmise mudel ehk prior, mida samuti modeleeritakse tõenäosusjaotusena. Bootstrapil (tavalisel ja Bayesi versioonil) on mõned imelikud formaalsed eeldused: 1. väärtused, mis ei esine valimiandmetes, on võimatud, 2. Väärtused, mis esinevad väljaspool valimi väärtuste vahemikku, on võimatud, 3. andmetes ei esine ajasõltuvusi ega hierarhilisi struktuure. Nendest puudustest hoolimata kasutatakse bootstrappimist laialt ja edukalt — eelkõige tema lihtsuse ja paindlikuse tõttu. Küll aga tähendavad eelnimetatud puudused, et bootstrap on harva parim meetod teie ülesande lahendamiseks. Ehkki bootstrappimine ei arvesta taustateadmistega, ei tee seda olulisel määral ka paljud Bayesi mudelid (mudeldaja vaba valiku tõttu, mitte selle pärast, et mudel ei suudaks taustainfot inkorporeerida). Bayesi meetodite väljatöötajad ei tea sageli ette, milliste teaduslike probleemide lahendamiseks nende mudeleid hakatakse kasutama, ja seega ei kirjuta nad mudelisse ka väga ranget eelteadmist. Nende mudelite teadlastest kasutajad lepivad sageli selllega ja lasevad oma mudelite kaudu “andmetel kõneleda” enam-vähem sellistena, nagu need juhtuvad olema. Sellist lähenemist ei saa alati hukka mõista, sest vahest ei olegi meil palju eelteadmisi oma probleemi kohta, küll aga tuleb mainida, et sellistel juhtudel annab bootstrappimine sageli lihtsama vaevaga väga sarnase tulemuse, kui Bayesi täismäng. "],
["bayesi-pohimote.html", "11 Bayesi põhimõte Esimene näide Teine näide: sõnastame oma probleemi ümber Kui n = 1", " 11 Bayesi põhimõte Bayesi arvutuseks on meil vaja teada milline on “parameter space” ehk parameetriruum? Parameetriruum koosneb kõikidest loogiliselt võimalikest parameetriväärtustest. Näiteks kui me viskame ühe korra münti, koosneb parameetriruum kahest elemendist: 0 ja 1, ehk null kulli ja üks kull. See ammendab võimalike sündmuste nimekirja. Kui me aga hindame mõnd pidevat suurust (keskmine pikkus, tõenäosus 0 ja 1 vahel jms), koosneb parameetriruum lõpmata paljudest elementidest (arvudest). milline on “likelihood function” ehk tõepärafunktsioon? Me omistame igale parameetriruumi elemendile (igale võimalikule parameetri väärtusele) tõepära. Tõepära parameetri väärtusel x on tõenäosus, millega me võiksime kohata oma andmete keskväärtust, juhul kui x oleks see ainus päris õige parameetri väärtus. Teisisõnu, tõepära on kooskõla määr andmete ja parameetri väärtuse x vahel. Tõepära = \\(P(andmed~\\vert~parameetri~v\\ddot{a}\\ddot{a}rtus)\\). Näiteks, kui tõenäolised on meie andmed, kui USA keskmine president on juhtumisi 183.83629 cm pikkune? Kuna meil on vaja modeleerida tõepära igal võimalikul parameetri väärtusel (mida pideva suuruse puhul on lõpmatu hulk), siis kujutame tõepära pideva funktsioonina (näiteks normaaljaotusena), mis täielikult katab parameetriruumi. Tõepärafunktsioon ei summeeru 100-le protsendile — see on normaliseerimata. milline on “prior function” ehk prior? Igale tõepära väärtusele peab vastama priori väärtus. Seega, kui tõepära on modelleeritud pideva funktsioonina, siis on ka prior pidev funktsioon (aga prior ei pea olema sama tüüpi funktsioon, kui tõepära). Erinevus tõepära ja priori vahel seisneb selles, et kui tõepärafunktsioon annab just meie andmete keskväärtuse tõenäosuse igal parameetriväärtusel, siis prior annab iga parameetriväärtuse tõenäosuse, sõltumata meie andmetest. See-eest arvestab prior kõikide teiste relevantsete andmetega, sünteesides taustateadmised ühte tõenäousmudelisse. Me omistame igale parameetriruumi väärtusele eelneva tõenäosuse, et see väärtus on üks ja ainus tõene väärtus. Prior jaotus summeerub 1-le. Prior kajastab meie konkreetsetest andmetest sõltumatut arvamust, kui suure tõenäosusega on just see parameetri väärtus tõene; seega seda, mida me usume enne oma andmete nägemist. Nendel parameetri väärtustel, kus prior (või tõepära) = 0%, on ka posteerior garanteeritult 0%. See tähendab, et kui te olete 100% kindel, et mingi sündmus on võimatu, siis ei suuda ka mäekõrgune hunnik uusi andmeid teie uskumust muuta (eelduselt, et te olete ratsionaalne inimene). http://optics.eee.nottingham.ac.uk/match/uncertainty.php aitab praktikas priorit modelleerida (proovige Roulette meetodit). Kui te eelnevast päriselt aru ei saanud, ärge muretsege. Varsti tulevad puust ja punaseks näited likelihoodi ja priori kohta. Edasi on lihtne. Arvuti võtab tõepärafunktsiooni ja priori, korrutab need üksteisega läbi ning seejärel normaliseerib saadud jaotuse nii, et jaotusealune pindala võrdub ühega. Saadud tõenäosusjaotus ongi posteeriorne jaotus ehk posteerior ehk järeljaotus. Kogu lugu. Me teame juba pool sajandit, et Bayesi teoreem on sellisele ülesandele parim võimalik lahendus. Lihtsamad ülesanded lahendame me selle abil täiuslikult. Kuna parameetrite arvu kasvuga mudelis muutub Bayesi teoreemi läbiarvutamine eksponentsiaalselt arvutusmahukamaks (sest läbi tuleb arvutada mudeli kõikide parameetrite kõikide väärtuste kõikvõimalikud kombinatsioonid), oleme sunnitud vähegi keerulisemad ülesanded lahendama umbkaudu, asendades Bayesi teoreemi ad hoc MCMC algoritmiga, mis teie arvutis peituva propelleri Karlsoni kombel lendu saadab, et tõmmata valim “otse” posterioorsest jaotusest. Meie poolt kasutatava MCMC Hamiltonian Monte Carlo mootori nimi on Stan (www.mc-stan.org). See on eraldiseisev programm, millel on R-i liides R-i pakettide rstan(), rethinking(), rstanarm() jt kaudu. Meie töötame ka edaspidi puhtalt R-s, mis automaatselt suunab meie mudelid ja muud andmed Stani, kus need läbi arvutatakse ja seejärel tulemused R-i tagasi saadetakse. Tulemuste töötlus ja graafiline esitus toimub jällegi R-is. Seega ei pea me ise kordagi Stani avama. Alustame siiski lihtsa näitega, mida saab käsitsi läbi arvutada. Esimene näide Me teame, et suremus haigusesse on 50% ja meil on palatis 3 patsienti, kes seda haigust põevad. Seega on meil kaks andmetükki (50% ja n=3). Küsimus: mitu meie patsienti oodatavalt hinge heidavad? Eeldusel, et meie patsiendid on iseseisvad (näiteks ei ole sugulased), on meil tüüpiline mündiviske olukord. Parameetriruum on neljaliikmeline: 0 surnud, 1 surnud, 2 surnud ja 3 surnud. Edasi loeme üles kõik võimalikud sündmusteahelad, mis loogiliselt saavad juhtuda, et saada tõepärafunktsioon. Me viskame kulli-kirja 3 korda: kiri = elus, kull = surnud Võimalikud sündmused on: | kull kull kull | kull kiri kull | kiri kull kull | kull kull kiri | kull kiri kiri | kiri kiri kull | kiri kull kiri | kiri kiri kiri Kui P(kull) = 0.5, siis lugedes kokku kõik võimalikud sündmused: 0 kulli ehk surnud - 1, 1 kulli ehk surnud - 3, 2 kulli ehk surnud - 3, 3 kulli ehk surnud - 1 Nüüd teame parameetriruumi iga liikme kohta, kui suure tõenäosusega me ootame selle realiseerumist. Näiteks, P(0 surnud) = 1/8, P(1 surnud) = 3/8, P(1 või 2 surnud) = 6/8 jne Selle teadmise konverteerime tõepärafunktsiooniks. # Parameter space as a grid x &lt;- seq(from = 0, to = 3) # Likelihood y &lt;- c(1, 3, 3, 1) plot(x, y, ylab = &quot;Number of possibilities&quot;, xlab = &quot;Number of deaths&quot;, type = &quot;b&quot;, main = &quot;Likelihood&quot;) Joonis 11.1: Tõepärafunktsioon. Siit näeme, et üks surm ja kaks surma on sama tõenäolised ja üks surm on kolm korda tõenäolisem kui null surma (või kolm surma). Tõepära annab meile tõenäosuse Pr(mortality=0.5 &amp; N=3) igale loogiliselt võimalikule surmade arvule (0 kuni 3). Me saame sama tulemuse kasutades formaalsel viisil binoomjaotuse mudelit. Ainus erinevus on, et nüüd on meil y teljel surmade tõenäosus. y &lt;- dbinom(x, 3, 0.5) plot(x, y, type = &quot;b&quot;, xlab = &quot;Number of deaths&quot;, ylab = &quot;Probability of x deaths&quot;, main = &quot;Probability of x deaths out of 3 patients\\nif P(Heads) = 0.5&quot;) Joonis 11.2: Tõepärafunktsioon binoomjaotuse mudelist. Proovime seda koodi olukorras, kus meil on 9 patsienti ja suremus on 0.67: x &lt;- seq(from = 0, to = 9) y &lt;- dbinom(x, 9, 0.67) plot(x, y, type = &quot;b&quot;, xlab = &quot;Number of deaths&quot;, ylab = &quot;Probability of x deaths&quot;, main = &quot;Probability of x out of 9 deaths\\nif P(Heads) = 0.67&quot;) Joonis 11.3: Veel üks tõepärafunktsioon. Lisame sellele tõepärafunktsioonile tasase priori (lihtsuse huvides) ja arvutame posterioorse jaotuse kasutades Bayesi teoreemi. Igale parameetri väärtusele on tõepära * prior proportsionaalne posterioorse tõenäosusega, et just see parameetri väärtus on see ainus tõene väärtus. Posterioorsed tõenäosused normaliseeritakse nii, et nad summeeruksid 1-le. Me defineerime X telje kui rea 10-st arvust (0 kuni 9 surma) ja arvutame tõepära igale neist 10-st arvust. Sellega ammendame me kõik loogiliselt võimalikud parameetri väärtused. # Define grid x &lt;- seq(from = 0 , to = 9) # Define flat prior prior &lt;- rep(1 , 10) # Compute likelihood at each value in grid likelihood &lt;- dbinom(x, size = 9, prob = 0.67) # Compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # Normalize the posterior, so that it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) sum(posterior) == 1 #&gt; [1] TRUE plot(x, posterior, type = &quot;b&quot;, xlab = &quot;Number of deaths&quot;, ylab = &quot;Posterior probability&quot;, main = &quot;Posterior distribution&quot;) Joonis 11.4: Posteerior. See on parim võimalik teadmine, mitu kirstu tasuks tellida, arvestades meie priori ja likelihoodi mudelitega. Näiteks, sedapalju, kui surmad ei ole üksteisest sõltumatud, on meie tõepäramudel (binoomjaotus) vale. Teine näide: sõnastame oma probleemi ümber Mis siis, kui me ei tea suremust ja tahaksime seda välja arvutada? Kõik, mida me teame on, et 6 patsienti 9st surid. Nüüd koosnevad andmed 9 patsiendi mortaalsusinfost (parameeter, mille väärtust me eelmises näites arvutasime) ja parameeter, mille väärtust me ei tea, on surmade üldine sagedus (see parameeter oli eelmises näites fikseeritud, ja seega kuulus andmete hulka). Seega on meil: Parameetriruum 0% kuni 100% suremus (0st 1-ni), mis sisaldab lõpmata palju numbreid. Kaks võimalikku sündmust (surnud, elus), seega binoomjaotusega modelleeritud tõepärafunktsioon. Nagu me juba teame, on r funktsioonis dbinom() kolm argumenti: surmade arv, patsientide koguarv ja surmade tõenäosus. Seekord oleme me fikseerinud esimesed kaks ja soovime arvutada kolmanda väärtuse. Tasane prior, mis ulatub 0 ja 1 vahel. Me valisime selle priori selleks, et mitte muuta tõepärafunktsiooni kuju. See ei tähenda, et me arvaksime, et tasane prior on mitteinformatiivne. Tasane prior tähendab, et me usume, et suremuse kõik väärtused 0 ja 1 vahel on võrdselt tõenäolised. See on vägagi informatsioonirohke (ebatavaline) viis maailma näha, ükskõik mis haiguse puhul! Tõepära parameetri väärtusel x on tõenäosus kohata meie andmeid, kui x on juhtumisi parameetri tegelik väärtus. Meie näites koosneb tõepärafunktsioon tõenäosustest, et kuus üheksast patsiendist surid igal võimalikul suremuse väärtusel (0…1). Kuna see on lõpmatu rida, teeme natuke sohki ja arvutame tõepära 20-l valitud suremuse väärtusel. Tehniliselt on sinu andmete tõepärafunktsioon agregeeritud iga üksiku andmepunkti tõepärafunktsioonist. Seega vaatab Bayes igat andmepunkti eraldi (andmete sisestamise järjekord ei loe). # Define grid (mortality at 20 evenly spaced probabilities from 0 to 1) x &lt;- seq(from = 0 , to = 1, length.out = 20) # Define prior prior &lt;- rep(1 , 20) # Compute likelihood at each value in grid likelihood &lt;- dbinom(6, size = 9 , prob = x) # Plot prior plot(x, prior, type = &quot;b&quot;, main = &quot;Prior&quot;) # Plot likelihood plot(x, likelihood, type = &quot;b&quot;, main = &quot;The likelihood function&quot;) # Compute product of likelihood and prior &amp; standardize the posterior posterior &lt;- likelihood * prior / sum(likelihood * prior) # Plot posterior plot(x, posterior, type = &quot;b&quot;, xlab = &quot;Mortality&quot; , ylab = &quot;Posterior probability&quot;, main = &quot;Posterior distribution&quot;) Joonis 11.5: Prior, tõepära ja posteerior. Nüüd on meil posterioorne tõenäosusfunktsioon, mis summeerub 1-le ja mis sisaldab kogu meie teadmist suremuse kohta. Alati on kasulik plottida kõik kolm funktsiooni (tõepära, prior ja posteerior). Kui n = 1 Bayes on lahe sest tema hinnangud väiksele N-le on loogiliselt sama pädevad kui suurele N-le. See ei ole nii klassikalises sageduslikus statistikas, kus paljud testid on välja töötatud N = Inf eeldusel ja töötavad halvasti väikeste valimitega. Hea küll, me arvutame jälle suremust. Bayes töötab andmepunkti kaupa (see et me talle ennist kõik andmed korraga ette andsime, on puhtalt mugavuse pärast). # Define grid x &lt;- seq(from = 0, to = 1, length.out = 20) # Define prior prior &lt;- rep(1, 20) # Compute likelihood at each value in grid likelihood &lt;- dbinom(1, size = 1, prob = x) posterior &lt;- likelihood * prior / sum(likelihood * prior) plot(x, posterior, type = &quot;b&quot;, xlab = &quot;Mortality&quot;, ylab = &quot;Posterior probability&quot; ) Joonis 11.6: N=1, esimene patsient suri. Esimene patsient suri - 0 mortaalsus ei ole enam loogiliselt võimalik (välja arvatud siis kui prior selle koha peal = 0) ja mortaalsus 100% on andmetega (tegelikult andmega) parimini kooskõlas. Posteerior on nulli ja 100% vahel sirge sest vähene sissepandud informatsioon lihtsalt ei võimalda enamat. # Define prior prior &lt;- posterior # Compute likelihood at each value in grid likelihood &lt;- dbinom(1 , size = 1, prob = x) posterior1 &lt;- likelihood * prior / sum(likelihood * prior) plot(x, posterior1, type = &quot;b&quot;, xlab = &quot;Mortality&quot; , ylab = &quot;Posterior probability&quot; ) Joonis 11.7: N=2, teine patsient suri. Teine patsient suri. Nüüd ei ole 0 ja 1 vahel enam sirge posteerior. Posteerior on kaldu 100 protsendi poole, mis on ikka kõige tõenäolisem väärtus. # Define prior prior &lt;- posterior1 # Compute likelihood at each value in grid likelihood &lt;- dbinom(0, size = 1, prob = x) # Compute product of likelihood and prior posterior2 &lt;- likelihood * prior / sum(likelihood * prior) plot(x, posterior2, type = &quot;b&quot;, xlab = &quot;Mortality&quot;, ylab = &quot;Posterior probability&quot;) Joonis 11.8: N=3, kolmas patsient jäi ellu. Kolmas patsient jäi ellu - 0 ja 100% mortaalsus on seega võimaluste nimekirjast maas ning suremus on ikka kaldu valimi keskmise poole (75%). Teeme sedasama prioriga, mis ei ole tasane. See illustreerib tõsiasja, et kui N on väike siis domineerib prior posteerior jaotust. (Suure N korral on vastupidi, priori kuju on sageli vähetähtis.) # Define prior prior &lt;- c(seq(1:10), seq(from = 10, to = 1)) # Compute likelihood at each value in grid likelihood &lt;- dbinom(1, size = 1 , prob = x) posterior &lt;- likelihood * prior / sum(likelihood * prior) def.par &lt;- par(no.readonly = TRUE) par(mfrow = c(1, 2)) plot(1:20, prior, type = &quot;b&quot;, main = &quot;Prior&quot;) plot(x, posterior, type = &quot;b&quot;, xlab = &quot;Mortality&quot;, ylab = &quot;Posterior probability&quot;, main = &quot;Posterior&quot;) par(def.par) Joonis 11.9: N=1 informatiivse prioriga. patsient suri # Define prior prior &lt;- posterior # Compute likelihood at each value in grid likelihood &lt;- dbinom(1, size = 1, prob = x) # Compute product of likelihood and prior posterior1 &lt;- likelihood * prior / sum(likelihood * prior) def.par &lt;- par(no.readonly = TRUE) par(mfrow = c(1, 2)) plot(1:20, prior, type = &quot;b&quot;, main = &quot;Prior&quot;) plot(x, posterior1, type = &quot;b&quot;, xlab = &quot;Mortality&quot;, ylab = &quot;Posterior probability&quot;, main = &quot;Posterior&quot;) par(def.par) Joonis 11.10: N=2 informatiivse prioriga. Teine patsient suri. # Define prior prior2 &lt;- posterior1 # Compute likelihood at each value in grid likelihood &lt;- dbinom(0, size = 1, prob = x) # Compute product of likelihood and prior posterior2 &lt;- likelihood * prior2 / sum(likelihood * prior2) def.par &lt;- par(no.readonly = TRUE) par(mfrow = c(1, 2)) plot(1:20, prior2, type = &quot;b&quot;, main = &quot;Prior&quot;) plot(x, posterior2, type = &quot;b&quot;, xlab = &quot;Mortality&quot;, ylab = &quot;Posterior probability&quot;, main = &quot;Posterior&quot;) par(def.par) Joonis 11.11: N=3 informatiivse prioriga. Kolmas patsient jäi ellu. Nüüd on posteeriori tipp mitte 75% juures nagu ennist, vaid kuskil 50% juures — tänu priorile. "],
["mudelite-keel.html", "12 Mudelite keel Beta prior Prioritest üldiselt", " 12 Mudelite keel Siin vaatame kuidas kirjeldada mudelit nii, et masin selle ära tunneb. Meie mudelid töötavad läbi “rethinking” paketi. See raamatukogu pakub kaks võimalust, kuidas mudelit arvutada, mis mõlemad kasutavad sama notatsiooni. Mõlemad võimalused arvutavad posteeriori mitte Bayesi teoreemi kasutades (nagu me ennist tegime), vaid kasutades stohhastilisi meetodeid, mis iseloomustavad posteeriori umbkaudu (aga piisavalt täpselt). Põhjuseks on, et keerulisemate mudelite korral on Bayesi teoreemi kasutamine liialt arvutusmahukas. Esiteks rethinking::map() leiab posteeriori tipu ja selle lähedal funktsiooni tõusunurga. Siin on eelduseks, et posteerior on normaaljaotus. See eeldus kehtib alati, kui nii prior kui tõepära on modelleeritud normaaljaotusena (ja ka paljudel muudel juhtudel). Teine võimalus on rethinking::map2stan(), mis suunab teie kirjutatud mudeli Stan-i. Stan teeb Hamilonian Monte Carlo simulatsiooni, kasutades valget maagiat selleks, et tõmmata valim otse posteerioorsest jaotusest. See on väga moodne lähenemine statistikale, töötab oluliselt aeglasemalt kui map, aga ei sõltu normaaljaotustest ning suudab arvutada hierarhilisi mudeleid, mis map-le üle jõu käivad. Me võime sama mudeli kirjelduse sööta mõlemasse funktsiooni. Lihtne mudel näeb välja niimodi: dead ~ dbinom(9, p) # binomial likelihood p ~ dunif(0, 1) # uniform prior Tõepärafunktsioon on modeleeritud binoomjaotusena. Parameeter, mille väärtust määratakse on p, ehk suremus. See on ainus parameeter, mille väärtust me siin krutime. NB! igale parameetrile peab vastama oma prior. Meil on selles mudelis täpselt 1 parameeter ja 1 prior. Vastuseks saame selle ainsa parameetri posterioorse jaotuse. Hiljem näeme, et kui meil on näiteks 452 parameetrit, mille väärtusi me koos arvutame, siis on meil ka 452 priorit ja 452 posterioorset jaotust. library(rethinking) # Fit model using rethinking m1 &lt;- map( alist( dead ~ dbinom(9, p), # Binomial likelihood p ~ dunif(0, 1) # Uniform prior ), data = list(dead = 6)) # Summary of quadratic approximation precis(m1) #&gt; Mean StdDev 5.5% 94.5% #&gt; p 0.67 0.16 0.42 0.92 Nüüd tõmbame posteerioroorsest jaotusest valimi n=10 000. Selleks on funktsioon extract.samples() samples &lt;- extract.samples(m1) # hist(samples$p) dens(samples$p) HPDI(samples$p, prob = 0.95) # Highest density 95% at the center #&gt; |0.95 0.95| #&gt; 0.371 0.986 Joonis 12.1: Tõmbame valimi posteeriorist. Kuus patsienti üheksast surid ja nüüd me usume, et tegelik suremus võib olla nii madal kui 37% ja nii kõrge kui 97%. Kui me tahame paremat hinnangut on meil vaja kas rohkem patsiente või informatiivsemat priorit (paremat taustainfot). m2 &lt;- map( alist( dead ~ dbinom(90, p), # Binomial likelihood p ~ dunif(0, 1) # Uniform prior ), data = list(dead = 60)) # Display summary of quadratic approximation precis(m2) #&gt; Mean StdDev 5.5% 94.5% #&gt; p 0.67 0.05 0.59 0.75 samples &lt;- extract.samples(m2) dens(samples$p) #PI(samples$p, prob = 0.95) # Leaves out equal 2.5% at both sides HPDI(samples$p, prob = 0.95) # Highest density 95% at the center #&gt; |0.95 0.95| #&gt; 0.569 0.764 Joonis 12.2: Veel üks valim posteeriorist (60 surma 90st). 10 korda rohkem andmeid: nüüd on suremus määratud kuskile 57% ja 77% vahele (suure tõenäosusega) Beta prior Nüüd anname sisse mõistlikuma struktuuriga priori: beta-jaotuse Beta-prior katab vahemiku 0st 1ni ja sellel on 2 parameetrit, a ja b. Siin mõned näited erinevatest beta parametriseeringutest. Joonis 12.3: Beta jaotuse parametriseeringuid. \\(beta(\\theta~\\vert~a,b)\\) jaotuse keskväärtus on \\(\\mu = a/(a + b)\\) ja mood on \\(\\omega= (a-1)/(a + b-2)\\) (kui \\(a &gt; 1\\) ja \\(b &gt; 1\\)). Seega, kui a = b, siis on keskmine ja mood 0.5. Kui a &gt; b, on keskmine ja mood &gt; 0.5 ja kuid a &lt; b, on mõlemad &lt; 0.5. Beta jaotuse “laiuse” annab “kontsentratsioon” \\(\\kappa = a + b\\). Mida suurem \\(\\kappa\\), seda kitsam jaotus. \\(a = \\mu\\kappa\\) \\(b = (1-\\mu)\\kappa\\) \\(a = \\omega(\\kappa-2) + 1\\) \\(b = (1-\\omega)(\\kappa-2)+1\\) kui \\(\\kappa &gt; 2\\) Me võime \\(\\kappa\\)-le omistada väärtuse nagu see oleks mündivisete arv, mis iseloomustab meie priori tugevust (juhul kui tõepära funktsioon tuleb andmetest, mis koosnevad selle sama mündi visetest). Kui meie jaoks piisaks ainult mõnest mündiviset, et priorist (eelnevast teadmisest) lahti ütelda, peaks meie prior sisaldama väikest kappat. Näiteks, mu prior on, et münt on aus (\\(\\mu = 0.5\\); \\(a = b\\)), aga ma ei ole selles väga veendunud. Niisiis ma arvan, et selle eelteadmise kaal võrdub sellega, kui ma oleksin näinud 8 mündiviske tulemust. Seega \\(\\kappa = 8\\), mis tähendab, et \\(a = \\mu\\kappa = 4\\) ja \\(b = (1-\\mu)\\kappa = 4\\). Aga mis siis kui me tahame beta priorit, mille mood \\(\\omega = 0.8\\) ja \\(\\kappa = 12\\)? Siis saame valemist, et \\(a = 9\\) ja \\(b = 3\\). ## Fit model m3 &lt;- rethinking::map( alist( dead ~ dbinom(9, p), # Binomial likelihood p ~ dbeta(200, 100) # Beta prior ), data = list(dead = 6)) ## Extract samples samples &lt;- extract.samples(m3) # Display summary of quadratic approximation precis(m3) #&gt; Mean StdDev 5.5% 94.5% #&gt; p 0.67 0.03 0.62 0.71 HPDI(samples$p, prob = 0.95) # Highest density 95% at the center #&gt; |0.95 0.95| #&gt; 0.614 0.720 dens(samples$p) Joonis 12.4: Posteerior, mis on arvutatud beta prioriga binoomsest tõepäramudelist. Nagu näha on ka kitsa priori mõju üsna väika, isegi kui kui n = 9. Prioritest üldiselt Neid võib jagada kolmeks: mitteinformatiivsed, väheinformatiivsed ehk “regularizing” ja informatiivsed. Mitteinformatiivseid prioreid ei ole sisuliselt olemas ja neid on soovitav vältida. Sageli kutsutakse tasaseid prioreid mitteinformatiivseteks. Neil on vähemalt 2 puudust. Tasane prior, mis ulatub lõpmatusse, on tehniliselt “improper”, sest tema alune pindala ei summeeru ühele. Ja teiseks muudavad sellised priorid mcmc ahelad vähem efektiivseteks, mis võib teie arvutuse kihva keerata. Väheinformatiivsed priorid kujutavad endast kompromissi: nad muudavad võimalikult vähe tõepärafunktsiooni kuju, aga samas piiravad seda osa parameetriruumist, kust MCMC ahelad posteeriori otsivad (mis on soodne arvutuslikult). Nende priorite taga on filosoofiline eeldus, et teadlast huvitavad eelkõige tema enda andmed ja see, mida need ühe või teise hüpoteesi (parameetri väärtuse) kohta ütlevad. See eeldus on vaieldav, aga kui selle järgi käia, siis kulub vähem mõttejõudu eelteadmiste mudelisse formaliseerimiseks. Nõrgalt regulariseerivad priorid on väheinformatiivsete priorite alamliik, mis on tsentreeritud nullile ja tõmbavad posteeriorit õrnalt nulli suunas. Vähemalt suured farmaatsiafirmad seda hoiakut ei jaga ja kulutavad oma miljoneid informatiivsete priorite tootmiseks. Selles protsessis saavad kokku statistikud, teaduseksperdid ja psühholoogid, et inimkonna teadmisi võimalikult adekvaatselt vormida tõenäosusjaotustesse. Meie töötame siiski enamasti väheinformatiivsete prioritega. "],
["lihtne-normaaljaotuse-mudel.html", "13 Lihtne normaaljaotuse mudel Kui lai on meie tõepärafunktsioon? Lihtne või robustne normaalne mudel? MCMC ahelate kvaliteet Lineaarne regressioon lm() - vähimruutude meetodiga fititud lineaarsed mudelid", " 13 Lihtne normaaljaotuse mudel Kui me eelmises peatükis modelleerisime diskreetseid binaarseid sündmusi (elus või surnud) üle binoomjaotuse, siis edasi tegeleme pidevate suurustega ehk parameetritega, millele saab omistada iga väärtuse vahemikus -Inf kuni Inf. library(tidyverse) library(stringr) library(gapminder) library(rethinking) library(bayesplot) library(gridExtra) Proovime veelkord USA presidentide keskmist pikkust ennustada (sama näide oli bootstrappimisel). Selleks on meil on vaja kahte asja: (1) tõepära mudelit ning (2) igale tõepära mudeli parameetrile oma priorit. Selline on täismudeli (tõepära ja priorid) struktuur: heights ~ dnorm(mu, sigma) # normal likelihood mu ~ dnorm(mean = 0, sd = 200) # normal prior for mean sigma ~ dcauchy(0, 20) #half-cauchy prior for sd Tõepära on siin modeleeritud normaaljaotusena, milles on 2 tuunitavat parameetrit: mu (keskmine) ja sigma (standardhälve). Pelgalt nende kahe parameetri fikseerimine annab meile unikaalse normaaljaotuse. See, et keskmise pikkuse prior on tsentreeritud nullile viib õige pisukesele (nõnda laia priori juures küll pigem märkamatule) mu hinnangu nihkumisele nulli suunas. Selle nihke õigustus on püüd vältida mudeli üle-fittimist ehk teisisõnu ülespoole kallutatud hinnangut keskmisele pikkusele. Sama hästi võiksime kasutada ka priorit mu ~ dnorm(mean = 178, sd = 10), kus 178 on ameerika meeste keskmine pikkus. Alati tasub mudeli priorid välja plottida, et veenduda, et nad tõesti kajastavad meile taustateadmisi ja on sobivas parameetrivahemikus (bayesi programmide default priorid on sageli kas liiga laiad või vastupidi eeldavad, et parameetriväärtused jäävad alla 10 ühiku). x &lt;- 0:100 y &lt;- dcauchy(x, 0, 20) plot(y ~ x, type = &quot;l&quot; , main = &quot;Cauchy prior for sd&quot;) Joonis 13.1: Cauchy prior. x &lt;- 150:200 y &lt;- dnorm(x, 0, 200) plot(y ~ x, type = &quot;l&quot;, main = &quot;Normal prior for mean = 0 and sd = 200&quot;) x &lt;- 150:200 y &lt;- dnorm(x, 178, 10) plot(y ~ x, type = &quot;l&quot;, main = &quot;Normal prior for mean = 178 and sd = 10&quot;) Joonis 13.2: Kaks normaaljaotuse priorit. Siin on valida kahe priori vahel mu-le. Võib-olla eelistaksid sina mõnda kolmandat? Kui jah, siis pole muud kui tee valmis ja kasuta! Sama hästi võiksime tõepära modelleerida ka mõne muu jaotusega (Studenti t jaotus, eksponentsiaalne jaotus, lognormaaljaotus jne). Sel juhul oleksid meil erinevad parameetrid, mida tuunida, aga põhimõte on sama. Bayes on modulaarne — kui sa põhimõtet tead, pole tehniliselt suurt vahet, millist mudelit soovid kasutada. Näiteks: heights ~ student_t(nu, mu, sigma) # t likelihood nu ~ dunif(1, 100) # uniform prior for the shape parameter mu ~ dnorm(mean = 0, sd = 200) # normal prior for mean sigma ~ dcauchy(0, 20) # half-cauchy prior for sd Normaaljaotusel on kaks parameetrit, millele posteerior arvutada: mu (mean) ja sigma (sd). Seega on vaja ka kahte priorit, üks mu-le ja teine sigma-le. Studenti t jaotuse korral lisandub veel üks parameeter: nu ehk jaotuse kuju määrav parameeter. nu-d saab tuunida 1 ja lõpmatuse vahel. Mida väiksem on nu, seda paksemad tulevad jaotuse sabad. Kui nu on suur, siis on t jaotuse kuju sama, mis normaaljaotusel. Siin andsime nu-le tasase priori 1 ja 100 vahel, hiljem proovime ka teisi prioreid nu-le. Studenti t jaotus on põnev alternatiiv normaaljaotusele, sest see on vähem tundlik outlieritele. Kuna normaaljaotus langeb servades väga kiiresti siis, kui meil on mõni andmepunkt, mis jääb jaotuse tipust kaugele, on ainus võimalus selle punkti normaaljaotuse alla mahutamiseks omistada jaotusele väga suur standardhälve. See muudab outlierit sisaldava normaaljaotuse ülemäära laiaks, mis viib analüüsis asjatult kaotatud efektidele. Seevastu t jaotuse sabasid saab nu abil üles-alla liigutada vastavalt sellele, kas andmed sisaldavad outliereid (selleks tuleb lihtsalt fittida nu parameeter andmete põhjal). Outlierid toovad meile paksema sabaga jaotuse, mis tipu ümber ei lähe aga kaugeltki nii laiaks, kui samade andmetega fititud normaaljaotus. Kui lai on meie tõepärafunktsioon? Normaaljaotusega modelleeritud tõepärafunktsioon on normaaljaotus, mille keskväärtus = mean(valim) ja mille standardhälve = sd(valim) / sqrt(N), kus N on valimi suurus. See tõepärafunktsioon modelleerib meie valimi keskväärtuse kohtamise tõenäosust igal võimalikul parameetriväärtusel. Kui oleme huvitatud USA presidentide keskmisest pikkusest, siis tõepärafunktsioon ütleb iga võimaliku pikkuse kohta, millise tõenäosusega kohtaksime oma valimi keskväärtust juhul, kui just see oleks tegelik presidentide keskmine pikkus. Sigma, mille posteeriori me mudelist arvutame, on aga standardhälve algsete andmepunktide tasemel. See on väga oluline eristus, sest sigma kaudu saab simueerida uusi andmepunkte. Lihtne või robustne normaalne mudel? Proovime mudeldada simuleeritud andmete keskväärtust. set.seed(890775) a &lt;- rnorm(20, mean = 0, sd = 1) # expected mean = 0, sd = 1 b &lt;- c(a, 5, 9) # plus 2 outliers Siin kasutame andmeid, mille keskväärtus on 0.38 ja sd = 1 ja millele on lisatud kaks outlierit (5 ja 9). Proovime neid andmeid mudeldada normaaljaotusega tõepäramudeliga ja seejärel üle studenti t jaotuse. Me fitime 4 mudelit, neist 3 koos outlieritega. Mudeli fittimine käib nii, et mcmc ahelad sammuvad parameetriruumis ja iga samm annab meile ühe juhusliku väärtuse posteeriorist. Defaultina on meil üks ahel, mis teeb 1000 sammu (seda saab muuta: vt ?map2stan). Kuna ahelad veedavad rohkem aega seal, kus posterioorne tõenäosuspilv on tihedam, siis saab nõnda sämplitud posteeriori juhuvalimi histogrammist posterioorse jaotuse kuju. Veelgi enam, selle asemel, et tegeleda posterioorsete jaotuste matemaatilise analüüsiga (integreerimisega) võime analüüsida oma mcmc sämpleid otse, mis tähendab, et kõrgema matemaatika asemel vajame 2. klassi aritmeetikat. Kõigepealt ilma outlieriteta mudel normaalse tõepärafuktsiooniga. Me kasutame sd priorina pool-Cauchy jaotust, mille tipp on 0 kohal ja millel on piisavalt paks saba suuremate numbrite poole. See on väheinformatiivne prior, mis on nähtud sd-de puhul mcmc algoritmides hästi töötavat. Andmed võime map2stan() funktsiooni sisestada nii listina kui data.frame-na (aga mitte tibble kujul). # Ilma outlierita andmed m0 &lt;- map2stan( alist( y ~ dnorm(mu, sigma), # normal likelihood mu ~ dnorm(0, 5), # normal prior for mean sigma ~ dcauchy(0, 2.5) # half-cauchy prior from sd ), data = list(y = a)) Sama mudel, aga outlieritega andmed. map2stan() tõlgib sisestatud mudeli Stan keelde ja see mudel kompileeritakse C++ keelde, milles on kodeeritud Stani mcmc mootor. Kuna kompileerimine on ajakulukas, kasutame m1 fittimiseks rstan raamatukogu (see loetakse sisse rethinkingu depency-na) ja juba komplieeritud m0 mudelit, millele lisame andmed kahe elemendina: N annab andmete arvu ja y tegelikud andmeväärtused. Selline andmete sisestamise viis on omane Stanile - map2stan() arvutab ise kapoti all N-i. m1 &lt;- stan(fit = m0@stanfit, data = list(N = length(b), y = b), chains = 4) Nüüd studenti t jaotusega tõepäramudel. Argumendid cores = 4, chains = 4 tähendavad, et me jooksutame 4 mcmc ahelat kasutades selleks oma arvuti 4 tuuma. Mudeli m2 juures tähendab argument constraints(list(nu = “lower=1”)), et mcmc sämpleri ahelad ei lähe kunagi allapoole ühte. See on siin kuna definitsiooni kohaselt ei saa nu olla väiksem kui 1. Argument start annab listi, mis annab iga parameetri jaoks väärtuse, millest mcmc ahel posteeriori sämplimist alustab. See on vahest vajalik, sest kui mcmc ahelad hakkavad posteeriori tõenäosuspilve otsima kaugel selle tegelikust asukohast n-mõõtmelises ruumis (n = mudeli parameetrite arv), siis võib juhtuda, et mudeli fittimine ebaõnnestub ja te saate veateate. m2 &lt;- map2stan( alist( y ~ student_t(nu, mu, sigma), nu ~ dnorm(5, 10), mu ~ dnorm(0, 5), sigma ~ dcauchy(0, 2.5) ), data = list(y = b), constraints = list(nu = &quot;lower=1&quot;), start = list(mu = mean(b), sigma = sd(b), nu = 10), cores = 4, chains = 4 ) m2 &lt;- readRDS(&quot;data/stan_m2.rds&quot;) m2@stanfit #&gt; Inference for Stan model: y ~ student_t(nu, mu, sigma). #&gt; 4 chains, each with iter=2000; warmup=1000; thin=1; #&gt; post-warmup draws per chain=1000, total post-warmup draws=4000. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; mu 0.28 0.00 0.22 -0.11 0.14 0.27 0.41 0.76 2066 1 #&gt; sigma 0.78 0.01 0.27 0.39 0.59 0.74 0.92 1.40 1254 1 #&gt; nu 2.21 0.04 1.48 1.04 1.39 1.84 2.53 5.57 1644 1 #&gt; dev 78.68 0.10 3.35 74.84 76.19 77.81 80.25 87.47 1057 1 #&gt; lp__ -27.23 0.04 1.45 -31.06 -27.86 -26.88 -26.16 -25.55 1323 1 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Mon Oct 23 15:51:33 2017. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). Ja viimasena studenti t mudel, kus nu on fikseeritud konstandina. Kuna me ei fiti nu-d mudeli parameetrina, pole meil vaja ka priorit nu-le. Me teeme selle mudeli, sest nu täpsel väärtusel pole väga suurt mõju tulemustele. Me lihtsalt fikseerime nu suvalisele väärtusele, mis annab t jaotusele piisavalt paksud sabad. m3 &lt;- map2stan( alist( y ~ student_t(4, mu, sigma), mu ~ dnorm(0, 5), sigma ~ dcauchy(0, 2.5) ), data = list(y = b), constraints = list(nu = &quot;lower=1&quot;), start = list(mu = mean(b), sigma = sd(b)), cores = 4, chains = 4) Üks esimesi asju mida koos parameetrite vaatamisega teha on lisaks vaadata, kas ka ahelad konvergeerusid. Selleks saab mugavalt kasutada rethinking::tracerplot() funktsiooni. tracerplot(m2) Joonis 13.3: Traceplot markovi ahelate inspekteerimiseks. Pildilt on näha, et neli ahelat (4 värvi) on hästi konvergeerunud. Hall ala on nn warmup ala, mille tulemusi ei salvestata. Muidu astub iga ahel sammu kaupa ja iga edukas samm salvestatakse ühe posteeriori väärtusena. Ahel sämplib korraga mu, sigma ja nu väärtusi n-mõõtmelises ruumis (n = mudeli parameetrite arv), mis tähendab, et ahela iga samm salvestatakse n kõrvuti numbrina. Kui näit sigma kõrgema väärtusega kaasneb keskeltäbi kõrgem (või madalam) mu väärtus, on sigma ja mu omavahel korreleeritud. Et kontrollida parameetrite posterioorsete väärtuste korrelatsioone kasutame funktsiooni rethinking::pairs(): pairs(m2) Joonis 13.4: Korrelatsiooniplot mudeli parameetritele. Normaaljaotus on selle poolest eriline, et tema parameetrid mu ja sigma ei ole korreleeritud. Paljud teised mudelid ei ole nii lahked. Siin on meil mõõdukas korrelatsioon nu ja sigma vahel. See on igati loogiline ja ei häiri meid. MCMC ahelate kvaliteet Kui Rhat on 1, siis see tähendab, et MCMC ahelad on ilusti jooksnud ja posteeriori sämplinud. Kui Rhat &gt; 1.1, siis on probleem. Suur Rhat viitab, et ahel(ad) pole jõudnud konvergeeruda. Kui ahelad ei konvergeeru, siis võib karta, et nad ei sämpli ka sama posteeriori jaotust. Kontrolli, kas mudeli kood ei sisalda vigu. Kui ei, siis vahest aitab, kui pikendada warm-up perioodi (map2stan(..., iter = 3000, warmup = 2000) pikendab default warm-upi 2 korda). Vahest aitab mudeli re-parametriseerimine (siin on lihtne trikk tekitada priorid, mis ei erineks väga palju oma vahemiku poolest; sellega kaasneb sageli andmete tsentreerimine või standardiseerimine; vt allpool). n_eff on efektiivne valimi suurus, mis hindab iseseisvalt sämplitud andmete arvu ning see ei tohi olla väga väike. Kui n_eff on palju väiksem kui jooksutatud markovi ahela pikkus (iga ahel on defaultina 1000 iteratsiooni pikk), on ahel jooksnud ebaefektiivselt. See ei tähenda tingimata, et posteerior vale oleks. Reegilina peaks Neff/N &gt; 0.1 Ahelad peavad plotitud kujul välja nägema nagu karvased tõugud, mis on ilma paljaste laikudeta. Kui ahelad omavad pikki sirgeid lõike (n_eff tuleb siis väga madal), kus ahel ei ole töötanud, siis see rikub korralikult posteeriori. Tüüpiliselt aitavad nõrgalt informatiivsed priorid — priorite õige valik on sama palju arvutuslik vajadus kui taustainfo lisamine. Igal juhul tuleb vältida aladefineeritud tasaseid prioreid, mis võimaldavad ahelatel sämplida lõpmatust ja sel viisil õige tee kaotada. Peale selle, tasased priorid, mis ütlevad, et kõik parameetri väärtused on võrdselt tõenäolised, kajastavad harva meie tegelikke taustateadmisi. Halvad WARNING-ud: divergent transitions (too many), BMFI too low — võivad tähendada, et ahelad ei tööta korralikult. WARNING-ute kohta saad abi siit http://mc-stan.org/misc/warnings.html. Ilusamad parameetriplotid saab kasutades “bayesplot” raamatukogu funktsioone. Esiteks usalduspiirid: fit2d &lt;- as.data.frame(m2@stanfit) pars &lt;- names(fit2d) # inner interval = 50% CI and outer interval = 95% CI. mcmc_intervals(fit2d, pars = pars[1:3], prob = 0.5, prob_outer = 0.90) Joonis 13.5: Posteeriorite CI plot. Ja teiseks täis posteeriorid. mcmc_areas(fit2d, pars = pars[1:2], prob = 0.8) Joonis 13.6: Posteeriorite tihedusplot. Funktsiooniga rethinking::extract.samples() saame koos sämplitud parameetrite numbrid kõrvuti (rea kaupa) tabelisse. m2sampl &lt;- extract.samples(m2) %&gt;% as.data.frame() %&gt;% mutate(CV = sigma / mu) Sellest tabelist võib arvutada posteerioreid ka uutele “väljamõeldud” parameetritele. Näiteks arvutame posteeriori CV-le: ggplot(m2sampl, aes(CV)) + geom_density(breaks = seq(0, 1, by = 0.1)) + xlim(0, 10) #&gt; Warning: Ignoring unknown parameters: breaks #&gt; Warning: Removed 609 rows containing non-finite values (stat_density). Joonis 4.6: Posteerior uuele parameetrile Kuna posteerior iseloomustab meie teadmiste piire, siis võime selle abil küsida, kui suure tõenäosusega jääb tõeline CV näiteks parameetrivahemikku 2 kuni 5? intv &lt;- filter(m2sampl, between(CV, 2, 5)) %&gt;% nrow(.) / nrow(m2sampl) intv #&gt; [1] 0.415 Vastus on, et me arvame 42 kindlusega, et tõde jääb kuskile sellesse vahemikku. Võime ka küsida, millesesse vahemikku jääb näiteks 67% meie usust mu tõelise väärtuse kohta? HPDI(m2sampl$CV, prob = 0.67) #&gt; |0.67 0.67| #&gt; 0.964 4.058 Nüüd võrdleme nelja fititud mudelit, et otsustada, milline mudel kirjeldab kõige paremini outlieritega andmeid. m0 on ilma outlierita mudel ja me tahame teada, milline mudel m1, m2 või m3 annab sellele kõige lähedasemad tulemused. coeftab_plot(coeftab(m0, m1, m2, m3), pars = c(&quot;mu&quot;, &quot;sigma&quot;), prob = 0.5) Joonis 13.7: Võrdlev plot mitme mudeli posteerioritele. Me sättisime usalduspiirid 0.5 peale, mis tähendab, et need ennustavad, kuhu peaks mudeli järgi jääma parameetri tegelik väärtus 50%-se tõenäosusega. Nagu näha, on m2 ja m3 posteeriorid palju lähemal m0-le kui normaaljaotusega fititud m1 oma. Eriti drastilised on erinevused sigma hinnangule. Lisaks, m1 mudeli mu usaldusintervall on palju laiem kui m0, m2 ja m3 oma — mudel nagu saaks aru, et andmed lõhnavad kala järgi. Näide: USA presidentide keskmine pikkus Läheme tagasi normaaljatuse ja USA presidentide juurde. Kõigepealt defineerime priorid. Alati on mõistlik priorid välja joonistada ja vaadata, kas nad vastavad meie ootustele. Pea meeles, et sigma ehk sd on samades ühikutes, mis mõõtmisandmed. Kui sulle need priorid ei meeldi, tuuni priorite parameetreid ja proovi uuesti plottida. x &lt;- -500:500 y &lt;- dnorm(x, 0, 200) plot(x, y, main = &quot;Prior for mu&quot;, type = &quot;l&quot;) Joonis 13.8: Prior keskmisele. Siin kasutame nõrgalt informatiivseid prioreid. Idee on selles, et normaaljaotus, mis on tsentreeritud 0 ümber, tõmbab meie posteeriorit nõrgalt nulli poole (nõrgalt, sest jaotus on hästi lai võrreldes tõepärafunktsiooniga). Pane tähele, et oma priori kohaselt usume me, et 50% tõenäususega on USA presidentide keskmine pikkus negatiivne. See prior on tehniline abivahend, mitte meie tegelike uskumuste peegeldus presidentide kohta. Aga tehniliselt kõik töötab selles mõttes, et andmed domineerivad posteeriori üle ja priori sisuliselt ainus ülesanne on veidi MCMC mootori tööd lihtsustada. Sigma priorina kasutame half-Cauchy jaotust, mis on samuti väheinformatiivne. Half-Cauchy ei saa olla &lt; 0 ja on meile soodsa kujuga sest annab suurema tõenäosuse nullile lähemal asuvatele sd-väärtustele — aga samas, kuna ta on paksu sabaga, ei välista see ka päris suuri sd väärtusi. x &lt;- 0:200 y &lt;- dcauchy(x , 0, 10) plot(x, y, main = &quot;Prior for sigma&quot;, type = &quot;l&quot;) Joonis 13.9: Prior SD-le Tekitame andmeraami analüüsiks ja mudeli, mis põhineb normaalsel tõepärafunktsioonil. heights &lt;- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185) us_presidents &lt;- data_frame(Height = heights, id = &quot;usa&quot;) potusm1 &lt;- map2stan( alist( Height ~ dnorm(mu, sigma), # normal likelihood mu ~ dnorm(0, 200), # normal prior for mean sigma ~ dcauchy(0, 10) # half-cauchy prior from sd ), data = us_presidents ) Mudeli koefitsiendid: precis(potusm1) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; mu 184.5 1.5 181.90 186.71 482 1 #&gt; sigma 4.7 1.3 2.87 6.36 471 1 Nüüd teeme katse võrrelda USA presidentide ja Euroopa ning mujalt pärit riigijuhtide keskmisi pikkusi. Kõigepealt loome analüüsitava andmeraami. world_leaders &lt;- read_csv2(&quot;data/world_leaders.csv&quot;) presidents &lt;- world_leaders %&gt;% select(Country, Height) %&gt;% bind_rows(us_presidents) knitr::kable(head(presidents)) Country Height Canada 188 Cuba 190 France 170 France 165 France 189 France 172 Ja siin on mudel. Nüüd on mu ümber defineeritud kui mu1[indeks], mis tähendab, et mu1 saab kaks hulka väärtusi, üks kummagil indeks muutuja tasemel. Sellega jagame oma andmed kahte ossa (USA versus Euroopa ja muu maailm), mida analüüsime eraldi. Sigma on mõlemale kontinendile sama, mis tähendab, et mudel eeldab, et presidentide pikkuste jaotus on mõlemal kontinendil identne. # Split into 2 groups presidents &lt;- presidents %&gt;% mutate(Groups = case_when( Country == &quot;USA&quot; ~ &quot;USA&quot;, Country != &quot;USA&quot; ~ &quot;World&quot; )) Adult human height varies country-by-country, we take 170 cm as relatively safe prior for male height. potusm2 &lt;- map2stan( alist( Height ~ dnorm(mu, sigma), mu &lt;- mu_1[Groups], # mu is redifined as mu_1, which takes values at each indeks level mu_1[Groups] ~ dnorm(170, 10), # normal prior for mean sigma ~ dcauchy(0, 10) # half-cauchy prior from sd ), data = presidents) precis(potusm2, depth = 2) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; mu_1[1] 182.8 3.57 177.11 188.2 885 1 #&gt; mu_1[2] 176.3 1.85 173.33 179.2 836 1 #&gt; sigma 11.7 1.26 9.86 13.8 628 1 Me võime ka vaadata 2 grupi standardhälbeid lahus. Järgnevas mudelis on mõistlik ahelale stardipositsioon ette anda. ## Calculate start values startvalues &lt;- presidents %&gt;% group_by(Groups) %&gt;% summarise_at(vars(Height), funs(mean, sd)) ## Fit model potusm2.1 &lt;- map2stan( alist( Height ~ dnorm(mu, sigma), mu &lt;- mu_1[Groups], sigma &lt;- sigma_1[Groups], mu_1[Groups] ~ dnorm(170, 10), # normal prior for mean sigma_1[Groups] ~ dcauchy(0, 10) # half-cauchy prior from sd ), data = presidents, start = list(mu_1 = startvalues$mean, sigma_1 = startvalues$sd) ) tracerplot(potusm2.1, n_cols = 2) Joonis 13.10: Traceplot. Tulemus ES-i osas tuleb üsna sarnane. plot(coeftab(potusm2, potusm2.1)) Joonis 13.11: mudelite võrdlusplot. precis(potusm2, depth = 2) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; mu_1[1] 182.8 3.57 177.11 188.2 885 1 #&gt; mu_1[2] 176.3 1.85 173.33 179.2 836 1 #&gt; sigma 11.7 1.26 9.86 13.8 628 1 Siin tuleb kasulik trikk: me lahutame rea kaupa mu1[1] posteeriori sampli liikmed mu1[2] sampli liikmetest. Nii saame posteeriori efekti suurusele ehk hinnangu sellele, mitme cm võrra on USA presidendid keskmiselt pikemad kui Euroopa omad! samplespm2 &lt;- extract.samples(potusm2) %&gt;% as.data.frame() %&gt;% mutate(ES = mu_1.1 - mu_1.2) dens(samplespm2$ES) Joonis 13.12: Posteerior ES-le. median(samplespm2$ES) #&gt; [1] 6.56 rethinking::HPDI(samplespm2$ES, prob = 0.9) #&gt; |0.9 0.9| #&gt; -1.09 12.02 mean(samplespm2$ES &lt; 0) #&gt; [1] 0.065 Võrdse SD-ga mudeli järgi on USA presidendid keskeltläbi 6.6 cm pikemad, ebakindlus selle hinnangu ümber on suur – 90% HDI on -1.1 kuni 12 ja tõenäosus et pikkuste erinevus on väiksem kui 0 on 0.06. samplesm2.1 &lt;- extract.samples(potusm2.1) %&gt;% as.data.frame() %&gt;% mutate(ES = mu_1.1 - mu_1.2) median(samplesm2.1$ES) #&gt; [1] 7.54 HPDI(samplesm2.1$ES, prob = 0.9) #&gt; |0.9 0.9| #&gt; 3.39 12.16 mean(samplesm2.1$ES &lt; 0) #&gt; [1] 0.001 Erineva SD-ga mudeli järgi on riigijuhtide pikkuste vahe 7.5 cm, ebakindlus väiksem – 90% HDI on 3.4 kuni 12.2 ja tõenäosus et pikkuste erinevus on väiksem kui 0 on 0. See ei tähenda tingimata, et me peaksime eelistama teist mudelit. Oluline on, mida me teoreetiliselt usume, kas seda, et tegelik presidentide varieeruvus on USAs ja Euroopas võrdne, või mitte. Lineaarne regressioon Eelmises peatükis hindasime ühe andmekogu (näiteks mõõdetud pikkuste) põhjal ehitatud mudelite parameetreid (näiteks keskmist ja statndardhälvet). Nüüd astume sammu edasi ja hindame kahe muutuja (näiteks pikkuse ja kaalu) koos-varieeruvust. Selleks ehitame mudeli, mis sisaldab mõlemaid muutujaid ja küsime: kui palju sõltub y varieeruvus x varieeruvusest. Lihtsaim viis sellele küsimusele läheneda on lineaarse regressiooni kaudu. Me ehitame lineaarse mudeli, mis vaatab kaalu-pikkuse paare (igal subjektil mõõdeti kaal ja pikkus ning mudel vaatab kaalu ja pikkuse koos-varieeruvust subjektide vahel). Enam ei tohiks tulla üllatusena, et meie arvutused ei anna numbrilist hinnangut mitte teaduslikule küsimusele selle kohta kuidas y-i väärtused sõltuvad x-i väärtustest, vaid mudeli parameetritele. Meie mudel on sirge võrrand \\(y = a + b*x\\) ja tavapärases R-i notatsioonis kirjutatakse see y~x. Kuna pikkused ja kaalud on igavad, proovime vaadata kuidas riigi keskmine eluiga on seotud riigi rikkusega. lm() - vähimruutude meetodiga fititud lineaarsed mudelid Kautame gapminder andmeid aastast 2007. library(gapminder) # Select only data from year 2007 g2007 &lt;- gapminder %&gt;% filter(year == 2007) knitr::kable(head(g2007)) country continent year lifeExp pop gdpPercap Afghanistan Asia 2007 43.8 31889923 975 Albania Europe 2007 76.4 3600523 5937 Algeria Africa 2007 72.3 33333216 6223 Angola Africa 2007 42.7 12420476 4797 Argentina Americas 2007 75.3 40301927 12779 Australia Oceania 2007 81.2 20434176 34435 Enne kui SKP ja eluea seoseid otsima hakkame, vaatame, mis juhtub, kui me arvutame ainult interceptiga mudeli, kus puudub SKP (kasutades lihtsuse mõttes mudeli fittimiseks nn vähimruutude meetodit lm() funktsiooni abil). gapmod1 &lt;- lm(lifeExp ~ 1, data = g2007) summary(gapmod1) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ 1, data = g2007) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -27.39 -9.85 4.93 9.41 15.60 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 67.01 1.01 66.1 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 12.1 on 141 degrees of freedom Ok, intercept = 67. Mida see tähendab? mean(g2007$lifeExp) #&gt; [1] 67 See on lihtsalt parameetri, mida me ennustame, keskmine väärtus ehk keskmine eluiga üle kõikide riikide. Nüüd fitime mudeli, kus on olemas SKP ja eluea seos aga puudub lõikepunkt. gapmod2 &lt;- lm(lifeExp ~ 0 + gdpPercap, data = g2007) summary(gapmod2) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ 0 + gdpPercap, data = g2007) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -65.5 17.6 44.9 54.7 67.0 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; gdpPercap 0.002951 0.000218 13.5 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 45.1 on 141 degrees of freedom #&gt; Multiple R-squared: 0.565, Adjusted R-squared: 0.562 #&gt; F-statistic: 183 on 1 and 141 DF, p-value: &lt;2e-16 p &lt;- ggplot(g2007, aes(gdpPercap, lifeExp)) + geom_point() + geom_line(aes(y = .fitted), data = gapmod2) p Joonis 13.13: Nulli surutud interceptiga lineaarne regressioon eluea sõltuvusele SKP-st. Nüüd on intercept surutud väärtusele y = 0. Ja lõpuks täismudel gapmod3 &lt;- lm(lifeExp ~ gdpPercap, data = g2007) summary(gapmod3) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ gdpPercap, data = g2007) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -22.83 -6.32 1.92 6.90 13.13 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 5.96e+01 1.01e+00 59.0 &lt;2e-16 *** #&gt; gdpPercap 6.37e-04 5.83e-05 10.9 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.9 on 140 degrees of freedom #&gt; Multiple R-squared: 0.461, Adjusted R-squared: 0.457 #&gt; F-statistic: 120 on 1 and 140 DF, p-value: &lt;2e-16 p + geom_line(aes(y = .fitted), data = gapmod3, color = &quot;blue&quot;) Joonis 13.14: Täismudeliga regressioon. Kuidas me seda m3 mudelit tõlgendame? Esiteks, Intercept on 59.6, mis tähendab, et mudel ennustab, et kui riigi SKP = 0 USD, siis selle riigi elanike keskmine euliga on ligi 60 aastat. See on selgelt imelik, sest ühegi riigi SKP ei ole null, ja kui oleks, oleks seal ka eluiga 0 (selle järgi peaksime eelistama mudelit gapmod2, kus me oleme intercepti nulli surunud). Teiseks, koefitsient b = 0.00064, mis on üsna väike arv. See tähendab, et SKP tõus 1 USD võrra tõstab eluiga keskmiselt 0.00064 aasta võrra (ja SKP tõus 1000 USD võrra tõstab eluiga 0.64 aasta võrra). Muidugi ainult siis, kui uskuda mudelit. Kolmandaks, adjusted R squared on 0.46, mis tähendab et mudeli järgi seletab SKP varieerumine 46% eluea varieeruvusest riikide vahel. Hea küll, aga milline mudel on siis parim? knitr::kable(AIC(gapmod1, gapmod2, gapmod3)) df AIC gapmod1 2 1113 gapmod2 2 1487 gapmod3 3 1028 AIC on Aikake informatsiooni kriteerium, mis võtab arvesse nii mudeli fiti headuse kui mudeli parameetrite arvu. Kuna R saab parameetreid lisades ainult kasvada ja me teame, et mingist hetkest oleme niikuinii oma mudeli üle fittinud, siis otsime AIC-i abil kompromissi: võimalult hea fit võimalikult väikese parameetrite arvuga. AIC on suhteline mõõt, selle absoluutnäit ei oma mingit tähendust. Me eelistame väiksema AIC-ga mudelit nende mudelite seast, mida me võrdleme. See ei tähenda, et võitnud mudel oleks hea mudel — alati on võimalik, et kõik head mudelid jäid võrdlusest välja. Seega parim mudel on gapmod3 ja kõige kehvem on gapmod2, mille lõikepunkt on realistlikult nulli fikseeritud! "],
["bayesi-meetodil-lineaarse-mudeli-fittimine.html", "14 Bayesi meetodil lineaarse mudeli fittimine Ennustused mudelist Lognormaalne tõepäramudel", " 14 Bayesi meetodil lineaarse mudeli fittimine library(tidyverse) library(gapminder) library(rethinking) Nüüd Bayesi mudelid. “rethinking” paketi glimmer() on abivahend, mis konverteerib lm() mudeli kirjelduse Bayesi mudeli kirjelduseks kasutades normaaljaotusega tõepära mudelit. Intercept only model g2007 &lt;- gapminder %&gt;% filter(year == 2007) intercept_only &lt;- glimmer(lifeExp ~ 1, data = g2007) #&gt; alist( #&gt; lifeExp ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept, #&gt; Intercept ~ dnorm(0,10), #&gt; sigma ~ dcauchy(0,2) #&gt; ) Ainult interceptiga mudel. Keskväärtus ehk mu on ümber defineeritud kui intercept, aga see annab talle lihtsalt uue nime. Sama hästi oleksime võinud fittida mudelit, kus hindame otse mu keskväärtust (nagu me eelmises peatükis tegime). Pane tähele, et võrreldes lm() funktsiooniga on meil mudelis lisaparameeter — sigma. Kui Intercept annab meile keskmise eluea, siis sigma annab eluigade standardhälbe riikide vahel. Kui me tahame fittida lineaarset mudelit, siis peab tõepära funktsioon olema kas normaaljaotus või studenti t jaotus. gapmod4 &lt;- map2stan(flist = intercept_only$f, data = intercept_only$d) precis(gapmod4) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; Intercept 66.3 0.99 64.6 67.8 694 1 #&gt; sigma 12.1 0.71 11.0 13.2 554 1 Nüüd ilma interceptita mudel no_intercept &lt;- glimmer(lifeExp ~ -1 + gdpPercap, data = g2007) #&gt; alist( #&gt; lifeExp ~ dnorm( mu , sigma ), #&gt; mu &lt;- b_gdpPercap*gdpPercap, #&gt; b_gdpPercap ~ dnorm(0,10), #&gt; sigma ~ dcauchy(0,2) #&gt; ) Selline Bayesi mudeli esitus on “ilusam” kui lm() sest ta toob mudeli eksplitsiitselt välja (samas kui lm notatsioon ütleb, et mudel on “miinus intercept”) gapmod5 &lt;- map2stan(flist = no_intercept$f, data = no_intercept$d) precis(gapmod5) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; b_gdpPercap 0.0 0.00 0.0 0.0 888 1 #&gt; sigma 45.2 2.75 40.8 49.4 187 1 Ja lõpuks täismudel: full_model &lt;- glimmer(lifeExp ~ gdpPercap, data = g2007) #&gt; alist( #&gt; lifeExp ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept + #&gt; b_gdpPercap*gdpPercap, #&gt; Intercept ~ dnorm(0,10), #&gt; b_gdpPercap ~ dnorm(0,10), #&gt; sigma ~ dcauchy(0,2) #&gt; ) gapmod6 &lt;- map2stan(flist = full_model$f, data = full_model$d) compare(gapmod4, gapmod5, gapmod6) #&gt; WAIC pWAIC dWAIC weight SE dSE #&gt; gapmod6 1028 2.6 0.0 1 14.07 NA #&gt; gapmod4 1113 1.5 85.7 0 12.09 9.67 #&gt; gapmod5 1486 0.8 458.8 0 7.29 15.70 Jälle on täismudel võitja ja kui intercept nulli suruda, saame kehveima tulemuse. Siin me kasutame AIC-i Bayesi analoogi WAIC, mis nende mudelite peal peaks töötama veidi paremini kui AIC. Aga see on tehniline detail. WAIC abil mudeleid võrreldes saame muuhulgas mudeli kaalu. Antud juhul on 100% kaalust gapmo6-l ja ülejäänud mudelitele ei jää midagi. plot(coeftab(gapmod4, gapmod5, gapmod6)) Joonis 14.1: Mudelite võrdlusplot. Viime SKP andmed log-skaalasse ja proovime uuesti. See tähendab, et me arvame, et iga SKP kümnekordne tõus võiks kaasa tuua eluea tõusu x aasta võrra. g2007 &lt;- g2007 %&gt;% mutate(l_GDP = log10(gdpPercap)) # glimmer(lifeExp ~ -1 + l_GDP, data = g2007) gapmod7 &lt;- map2stan(alist( lifeExp ~ dnorm(mu, sigma), mu &lt;- b_gdp * l_GDP, b_gdp ~ dnorm(0, 10), sigma ~ dcauchy(0, 2) ), data = g2007) gapmod8 &lt;- map2stan(alist( lifeExp ~ dnorm(mu, sigma), mu &lt;- Intercept + b_gdp * l_GDP, Intercept ~ dnorm(0, 100), b_gdp ~ dnorm(0, 10), sigma ~ dcauchy(0, 2) ), data = g2007) compare(gapmod4, gapmod5, gapmod6, gapmod7, gapmod8) #&gt; WAIC pWAIC dWAIC weight SE dSE #&gt; gapmod7 965 3.0 0.0 0.53 25.11 NA #&gt; gapmod8 966 3.8 0.2 0.47 25.37 2.56 #&gt; gapmod6 1028 2.6 62.4 0.00 14.07 18.21 #&gt; gapmod4 1113 1.5 148.1 0.00 12.09 23.18 #&gt; gapmod5 1486 0.8 521.1 0.00 7.29 26.82 Joonis 13.9: Log skaalas töötab nulli surutud interceptiga mudel sama hästi kui täismudel. See ei ole paraku mudeldamise üldine omadus. Kuna Bayesi mudelite fittimine on keerulisem kui lm() abil, on eriti tähtis fititud mudel välja plottida. See on esimene kaitseliin lollide vigade ja halvasti jooksvate Markovi ahelate vastu. Kui Bayesi mudeleid on raskem fittida, siis milleks me peaksime neid eelistama tavalistele vähimruutude meetodil fititud mudelitele? Tegelikult alati ei peagi. Aga siiski, Bayesi mudelid sisaldavad eksplitsiitset veakomonenti (sigma), mis on kasulik mudelist uusi andmeid ennustades. Samuti annavad nad parima hinnangu ebakindlusele parmeetrite väärtuste hinnangute ümber, võimaldavad mudeli fittimisel siduda andmeid taustainfoga (prior) ning, mis kõige tähtsam, võimaldavad paindlikumalt fittida hierarhilisi mudeleid (nende juurde tuleme hiljem). Samas, kui prior on väheinformatiivne, siis Bayesi hinnangud mudeli koefitsientide kõige tõenäolisematele väärtustele on praktiliselt samad, kui vähimruutude meetodiga lm() abil saadud punkt-hinnangud. Siin me fitime pedagooglistel kaalutlustel kõike Bayesiga aga praktikas jätavad paljud mõistlikud inimesed Bayesi hierarhiliste mudelite jaoks ja kasutavad lihtsate mudelite jaoks lm(). Tagasi gapmod7 ja gapmod8 mudelite juurde. Plotime nende koefitsiendid koos usalduspiiridega. plot(coeftab(gapmod7, gapmod8)) Joonis 14.2: Mudelite võrdlusplot. Pane tähele, et gapmod8 “b_gdp” koefitsiendi posteerior on palju laiem kui gapmod7 “b_gdp” oma. See on üldine nähtus, mis tuleneb sellest, et gapmod7-s on vähem parameetreid. Iga lisatud parameeter kipub vähendama teiste parameetrite hindamise täpsust. Ennustused mudelist Kuidas plottida meie hinnangud ebakindlusele parameetri tegeliku väärtuse ümber? Siin tuleb appi rethinking::link(). Nii tõmbame posteriorist igale meie andmetes esinevale log GDP väärtusele vastavad 1000 ennustust keskmise eluea kohta sellel l_GDP väärtusel: linked &lt;- link(gapmod8) linked &lt;- as_tibble(linked) linked_mean &lt;- apply(linked, 2, HPDI, prob = 0.95) Sel viisil saab tabeli, kus igale 142-le andmepunktist vastab üks veerg, milles on 1000 posteeriorist arvutatud ennustust lifeExp väärtusele. Praktikas soovime aga enamasti meie poolt ette antud l_GDP väärtustel põhinevaid ennustusi keskmise eluea kohta. See käib nii: # link() draws from the posterior 1000 mu values for each l_GDP value in the width object; out pops a table with 1000 rows and 41 columns. mu1 &lt;- as_tibble(link(gapmod8, data = list(l_GDP = seq(2, 6, 0.1)))) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] Nüüd on meil mu1 objektis 41 l_GDP väärtust, millest igale vastab 1000 ennustust keskmise eluea kohta sellel l_GDP-l. Järgmiseks arvutame igale neist 41-st tulbast keskmise ja 95% HPDI ning plotime need koos andmepunktidega kasutades base-R graafikasüsteemi. Pane tähele, et hall riba näitab ebakindlust ennustuse ümber keskmisele elueale üle kõikide riikide, mis võiksid sellist l_GDP-d omada (ehk ebakindlust regressioonijoonele). Kui me aga tahame ennustada ka keskmiste eluigade varieeruvust riigi tasemel (kasutades Bayesi hinnangut sigma parameetrile), siis on meil vaja sim() funktsiooni: mu.mean &lt;- apply(mu1, 2, mean) # applies the FUN mean() to each column mu.HPDI &lt;- apply(mu1, 2, HPDI, prob = 0.95) %&gt;% t() %&gt;% as_data_frame() mu.HPDI &lt;- bind_cols(width = seq(2, 6, 0.1), mu.HPDI) colnames(mu.HPDI) &lt;- c(&quot;width&quot;, &quot;lower&quot;, &quot;upper&quot;) sim.length &lt;- as_tibble(sim(gapmod8, data = list(l_GDP = seq(2, 6, 0.1)))) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] height.PI &lt;- sapply(sim.length, PI, prob = 0.95, simplify = FALSE) %&gt;% do.call(rbind,. ) height.PI &lt;- cbind(width = seq(2, 6, 0.1), height.PI) %&gt;% as_tibble() colnames(height.PI) &lt;- c(&quot;width&quot;, &quot;lower&quot;, &quot;upper&quot;) library(viridis) ggplot(g2007) + geom_point(aes(l_GDP, lifeExp, color = continent)) + geom_line(data = data_frame(width = seq(2, 6, 0.1), mu.mean), aes(width, mu.mean)) + geom_ribbon(data = mu.HPDI, aes(x = width, ymin = lower, ymax = upper), fill = &quot;grey5&quot;, alpha = 0.1) + geom_ribbon(data = height.PI, aes(x = width, ymin = lower, ymax = upper), fill = &quot;grey50&quot;, alpha = 0.1) + labs(caption = &quot;Dark grey, 95% HDPI - highest posterior density.\\nLight grey, 95% PI - percentile interval.&quot;) + theme(legend.title = element_blank()) + scale_color_viridis(discrete = TRUE) Joonis 14.3: Ennustused mudelist. Nüüd ütleb laiem hall ala, et me oleme üsna kindlad, et nende riikide puhul, mille puhul mudel töötab, kohtame individaalsete riikide keskmiseid eluigasid halli ala sees ja mitte sealt väljas. Nagu näha, on meil ka riike, mis jäävad hallist alast kaugele ja mille keskmine eluiga on kõvasti madalam, kui mudel ennustab. Need on äkki riigid, kus parasjagu on sõda üle käinud ja mille eluiga ei ole näiteks seetõttu SKP-ga lihtsas põhjuslikus seoses. Igal juhul tasuks need ükshaaval üle vaadata sest punktid, mida mudel ei seleta, võivad varjata endas mõnd huvitavat saladust, mis pikisilmi ootab avastajat. Lisaks: pane tähele, et mudel eeldab, et riikide keskmise eluea SD on muutumatu igal GDP väärtusel. Kuidas saada ennustusi kindlale l_GDP väärtusele? Näiteks tulp V10 vastab l_GDP väärtusele 2.9. Järgnevalt arvutame oodatavad keskmised eluead sellele SKP väärtusele (fiksionaalsetele riikidele, millel võiks olla täpselt selline SKP): dens(sim.length$V10) HPDI(sim.length$V10, prob = 0.95) #&gt; |0.95 0.95| #&gt; 40.7 68.5 Joonis 14.4: Ennustus mudelist kindlale log GDP väärtusele. Nagu näha, võib mudeli kohaselt sellise riigi keskmine eluiga tulla nii madal, kui 40 aastat ja nii kõrge kui 67 aastat. Lognormaalne tõepäramudel See mudel on alternatiiv andmete logaritmimisele, kui Y-muutuja (see muutuja, mille väärtust te ennustate) on lognormaalse jaotusega. Lognormaalne Y-i tõepäramudel on mittelineaarne. Lognormaaljaotus defineetitakse üle mu ja sigma, mis aga vastavd hoopis log(Y) normaaljaotuse mu-le ja sigmale. Lognormaalse mudeli koefitsiendid mu ja sigma annavad keskmise ja standardhälbe log(y)-le, mitte y-le! Seega tuleb need koefitsiendid lineaarse mudeli andmeskaalas tõlgendamiseks ümber arvutada. \\[e^\\mu = \\mu^*\\] kus \\(\\mu^*\\) on geomeetriline keskmine ehk mediaan \\[e^\\sigma = \\sigma^*\\] kus \\(\\sigma^*\\) on multiplikatiivne standardhälve \\(\\mu^*\\) korda/jagatud \\(\\sigma^*\\) katab 68% lognormaaljaotusest, ja 2 \\(\\sigma^*\\) katab 96% jaotusest. Lognormaaljaotuse parameetrite põhjal on võimalik arvutada ka tavalised additiivsed keskväärtused ja standardhälbed, aga oluliselt keerulisemate valemitega: \\[e^\\mu e^{\\frac{\\sigma^2}2} = \\mu\\] kus saadud \\(\\mu\\) on tavaline additiivne keskväärtus (aritmeetiline keskmine) \\[e^{\\sigma^2}(e^{\\sigma^2} - 1)e^{2\\sigma} =\\sigma\\] ja saadud \\(\\sigma\\) on tavaline additiivne standardhälve Seekord ennustame GDP-d keskmise eluea põhjal (mis, nagu näha jooniselt, ei ole küll päris lognormaalne). Joonis 14.5: SKP-de jaotus Mustaga on näidatud empiiriline SKP jaotus, rohelisega fititud lognormaalne mudel sellest samast jaotusest. Järgnevalt ennustame SKP-d keskmise eluea põhjal, milleks fitime lognormaalse tõepäramudeli, kus mu on ümber defineeritud regressioonivõrrandiga: m_ln1 &lt;- map2stan( alist( gdpPercap ~ dlnorm( mu , sigma ), mu &lt;- a + b * lifeExp, a ~ dnorm( 0, 10 ), b ~ dnorm( 0, 10 ), sigma ~ dcauchy( 0, 2 ) ), data = g2007, start = list( a = 3, b = 0, sigma = 0.5 ) ) precis(m_ln1) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; a 2.48 0.38 1.87 3.08 283 1 #&gt; b 0.09 0.01 0.08 0.10 287 1 #&gt; sigma 0.81 0.05 0.74 0.88 294 1 Kuna meil on tegemist mitte-lineaarse mudeliga, sõltub tõusu (\\(\\beta\\)) väärtus ka mudeli interceptist: \\(\\beta = exp(\\alpha + \\beta)-exp(\\alpha)\\). See ei ole lineaarne seos: \\(\\beta\\) omab seda suuremat mõju efektile (tõusule), mida suurem on \\(\\alpha\\). Kui meil on tegu binaarse prediktoriga, siis kodeerime selle 2 taset kui -1 ja 1. Sellises mudelis on tõus sama, mis efekti suurus ES, ja \\(ES = exp(\\alpha + \\beta)-exp(\\alpha-\\beta)\\) a &lt;- seq( 0, 10, length.out = 1000 ) b &lt;- 2 b1 &lt;- 3 y &lt;- exp( a + b ) - exp( a ) y1 &lt;- exp( a + b1 ) - exp( a ) plot( a, y, type = &quot;l&quot;, xlab = &quot;a value&quot;, ylab = &quot;slope&quot; ) lines( a, y1, col = &quot;red&quot; ) Joonis 13.10: Mudeli tõus sõltub interceptist. Must joon näitab mudeli tõusu sõltuvust parameetri a väärtusest, kui parameeter b = 2. Punane joon teeb sedasama, kui b = 3. Selline on siis mudeli tõusude (beta) posteerior: s_ln1 &lt;- extract.samples( m_ln1 ) %&gt;% as.data.frame() beta &lt;- exp(s_ln1$a + s_ln1$b) - exp(s_ln1$a) dens(beta) Joonis 14.6: Mudeli tõusude (beta) posteerior. Lognormaaljaotusega mudelis täidab normaaljaotusega mudeli intercepti rolli eelkõige mediaan, mis on defineeritud kui exp(a), aga arvutada saab ka keskmise: i_median &lt;- exp(s_ln1$a) mean(i_median) #&gt; [1] 12.8 i_mean &lt;- exp(s_ln1$a + (s_ln1$sigma ^ 2) / 2) mean(i_mean) #&gt; [1] 17.8 Siin ennustame fititud mudelist uusi andmeid (väljamõeldud riikide rikkust): sim_ci &lt;- sim(m_ln1) %&gt;% as_tibble() %&gt;% apply(2, HPDI, prob = 0.95) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] ggplot(g2007, aes(lifeExp, gdpPercap)) + geom_point(aes(color = continent), size = 0.8) + geom_ribbon(aes( ymin = sim_ci[1,], ymax = sim_ci[2,]), alpha = 0.2) + scale_color_viridis(discrete = TRUE) Joonis 14.7: Ennustus mudelist. Ka see mudel jääb hätta Aafrika outlieritega, mille eluiga ei suuda ennustada rikkust. "],
["mitme-prediktoriga-lineaarne-regressioon.html", "15 Mitme prediktoriga lineaarne regressioon Miks multivariaatsed mudelid head on? Mudeldamine standardiseeritud andmetega", " 15 Mitme prediktoriga lineaarne regressioon library(tidyverse) library(gapminder) library(rethinking) Vaatame jälle gapminderi andmeid aastast 2007. Meil on võimalik lisada regressioonivõrrandisse lisaprediktoreid. Nüüd ei küsi me enam, kuidas mõjutab l_GDP varieeruvus keskmise eluea varieeruvust vaid: kuidas mõjutavad muutujad l_GDP, continent ja logaritm pop-ist (rahvaarvust) keskmist eluiga. Me modelleerime selle lineaarselt nii, et eeldusena varieeruvad need x-i muutujad üksteisest sõltumatult: \\(y = a + b_1x_1 + b_2x_2 + b_3x_3\\) g2007 &lt;- gapminder %&gt;% filter(year == 2007) g2007 &lt;- g2007 %&gt;% mutate(l_GDP = log10(gdpPercap), l_pop = log10(pop), lpop_s = (l_pop - mean(l_pop )) / sd(l_pop), lGDP_s = (l_GDP - mean(l_GDP )) / sd(l_GDP)) %&gt;% as.data.frame() Sellise mudeli tõlgendus on suhteliselt lihtne: koef b1 ütleb meile, kui mitme ühiku võrra tõuseb/langeb muutuja y (eluiga) kui muutuja x1 (l_GDP) tõuseb 1 ühiku võrra; tingimusel, et me hoiame kõigi teiste muutujate väärtused konstantsed. Sarnane definitsioon kehtib ka kõigi teiste prediktorite (x-de) kohta. Kui meil on mudelis SKP ja pop (rahvaarv), siis saame küsida kui me juba teame SKP-d, millist ennustuslikku lisaväärtust annab meile ka populatsiooni suuruse teadmine? ja kui me juba teame populatsiooni suurust, millist lisaväärtust annab meile ka SKP teadmine? Järgenval mudelil on 4 parameetrit (intercept + 3 betat). m1 &lt;- lm(lifeExp ~ l_GDP + continent + l_pop, data = g2007) summary(m1) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ l_GDP + continent + l_pop, data = g2007) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -19.425 -2.246 -0.014 2.468 14.957 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.4182 7.4557 2.60 0.0102 * #&gt; l_GDP 10.6876 1.2378 8.63 1.5e-14 *** #&gt; continentAmericas 11.6564 1.6929 6.89 2.0e-10 *** #&gt; continentAsia 10.0521 1.5776 6.37 2.7e-09 *** #&gt; continentEurope 11.2320 1.9265 5.83 3.9e-08 *** #&gt; continentOceania 12.8918 4.5493 2.83 0.0053 ** #&gt; l_pop 0.0928 0.8076 0.11 0.9087 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.95 on 135 degrees of freedom #&gt; Multiple R-squared: 0.767, Adjusted R-squared: 0.757 #&gt; F-statistic: 74.2 on 6 and 135 DF, p-value: &lt;2e-16 loeme mudelis “+” märki nagu “või”. Ehk, “eluiga võib olla funktsioon SKP-st või rahvaarvust”. Intercept 19 ei tähenda tõlgenduslikult midagi. l-GDP tõus ühiku võrra tõstab eluiga 10.7 aasta võrra. Võrdluseks lihtne mudel m2 &lt;- lm(lifeExp ~ l_GDP, data = g2007) summary(m2) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ l_GDP, data = g2007) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -25.95 -2.66 1.22 4.47 13.12 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.95 3.86 1.28 0.2 #&gt; l_GDP 16.59 1.02 16.28 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 7.12 on 140 degrees of freedom #&gt; Multiple R-squared: 0.654, Adjusted R-squared: 0.652 #&gt; F-statistic: 265 on 1 and 140 DF, p-value: &lt;2e-16 Siin on l_GDP mõju suurem, 16.6 aastat. Millisel mudelil on siis õigus? Proovime veel ülejäänud variandid m3 &lt;- lm(lifeExp ~ l_GDP + continent, data = g2007) summary( m3 ) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ l_GDP + continent, data = g2007) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -19.492 -2.315 -0.043 2.550 14.882 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 20.14 4.03 4.99 1.8e-06 *** #&gt; l_GDP 10.66 1.21 8.78 6.1e-15 *** #&gt; continentAmericas 11.69 1.65 7.07 7.5e-11 *** #&gt; continentAsia 10.11 1.48 6.85 2.3e-10 *** #&gt; continentEurope 11.27 1.89 5.95 2.1e-08 *** #&gt; continentOceania 12.93 4.52 2.86 0.0049 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.93 on 136 degrees of freedom #&gt; Multiple R-squared: 0.767, Adjusted R-squared: 0.759 #&gt; F-statistic: 89.7 on 5 and 136 DF, p-value: &lt;2e-16 m4 &lt;- lm(lifeExp ~ l_GDP + l_pop, data = g2007) AIC(m1, m2, m3, m4) #&gt; df AIC #&gt; m1 8 918 #&gt; m2 3 965 #&gt; m3 7 916 #&gt; m4 4 962 Võitja mudel on hoopis m3, mis võtab arvesse kontinendi. Siin on l_GDP mõju samuti 10.7 aastat. Lisaks näeme, et kui riik ei asu Aafrikas, siis on l_GDP mõju elueale u 11 aasta võrra suurem. Seega elu Aafrika kisub alla keskmise eluea riigi rikkusest sõltumata. Võib olla on põhjuseks sõjad, võib-olla AIDS ja malaaria, võib-olla midagi muud. Millise mudeli me peaksime siis avaldama? Vastus on, et need kõik on olulised, et vastata küsimusele, millised faktorid kontrollivad keskmist eluiga? Mudelite võrdlusest näeme, et rahvaarvu mõju elueale on väike või olematu ning et SKP mõju avaldub log skaalas (viitab teatud tüüpi eksponentsiaalsetele protsessidele, kus rikkus tekitab uut rikkust) ning, et Aafrikaga on midagi pahasti ja teistmoodi kui teiste kontinentidega. Aafrikast tasub otsida midagi, mida meie senised mudelid ei kajasta. Miks ei ole mudeli summary tabelis Aafrikat? Põhjus on tehniline. Kategoorilisi muutujaid, nagu kontinent, vaatab mudel paariviisilises võrdluses, mis tähendab et k erineva tasemega muutujast tekitatakse k - 1 uut muutujat, millest igaühel on kaks taset (0 ja 1). See algne muutuja, mis üle jääb (antud juhul Africa), jääb ilma oma uue muutujata. Me saame teisi uusi kontinendi põhjal tehtud muutujaid tõlgendada selle järgi, kui palju nad erinevad Africa-st. Miks multivariaatsed mudelid head on? nad aitavad kontrollida nn kaasnevaid (confounding) muutujaid. Kaasnev muutuja võib olla korreleeritud mõne teise muutujaga, mis meile huvi pakub. See võib nii maskeerida signaali, kui tekitada võlts-signaali, kuni y ja x1 seose suuna muutmiseni välja. ühel tagajärjel võib olla mitu põhjust. Isegi kui muutujad ei ole omavahel üldse korreleeritud, võib ühe tähtsus sõltuda teise väärtusest. Näiteks taimed vajavad nii valgust kui vett. Aga kui ühte ei ole, siis pole ka teisel suurt tähtsust. Mudeldamine standardiseeritud andmetega Kui me lahutame igast andmepunktist selle muutuja keskväärtuse siis saame 0-le tsentreeritud andmed. Kui me sellisel viisil saadud väärtused omakorda läbi jagame muutuja standardhälbega, siis saame standardiseeritud andmed, mille keskväärtus on null ja SD = 1. \\(Standard.andmed = (x - mean(x))/sd(x)\\) Nii on lihtsam erinevas skaalas muutujaid omavahel võrrelda (1 ühikuline muutus võrdub alati muutusega 1 standardhäve võrra) ja mudeli arvutamine üle mcmc ahelate on ka lihtsam. m5 &lt;- map2stan( alist( lifeExp ~ dnorm( mu , sigma ) , mu &lt;- a + b_GDP * lGDP_s + b_pop * lpop_s , a ~ dnorm( 0 , 10 ) , c(b_GDP, b_pop) ~ dnorm( 0 , 1 ) , sigma ~ dunif( 0 , 10 ) ), data = g2007 ) precis(m5) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; a 66.74 0.64 65.77 67.79 1000 1 #&gt; b_GDP 6.95 0.58 6.06 7.84 775 1 #&gt; b_pop 0.80 0.55 -0.11 1.58 870 1 #&gt; sigma 7.64 0.50 6.82 8.40 876 1 kui l_GDP kasvab 1 sd võrra, siis eluiga kasvab 6.9 aasta võrra. f1 &lt;- glimmer(lifeExp ~ lGDP_s + lpop_s + continent, data = g2007) #&gt; alist( #&gt; lifeExp ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept + #&gt; b_lGDP_s*lGDP_s + #&gt; b_lpop_s*lpop_s + #&gt; b_continentAmericas*continentAmericas + #&gt; b_continentAsia*continentAsia + #&gt; b_continentEurope*continentEurope + #&gt; b_continentOceania*continentOceania, #&gt; Intercept ~ dnorm(0,10), #&gt; b_lGDP_s ~ dnorm(0,10), #&gt; b_lpop_s ~ dnorm(0,10), #&gt; b_continentAmericas ~ dnorm(0,10), #&gt; b_continentAsia ~ dnorm(0,10), #&gt; b_continentEurope ~ dnorm(0,10), #&gt; b_continentOceania ~ dnorm(0,10), #&gt; sigma ~ dcauchy(0,2) #&gt; ) See on mudeli struktuur, mis sisaldab uusi kategoorilisi muutujaid Siin on tähtis anda map2stan()-le ette glimmeri poolt eeltöödeldud andmed: m6 &lt;- map2stan( f1$f, data = f1$d) precis(m6) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; Intercept 59.92 0.97 58.31 61.42 344 1 #&gt; b_lGDP_s 6.29 0.68 5.20 7.31 450 1 #&gt; b_lpop_s 0.06 0.50 -0.75 0.79 1000 1 #&gt; b_continentAmericas 11.66 1.53 9.07 13.94 436 1 #&gt; b_continentAsia 10.11 1.47 7.87 12.51 428 1 #&gt; b_continentEurope 11.26 1.83 8.50 14.26 434 1 #&gt; b_continentOceania 11.18 4.00 4.87 17.50 784 1 #&gt; sigma 5.95 0.36 5.38 6.49 931 1 "],
["keerulisemate-mudelitega-tootamine.html", "16 Keerulisemate mudelitega töötamine Predictor residual plots Ennustavad plotid Posterior prediction plots Interaktsioonid prediktorite vahel Interaktsioonid pidevatele tunnustele", " 16 Keerulisemate mudelitega töötamine library(tidyverse) library(gapminder) library(rethinking) library(bayesplot) library(gridExtra) Kasuta graafilisi meetodeid. Mudeli koefitsientide jõllitamine üksi ei päästa. g2007 &lt;- gapminder %&gt;% filter(year == 2007) g2007 &lt;- g2007 %&gt;% mutate(l_GDP = log10(gdpPercap), l_pop = log10(pop), lpop_s = (l_pop - mean(l_pop )) / sd(l_pop), lGDP_s = (l_GDP - mean(l_GDP )) / sd(l_GDP)) %&gt;% as.data.frame() Predictor residual plots m5 &lt;- map2stan( alist( lifeExp ~ dnorm( mu , sigma ) , mu &lt;- a + b_GDP * lGDP_s + b_pop * lpop_s , a ~ dnorm( 0 , 10 ) , c(b_GDP, b_pop) ~ dnorm( 0 , 1 ) , sigma ~ dunif( 0 , 10 ) ), data = g2007 ) Plotime varieeruvuse, mida mudel ei oota ega seleta. names(coef(m5)) #&gt; [1] &quot;a&quot; &quot;b_GDP&quot; &quot;b_pop&quot; &quot;sigma&quot; Kõigepealt lihtne residuaalide plot, kus meil on y-teljel residuaalid ja x-teljel X1 muutuja tegelikud valimiväärtused. Y = 0 tähistab horisontaalse joonena mudeli ennustatud Y (eluea) väärtusi kõigil prediktori X1 (lGDP_s) väärtustel ja residuaal on defineeritud kui tegelik Y miinus mudeli poolt ennustatud eluiga sellel X1 väärtusel. Mudeli ennustuse saamiseks anname mudelile ette fikseeritud parameetrite (koefitsientide) a, b_GDP ja b_pop väärtused ning arvutame oodatava keskmise eluea üle kõigi valimis leiduvate lGDP_s ja lpop_s väärtuste. Seega saame sama palju keskmise eluea ennustusi, kui palju on meie andmetabelis ridu. # Using the fitted model compute the expected value of y (mu) # for each of the 142 data rows. mu &lt;- coef(m5)[&#39;a&#39;] + coef(m5)[&#39;b_GDP&#39;] * g2007$lGDP_s + coef(m5)[&#39;b_pop&#39;] * g2007$lpop_s # compute residuals - a vector w. 142 values m.resid &lt;- g2007$lifeExp - mu ggplot( g2007, aes( lGDP_s, m.resid ) ) + geom_segment( aes( xend = lGDP_s, yend = 0 ), size = 0.2 ) + geom_point( size = 0.5, type = 1 ) #&gt; Warning: Ignoring unknown parameters: type Joonis 4.3: Mudeli residuaalide plot (m.resid ~ X1). Me näeme, et seal kus SKP on väiksem kipuvad residuaalid olema negatiivsed, mis tähendab, et mudel ülehindab keskmist eluiga. Ja vastupidi, seal kus SKP on üle keskmise, mudel kipub alahindma keskmist eluiga. See seos tuleb eriti selgelt välja järgmisel pildil, kus plotime residuaalide sõltuvuse elueast (kui eelmine plot oli m.resid ~ X1, siis nüüd plotime m.resid ~ Y). Lisaks joonistame selguse mõttes regressioonisirge. Kui residuaalid oleks ühtlaselt jaotunud mõlemale poole mudeli ennustust, siis saaksime horisontaalse regressioonisirge. Tegeliku sirge tõus näitab, et suuremad eluead omavad eelistatult poitiivseid residuaale ja väiksemad eluead negatiivseid residuaale. See tähendab, et mudel alahindab eluiga seal, kus SKP on kõrge ja vastupidi, ülehindab eluiga seal, kus SKP on madal. g2007$m.resid &lt;- m.resid ggplot(g2007, aes(lifeExp, m.resid)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_point() + geom_hline(yintercept = 0, color = &quot;grey&quot;, linetype = 2) Joonis 16.1: m.resid ~ Y plot Horisontaalne punktiirjoon näitab, kus mudel vastab täpselt andmetele. Ennustavad plotid Plot, kus me ennustame keskmise eluea sõltuvust SKP-st nii riikide kaupa eraldi (andmepunktide paupa) kui üldiselt kõikide riikide keskmisena, millel on mingi kindel SKP (mudeli parima ennustuse ehk sirge asendi ümber valitsevat ebakindlust). Et seda teha, hoiame rahvaarvu konstantsena oma keskväärtusel, mis standardiseeritud andmetl võrdub alati nulliga. link() funktsioon annab meile keskmiste eluigade ennustused meie poolt ette antud X1 ja X2 väärtustel, ning sim() annab meile eluigade ennustused fiktsionaalsete riikide kaupa samadel X1 ja X2 väärtustel. Nagu näha, on meie mudeli arvates riikide kaupa ennustamine palju laiema varieeruvusega kui üle kõikväimalike riikide kesmise kaupa ennustamine. # prepare new counterfactual data pred.data &lt;- tibble( lGDP_s = seq(-3, 3, length.out = 30), # need meie poolt valitud lGDP_s väärtused, millele me ennustame vastavad eluead lpop_s = 0 # rahvaarv fikseeritakse muutuja keskmisele tasemele, mis standardiseeritud andmete korral = 0 ) # compute counterfactual mean lifeExp (mu) mu &lt;- link(m5, data = pred.data) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI) # simulate counterfactual lifeExpectancies of individual countries R.sim &lt;- sim(m5, data = pred.data) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] R.sim &lt;- na.omit(R.sim) R.PI &lt;- apply(R.sim, 2, PI) ggplot(pred.data, aes(lGDP_s, mu.mean)) + geom_line(y = mu.mean) + geom_ribbon(ymin = mu.PI[1,], ymax = mu.PI[2,], fill = &quot;grey60&quot;, alpha = 0.3) + geom_ribbon(ymin = R.PI[1,], ymax = R.PI[2,], fill = &quot;grey10&quot;, alpha = 0.3) Joonis 4.4: Ennustav plot Näeme, kuidas ennustus sobib/ei sobi andmetega. Võrdle eelneva ennustuspildiga, kus mudel ei sisalda rahvaarvu. Ennustuse intervallid on originaalandmete skaalas (aastates), mis on hea. Posterior prediction plots Posterioorsed ennustusplotid panevad kõrvuti (või üksteise otsa) Y-i algandmed ja mudeli ennustused Y-väärtustele. Kui meie valimi suurus on N, siis me tõmbame mudelist näiteks 5 valimit, igaüks suurusega N ja plotime need kõrvuti valimiandmete plotiga. Siis me vaatame sellele plotile peale ja otsustame, kas mudeli ennustused on piisavalt lähedal valimi andmetele. Kui ei, siis on tõenäoline, et meie mudelis on midagi mäda ja me peame hakkama sealt vigu otsima. Tõsi küll, keerulisemate hierarhiliste mudelite korral on vahest raske otsustada, millised peaksid tulema eduka mudeli ennustused võrreldes algandmetega — aga siiski, see on arvatavasti kõige tähtsam plot, mida oma mudelist teha! võrdle mudeli ennustusi andmetega. (Aga arvesta sellega, et mitte kõik mudelid ei püüagi täpselt andmetele vastata.) yrep &lt;- sim(m5) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] ppc_dens(g2007$lifeExp, yrep[1:5, ]) Joonis 16.2: Valimi andmed vs. mudeli poolt ennustatud andmed. Millisel viisil täpselt meie mudel ebaõnnestub? See plot annab mõtteid, kuidas mudelit parandada. Ploti ennustused andmepunktide vastu, pluss jooned, mis näitavad igale ennustusele omistatud usaldusintervalli. Lisaks veel sirge, mis näitab täiuslikku ennustust (slope = 1, intercept = 0). Ja nüüd plotime ennustused Y-le tegelike Y valimi väärtuste vastu: mu &lt;- link(m5) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu , 2 , PI) g2007$mu.mean &lt;- mu.mean ggplot(g2007, aes(lifeExp, mu.mean)) + geom_point() + geom_crossbar(ymin = mu.PI[1,], ymax = mu.PI[2,]) + geom_abline(intercept = 0, slope = 1, lty = 2) + ylab(&quot;Predicted life expectancy&quot;) + xlab(&quot;Observed life expectancy&quot;) + coord_cartesian( xlim=c( 40, 85 ), ylim=c( 40, 85 )) Joonis 4.5: Ennustus vs. valimi väärtus Siin on ennustus ja seda ümbritsev ebakindlus iga riigi keskmisele elueale. Järgnev plot annab ennustusvea igale riigile. Siin tähistab 89% CI näiteks Vietnamile eluigade vahemikku, millese jääb mudeli ennustuse kohaselt 89% kõikvõimalike fiktsionaalsete riikide keskmistest eluigadest, mille SKP ja rahvaarv võrdub Vietnami omaga. Kuna me tsentreerime CI Vietnami tegeliku keskmise eluea residuaalile (erinevusele mudeli ennustusest), näitab see, kui palju erineb Vietnami eluiga mudeli ennustusest riikidele, nagu Vietnam. See plot annab meile riigid, mille suhtes mudel jänni jääb. Enamasti leiame need riigid Aafrikast. # compute residuals life.resid &lt;- g2007$lifeExp - mu.mean mu_sim &lt;- sim( m5 ) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] sim.PI &lt;- apply( mu_sim , 2 , PI ) ggplot(g2007, aes(x = life.resid, y = reorder(country, life.resid))) + geom_point() + geom_errorbarh(aes(xmin = lifeExp - sim.PI[1,], xmax = lifeExp - sim.PI[2,] ), color = &quot;red&quot;) + geom_vline(xintercept = 0) + theme(text = element_text(size = 7), axis.title.y = element_blank()) Joonis 13.6: Ennustused riigi kaupa. punased jooned näitavad 89% ennustuspiire igale residuaalile riigi tasemel (89% kõikvõimalike riikide keskmiste eluigade residuaalidest sellel SKPl jääb punasesse vahemikku). Interaktsioonid prediktorite vahel Eelnevad mudelid eeldavad, et prediktorite varieeruvused on üksteisest sõltumatud. Aga mis siis, kui see nii ei ole ja ühe prediktori mõju suurus sõltub teisest prediktorist, ehk prediktorite vahel on interaktsioon? Lihtsaim viis sellist interaktsiooni modelleerida on lisades interaktsiooni aditiivsele mudelile korrutamisetehtena: \\(y = a + b1x1 + b2x2 + b3x1x2\\) Sellise mudeli järgi erineb sirge tõus b1 erinevatel b2 väärtustel, ja erinevuse määr sõltub b3-st (b3 annab interaktsiooni tugevuse). Samamoodi ja sümmeetriliselt erineb ka tõus b2 sõltuvalt b1 väärtusest. See on ühine paljude hierarhiliste mudelitega, mida võib omakorda vaadelda massivsete interaktsioonimudelitena. Seevastu y = a + b1x1 + b2x2 tüüpi mudel annab b1-le konstantse tõusunurga, kuid laseb intercepti muutuma sõltuvalt b2 väärtusest (ja vastupidi). Interaktsioonimudeli fittimises pole midagi erilist võrreldes sellega, mida me oleme juba õppinud. Aga fititud parameetrite tõlgendamine on keeruline. Alustame diskreetse muutujaga, continent, ja mudeldame selle interaktsiooni SKP-ga. f1 &lt;- glimmer(lifeExp ~ lGDP_s * continent, data = g2007) #&gt; alist( #&gt; lifeExp ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept + #&gt; b_lGDP_s*lGDP_s + #&gt; b_continentAmericas*continentAmericas + #&gt; b_continentAsia*continentAsia + #&gt; b_continentEurope*continentEurope + #&gt; b_continentOceania*continentOceania + #&gt; b_lGDP_s_X_continentAmericas*lGDP_s_X_continentAmericas + #&gt; b_lGDP_s_X_continentAsia*lGDP_s_X_continentAsia + #&gt; b_lGDP_s_X_continentEurope*lGDP_s_X_continentEurope + #&gt; b_lGDP_s_X_continentOceania*lGDP_s_X_continentOceania, #&gt; Intercept ~ dnorm(0,10), #&gt; b_lGDP_s ~ dnorm(0,10), #&gt; b_continentAmericas ~ dnorm(0,10), #&gt; b_continentAsia ~ dnorm(0,10), #&gt; b_continentEurope ~ dnorm(0,10), #&gt; b_continentOceania ~ dnorm(0,10), #&gt; b_lGDP_s_X_continentAmericas ~ dnorm(0,10), #&gt; b_lGDP_s_X_continentAsia ~ dnorm(0,10), #&gt; b_lGDP_s_X_continentEurope ~ dnorm(0,10), #&gt; b_lGDP_s_X_continentOceania ~ dnorm(0,10), #&gt; sigma ~ dcauchy(0,2) #&gt; ) m1 &lt;- map2stan(f1$f, f1$d) plot(precis(m1)) Joonis 4.7: Mudeli koefitsientide plot. Aafrika on siin võrdluseks. Interaktsioon on sümmeetriline. Me võime sama hästi küsida, kui palju SKP mõju elueale sõltub kontinendist, kui seda, kui palju kontinendi mõju eluale sõltub SKP-st. Nüüd joonistame välja regressioonisirge Aafrika ja Euroopa jaoks eraldi m1 mudeli põhjal c1 &lt;- coef(m1) names(c1) #&gt; [1] &quot;Intercept&quot; &quot;b_lGDP_s&quot; #&gt; [3] &quot;b_continentAmericas&quot; &quot;b_continentAsia&quot; #&gt; [5] &quot;b_continentEurope&quot; &quot;b_continentOceania&quot; #&gt; [7] &quot;b_lGDP_s_X_continentAmericas&quot; &quot;b_lGDP_s_X_continentAsia&quot; #&gt; [9] &quot;b_lGDP_s_X_continentEurope&quot; &quot;b_lGDP_s_X_continentOceania&quot; #&gt; [11] &quot;sigma&quot; Kõigepealt defineerime X1 ja X2 väärtused, millele teeme ennustused link() funktsiooni abil. Link tabelist veergude keskmine annab keskmise eluea ennustuse vastavale mandrile ja SKP-le. PI() abil saame 89% CI igale ennustusele. dd &lt;- as.data.frame(f1$d) #we use the dataframe made by glimmer() #in dd all continents are in separate 2-level columns (except Africa) dd1 &lt;- dd %&gt;% filter(continentAmericas == 0, continentAsia == 0, continentEurope == 0, continentOceania == 0) mu.Africa &lt;- link(m1, dd1) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] mu.Africa.mean &lt;- apply(mu.Africa, 2, mean) mu.Africa.PI &lt;- apply(mu.Africa, 2, PI, prob = 0.9) ggplot(dd1, aes(lGDP_s, lifeExp)) + geom_point() + geom_ribbon(aes(ymin = mu.Africa.PI[1,], ymax = mu.Africa.PI[2,]), alpha = 0.15) + geom_line(aes(y = mu.Africa.mean)) Joonis 13.8: Ennustusplot Aafrikale. dd1 &lt;- dd %&gt;% filter(continentEurope == 1) mu.Europe &lt;- link(m1, dd1) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] mu.Europe.mean &lt;- apply( mu.Europe , 2 , mean ) mu.Europe.PI &lt;- apply( mu.Europe , 2 , PI , prob=0.9 ) ggplot(data=dd1, aes(lGDP_s, lifeExp)) + geom_point()+ geom_ribbon( aes(ymin=mu.Europe.PI[1,], ymax=mu.Europe.PI[2,]), alpha=0.15)+ geom_line( aes( y=mu.Europe.mean)) Joonis 13.9: Ennustusplot Euroopale. Nagu näha, on meil nüüd üsna erinevad sirge tõusunurgad. Interaktsioonid pidevatele tunnustele Kasutame standardiseeritud prediktoreid, sest nende koefitsiente saab paremini tõlgendada (tegelikult piisab prediktorite tsentreerimisest). Meie andmed käsitlevad diabeedimarkereid Ameerika lõunaosariikide neegritel 1960-ndatel. Me ennustame siin sõltuvalt vanusest ja vööümbermõõdust hdl-i — high density cholesterol — mis on nn hea kolesterool. diabetes &lt;- read.csv2(&quot;data/diabetes.csv&quot;) d1 &lt;- diabetes %&gt;% select(hdl, age, waist) %&gt;% na.omit() d2 &lt;- d1 %&gt;% mutate(age_st = (age - mean(age)) / sd(age), waist_st = (waist - mean(waist)) / sd(waist)) m2 &lt;- map2stan( alist( hdl ~ dnorm( mu , sigma ) , mu &lt;- a + bR*age_st + bA*waist_st + bAR*age_st*waist_st, a ~ dnorm( 0, 100 ), bR ~ dnorm( 0, 2 ), bA ~ dnorm( 0, 2 ), bAR~ dnorm( 0, 2), sigma ~ dcauchy( 0, 1 ) ), data = d2) plot(precis(m2)) Joonis 16.3: mudeli koefitsientide plot NB! Järgmised interpretatsioonid kehtivad ainult siis, kui mudeldame nullile tsentreeritud andmeid. a - hdl-i oodatav keskväärtus siis kui võõ-ümbermõõt ja vanus on fikseeritud oma keskmistel väärtustel. bR - oodatav hdl-i muutus, kui vanus kasvab 1 aasta võrra ja võõ-ümbermõõt on fikseeritud oma keskväärtusel bA - sama, kui võõ-ümbermõõt kasvab 1 ühiku (inch) võrra bAR - kaks ekvivalentset tõlgendust: 1) oodatav muutus vanuse mõju määrale hdl-le, kui vöö-ümbermõõt kasvab 1 ühiku võrra. 2) oodatav muutus vöö-ümbermõõdu mõju määrale hdl-le, kui vanus kasvab 1 ühiku võrra. Negatiivne bAR tähendab, et vanus ja vöö-ümbermõõt omavad vastandlikke mõjusid hdl-i tasemele, aga samas kumgki tõstab teise tähtsust hdl-le. m3 &lt;- map2stan( alist( hdl ~ dnorm(mu, sigma), mu &lt;- a + bR * age_st + bA * waist_st, a ~ dnorm(0, 100), c(bR, bA) ~ dnorm(0, 2), sigma ~ dcauchy(0, 1) ), data = d2) compare(m2, m3) #&gt; WAIC pWAIC dWAIC weight SE dSE #&gt; m3 3391 5.6 0.0 0.72 41.8 NA #&gt; m2 3393 7.1 1.9 0.28 41.9 1.91 Siin on tegelikult eelistatud ilma interaktsioonita mudel. Aga kuna interaktsioonimudeli kaal on ikkagi 28%, tasub meil ennustuste tegemisel mõlemat mudelit koos arvestada vastavalt oma kaalule. coeftab(m2, m3) #&gt; m2 m3 #&gt; a 50.4 50.4 #&gt; bR 1.12 1.09 #&gt; bA -4.13 -4.10 #&gt; bAR -0.54 NA #&gt; sigma 16.6 16.6 #&gt; nobs 400 400 Tõesti, bA ja bR on mõlemas mudelis väga sarnased. m3 on kindlasti lihtsamini tõlgendatav. Ensemble teeb ära nii link()-i kui sim()-i, kasutades mõlemat mudelit vastavalt nende mudelite WAIC-i kaaludele ja toodab listi, mille elementideks on link() toodetud maatriks ja sim() toodetud maatriks. Teeme 3 plotti: waist = 0 (keskmine), waist = -1 (miinus üks sd) ja waist = 1 waist_fun &lt;- function(waist, ...) { d.pred &lt;- data.frame(age_st = seq( -2, 2, length.out = 20 ), waist_st = waist) e &lt;- ensemble(..., data = d.pred) hdl &lt;- apply(e$link, 2, mean) mu.PI &lt;- apply(e$link, 2, PI, prob = 0.97) ggplot(d.pred, aes(x = age_st)) + geom_line(aes(y = hdl)) + geom_line( aes(y = mu.PI[1,]), linetype = 2) + geom_line( aes( y = mu.PI[2,] ), linetype = 2) + ylim(40, 70) } Ensemble mudel: ## Fit ensemble model # p &lt;- lapply(-1:1, waist_fun, m2, m3) ## Plot three plots # do.call(grid.arrange, c(p, ncol = 3)) ## p_1 &lt;- waist_fun(-1, m2, m3) p0 &lt;- waist_fun(0, m2, m3) p1 &lt;- waist_fun(1, m2, m3) grid.arrange(p_1, p0, p1, ncol = 3) #&gt; Warning: Removed 2 rows containing missing values (geom_path). Joonis 16.4: Ennustusplot üle kahe mudeli. Ja sama ainult ühe mudeliga – m2. w0 &lt;- waist_fun(-1, m2) w_1 &lt;- waist_fun(0, m2) w1 &lt;- waist_fun(1, m2) grid.arrange(w0, w_1, w1, ncol = 3) Joonis 13.10: Ennustusplot m2-le. Nüüd on hästi näha, et interaktsioonimudel laseb sirge tõusunurgad vabaks! Üldiselt tasub interaktsioon mudelisse sisse kirjutada siis, kui see interaktsioon on teoreetiliselt mõtekas (ühe prediktori mõju võiks sõltuda teise prediktori tasemest). Interaktsiooni koefitsiendi määramine võib suurendada ebakindlust teiste parameetrite määramisel, seda eriti siis kui interaktsiooni parameeter on korreleeritud oma komponentide parameetritega (vt pairs(model)). Isegi kui interaktsiooniparameetri posteerior hõlmab 0-i, tuleb interaktsiooni parameetrit mudelisse pannes arvestada, et individuaalsete prediktorite mõju ei saa summeerida pelgalt läbi nende koefitsientide. Selle asemel tuleb vaadata sirge tõusu erinevatel teiste prediktorite väärtustel (nagu eelneval joonisel) Kui tavaline interaktsioonimudel on \\(y = a + b_1x_1 + b_2x_2 + b_3x_1x_2\\), siis mis juhtub, kui meie mudel on \\(y = b_1x_1 + b_3x_1x_2\\)? See tähendab, et me surume b2 väärtuse nulli, mis võib ära rikkuda mudeli teiste parameetrite posteeriorid! Kui teil on alust arvata, et b2-l puudub otsene mõju y väärtusele (kuid tal on mõju b1 väärtusele), siis võib muidugi ka sellist mudelit kasutada. Aga see on haruldane juhtum. "],
["mitmetasemelised-mudelid.html", "17 Mitmetasemelised mudelid 17.1 kahetasemeline mudel agebra keeles 17.2 mitmetasemeline mudel R-i mudelikeeles 17.3 Mitmetasemeliste mudelite lisaeeldused 17.4 Mitmetasemeline mudel töötab korraga mitmel tasmel Shrinkage ANOVA-laadne mudel Vabad interceptid klassikalises regressioonimudelis Vabad tõusud ja interceptid Hierarhiline mudel pidevate prediktoritega", " 17 Mitmetasemelised mudelid Mitmetasemeline mudel on regressioonimudel, kus andmed on struktureeritud gruppidesse ja mudeli koefitsiendid võivad erineda grupist gruppi. Statistika teooria ütleb, et me peaksime oma mudelitesse hõlmama need faktorid, mida kasutati eksperimendi disainis. Mitmetasandilised mudelid on parim viis, kuidas mudelisse panna katsedisainis esinevaid erinevatel tasemetel klastreid ja samas vältida mudeli ülefittimist. Mitmetasandiline mudel kajastab sellise katse või vaatluse struktuuri, kus andmed ei grupeeru mitte ainult katse- ja kontrolltingimuste vahel, vaid ka lisaklastritesse ehk gruppidesse. Näiteks, kui me mõõdame platseebo-kontrollitud uuringus kümmet patsienti ja teeme igale patsiendile viis kordusmõõtmist (kahetasemeline mudel). Või kui meil on geeniuuring, kus uuritakse korraga 1000 valgu taset ja uuring toimub 10 laboris (mitte-hierarhiline 3-tasemeline mudel). Või kui mõõdame kalamaksaõli mõju matemaatikaeksami tulemustele kümnes koolis, ja igas neist viies klassis (hierarhiline kolmetasemeline mudel). Tavapärane lähenemine oleks kõigepealt keskmistada andmed iga klassi sees ning seejärel keskmistada iga kooli sees (võtta igale koolile 5 klassi keskmine). Ning seejärel, võttes iga kooli keskmise üheks andmepunktiks, teha soovitud statistiline test (N = 10, sest meil on 10 kooli). Paraku, sellisel viisil talitades alahindame varieeruvust, mistõttu meie statistiline test alahindab ebakindluse määra arvutatud statistiku ümber. Hierarhilised mudelid, mis kajastavad adekvaatselt katse struktuuri, aitavad sellest murest üle saada. Üldine soovitus on, et kui teie katse struktuur seda võimaldab, siis peaksite alustama modelleerimist hierarhilistest mudelitest. Mitmetasemelised mudelid on eriti kasulikud, kui teil on osades klastrites vähem andmepunkte kui teistes, sest nad vaatavad andmeid korraga nii klastrite vahel kui klastrite sees ning kannavad informatsiooni üle klastritest, kus on rohkem andmepunkte, nendesse klastritesse, kus on vähe andmeid. See parandab hinnangute täpsust. Tavapärane regressioonimudel on sageli vaadeldav mitmetasandilise mudeli erijuhuna. Näiteks kujutage ette mudelit, kus laste õppedukust on mõõdetud mitmes koolis. Kui gruppide (koolide) vaheline varieeruvus on väga madal, siis annab mitmetasemeline mudel sarnase tulemuse lihtsa mudeliga, kus kõik koolid on ühte patta kokku pandud. Ja vastupidi, kui koolide vaheline varieeruvus on väga suur, võrreldes koolide sisese varieeruvusega, siis võime sama hästi modelleerida iga kooli eraldi ja teistest sõltumatult. Samuti, kui meil on andmeid väga väheste koolide kohta, siis võib mitmetasandilsest mudelist saadav kasu olla tagasihoidlik, sest meil pole piisavalt andmeid, et modelleerida koolide vahelist varieeruvust. Samuti, kui meil on iga kooli kohta piisavalt palju andmeid, siis saame iga kooli eraldi modelleerides praktiliselt sama tulemuse kui mitmetasemelisest mudelist. Muudel juhtudel on tõenäoliselt mõistlikum modelleerida õppedukust kahetasemelises mudelis, korraga õpilase tasemel ja kooli tasemel. 17.1 kahetasemeline mudel agebra keeles tase on õpilse tase, 2. tase on klassi tase. meil on J klassi, milles on erinev arv nj õpilasi. Iga õpilase kohta teame populaarsusindeksit (y - muutuja) ja sugu (x - muutuja). Klassi tasemel teame õpetaja staazi aastates (z - muutuja). Alustuseks on meil eraldi regressioonivõrrand igale klassile. \\[y_{ij} = b_{0j} + b_{1j}X_{ij} + e_{ij}\\] kus subskript j tähistab klassi ja i tähistab õpilast. Näit bi1 tähendab, et me fitime iga klassi kohta ühe b1 koefitsiendi. Seega eeldame, et igal klassil on erinev b0 ja b1 koefitsient. Residuaalvigadele eeldame, et mean = 0 ja e varieeruvus tuleb meil hinnata. Enamasti eeldame, et kõikide klasside varieeruvus on sama. b0 ja b1 omavad jaotust üle kõikide klasside, milledel on keskväärtus ja sd. Me modelleerime b0 ja b1 jaotusi tuues sisse klassi tasemel muutuja z: \\[b_{0j} = \\gamma_{00} + \\gamma_{01} Z_j + u_{0j}\\] \\[b_{1j} = \\gamma_{10} + \\gamma_{11} Z_j + u_{1j}\\] võrrand ennustab klassi keskmist populaarsusindeksit vastavalt õpetaja staazile. võrrand ütleb, et populaarsuse ja soo seose tugevus sõltub õpetaja staazist. kui y11 &gt; 0, siis on seos seda tugevam, mida staazikam on õpetaja. u0j ja u1j on residuaalide vead klassi tasemel, mille kohta me eeldame, et mean = 0 ja sõltuamtust residuaalide vigadest õpilase tasemel (eij). u0j residuaalide vigade variance on sigma_sq_u0 jne. u0j ja u1j covariance on sigma_sq_u01 ja me ei eelda, et see = 0. Gamma koefitsiendid ei varieeru klasside vahel. Asendades 1. võrrandis b0j ja b1j, saame oma võrrandisüsteemi muuta üheks pikaks võrrandiks. \\[Y_{ij} = \\gamma_{00} + \\gamma_{10} X_{ij} + \\gamma_{01} Z_j + \\gamma_{11}X_{ij} Z_j + u_{1j} X_{ij} + u_{0j} + e_{ij}\\] Siis näeme uues võrrandis liiget \\(\\gamma_1 Z_j X_{ij}\\), mis on interaktsiooniliige. Ilma interaktsioonita mudel näeb välja niimodi \\[Y_{ij} = \\gamma_{00} + \\gamma_{10} X_{ij} + \\gamma_{01} Z_j + u_{1j} X_{ij} + u_{0j} + e_{ij}\\] juhusliku vealiige \\(u_{ij}\\) korrutub \\(X_{ij}\\)-ga, mistõttu sellest tulenev totaalne viga on erinev erinevatel \\(X_{ij}\\) väärtustel. Seega on meie mudel heteroskedastiline ja erineb selle poolest tavalistest lineaarsetest mudelitest, mis eeldavad homoskedastilisust e residuaalvea sõltumatust X-i väärtusest. Teine oluline erinevus tavalisest lin mudelist on, et gupeeritud andmete puhul ei ole täidetud andmete iseseisvuse eeldus (gruppide sees modelleeritakse andmed korreleerituna - klassisisene korrelatsioon), mis muudab veapiirid liiga kitsaks. Klassisisese korrelatsiooni saame intercept-only mudelist \\[Y_{ij} = \\gamma_{00} + u_{0j} + e_{ij}\\] (selle mudeli saad, kui viskad pikast mudelist välja kõik X ja Z sisaldavad liikmed.) See mudel ajab varieeruvuse lahku kahe iseseisva komponendi vahel: \\(\\sigma^2_e\\) (1. taseme vigade variance \\(e_{ij}\\)) ja \\(\\sigma^2_{u0}\\) (2. taseme vigade variance \\(u_{0j}\\)). Klassisisene korrelatsioon: \\[\\rho = \\frac{\\sigma^2_{u0}}{\\sigma^2_{u0} + \\sigma^2_e}\\] rho annab grupistruktuuri poolt seletatud variace proportsiooni, mida võib tõlgendada kui kahe sama grupi juhusliku liikme vahelist oodatavat korrelatsiooni. Üldiselt on kasulik töötada standardiseeritud regressioonikoefitsientidega, mida saab tõlgendada sd ühikutes. Erandiks on olukord, kus analüüsi eesmärk on võrrelda erinevaid valimeid omavahel. Enamasti on kasulik standardiseerida andmed, mis mudelisse sisse lähevad, aga on võimalik ka standardiseerida koefitsiente kui selliseid. st_coef = (unstand_coef x sd(X))/sd(Y) Standardiseeritud andmete kasutamine muudab totaalselt varieeruvuskompnmentide fitte, aga jätab b koefitsientide fitid olemuselt samaks (see kehtib X-muutujate suvaliste lineaarsete transformatsioonide korral). Kui me jagame X-muutuja 2-ga, siis uus b1 = vana b1 x 2. Mudeli poolt seletamata varieeruvuse proportsioon ei muutu. Eelnev kehtib kuni mudeli tõusud (b1) ei ole vabaks lastud - st need ei varieeru klassist klassi. Tsentreerimine (\\(x_i - mean(x)\\)) mõjutab b0 aga mitte b1 koefitsiente, mis võib rääkida selle meetodi kasuks üle standardiseerimise ((\\(\\frac {x_i - mean(x)}{sd(x)}\\)), mis tekitab X muutujate jaotused, mille mean = 0 ja sd = 1. NB! grupi tasemel tsentreerimine, ehki vahest kasulik, töötab hoopis teistmoodi kui üle kõikide gruppide tsentreerimine ja viib täiesti erineva mudelini - sellest tuleks hoiduda, senikaua kui te ei tea täpselt, mida te teete. 17.1.1 Aegread Aegridasid saab analüüsida mitmetasemilisena, kus korduvad mõõtmised (1. tase) on grupeeritud indiviidide sisse (2. tase). Nii saab analüüsida ka ebaühtlase ajavahemiku järel tehtud mõõtmisi. \\(Y_{ti}\\) - indiviidi i õpitulemus ajapunktis t. \\(T_{ti}\\) - ajapunkt \\(X_{ti}\\) - ajas muutuv covariaat - kas õpilane töötab \\(Z_t\\) - ajast mittesõltuv kovariaat - sugu tase (indiviidi tase): \\[Y_{ti} = \\pi_{0i} + \\pi_{1i} x T_{ti} + \\pi_{2i} X_{ti} + e_{ti} \\] tase (üle indiviidie): \\[\\pi_{0i} = \\beta_{00} + \\beta_{01} Z_t + u_{01}\\] \\[\\pi_{1i} =\\beta_{10} + \\beta_{11} Z_t + u_{1i}\\] \\[\\pi_{2i} = \\beta_{20} + \\beta_{21} Z_t + u_{21}\\] Oluline punkt: aegridade modelleerimisel pakub meile sageli huvi ka ka korrelatsioon mudeli tõusu ja intercepti vahel. Kahjuks sõltub see näitaja sellest, millises skaalas me ajamuutuja mudelisse sisse anname. Oluline on tagada, et ajaskaala nullpunkt on mõtekas. To model growth - polynomial, logistic curve (first slow change, then quick, then slow again). Logistic parameters have meaning! Cubic polynomial approximates logistic and exponential curves - but here interpretation is on the level of some predicted growth curves. https://facebook.github.io/prophet sessoonsete andmete fittimine ennustavasse mudelisse https://github.com/nwfsc-timeseries kogu aegridade analüüsi pakette library(coda) #library(R2jags) library(gridExtra) library(broom) 17.1.2 Temporaalne autokorrelatsioon eeldus: Mida lähemal on 2 ajapunkti üksteisele, seda suurem on nende vaheline korrelatsioon (ja residuaalide vaheline korrelatsioon). Tavaline lm eeldab, et see korrelatsioon = 0. Seda korrelatsiooni saab kas hinnata, või selle vastu võidelda. Meie teeme siin viimast. Brms mudel näeb välja niimoodi data.temporalCor.brms &lt;- brm(y ~ x, data = d, autocor = cor_ar(formula = ~year)) cor_ar() on brms funktsioon autokorrelatsiooni modelleerimiseks. selle formula on ühepoolne valem ~t või ~ t | g, mis annab ajakovariaadi t ja grupeeriva faktori g. Kui g on antud, siis modelleeritakse iga grupp iseseisvalt ja teistest gruppidest sõltumata (gruppide vahel on korrelatsioon 0). A covariate for this correlation structure must be integer valued. When a grouping factor is present in formula, the correlation structure is assumed to apply only to observations within the same grouping level; observations with different grouping levels are assumed to be uncorrelated. Defaults to ~ 1, which corresponds to using the order of the observations in the data as a covariate, and no groups. Teine võimalus inkorporeerib mudelisse AR1 residuaalide autokorrelatsioonistruktuuri, kus korrelatsiooni eksponent väheneb ajas lineaarselt. Me eeldame, et see vähenemine toimub samamoodi sõltumata sellest, millises ajavahemikus me parasjagu oleme (statsionaarsuse eeldus). data.temporalCor.brms = brm(y ~ x, data = d, autocor = cor_ar( ~year, cov = TRUE)) Lihtsuse mõttes eeldame, et iga ajapunkti oodatud väärtus = tavaline lin prediktor + autokorrelatsiooni parameeter (\\(ρ\\)) korrutatuna eelmise vaatluse residuaaliga + tavapärane sõltumatu müra (\\(σ^2\\)). 17.2 mitmetasemeline mudel R-i mudelikeeles Kui meie muutujad andmetabelis “data” on y = õpilase testiskoor, x = katsetingimus (binaarne faktor katse-kontroll, kalamaksaöli - platseebo), ja kool, siis “ühepajamudel” väljendub R-i mudelikeeles: mudel &lt;- lm(y ~ x, data=data) ja mudel, kus iga kool on eraldi modelleeritud: mudelid &lt;- data %&gt;% group_by(kool) %&gt;% do(model = lm(y ~ x, data = .)) või purrr-i abil data %&gt;% split(.$kool) %&gt;% map(~ lm(y ~ x, data = .)) %&gt;% map(summary) %&gt;% map_dfr(~ broom::glance(.), .id = &quot;kool&quot;) Seevastu hierarhiline mudel kirjutatakse kui mudel &lt;- lme4::lmer(y ~ x + (1 + x | kool), data=data) või mudel &lt;- lme4::lmer(y ~ x + (1 | kool), data=data) Esimesel juhul modelleeritakse igale koolile nii tõus kui intercept ja teisel juhul modelleeritakse igale koolile ainult intercept, seeläbi eeldades, et kõikidel koolidel on mudelis sama tõus, ehk kalamaksaõli efekt (ES = testitulemus kalamaksaõli grupis - testitulemus platseebogrupis). Intercept tähendab sellises mudelis enamasti baastaset (kontrolltingimus) ja tõus tähendab katseefekti (katsetingimus - kontrolltingimus). Seega eeldab teine mudel, et igas grupis võib küll olla oma baastase, aga katsefekt sellest ei muutu. Lisaks, mudel mudel &lt;- lme4::lmer(y ~ x + (1 + x || kool), data=data) modelleerib igale koolile tõusu ja intercepti lisaeeldusega, et tõusude ja interceptide vaheline korrelatsioon puudub. Ilma selle eelduseta püüab mudel selle korrelatsiooni andmete põhjal leida. Kui andmeid on liiga vähe või mudel on liiga keeruline või korrelatsiooni võimalik esinemine tundub teadulsikult väga väheusutav, võib korrelatsiooni hindamisest loobuda, aga muidu tasub seda siiski hinnata. mudel &lt;- lme4::lmer(y ~ x + (1 + x | kool) + (1 + x | linn), data=data) Kui meil on mudelis rohkem kui 2 taset, kirjutame need sõltumata sellest, kas tasemed on hierarhiliselt üksteise sees (õpilane - kool - linn) või mitte (patsient - haigla - ravimi batch) mudel &lt;- lme4::lmer(y ~ x + (1 + x | grupp1) + (1 + x | grupp2), data=data) Kui esimesed 2 mudelit saab fittida lm() funktsiooniga, siis lihtne mitte-bayesiaanlik alternatiiv hierarhilise mudeli tarbeks on lme4 pakett (https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf), mis on lihtsam, kiirem ja ebatäpsem ad hoc viis arvutada mitmetasemeliseid mudeleid, kui Stan. Selle eelis Stani ees on eelkõige kiirus ja puuduseks on väiksem paindlikus mudelite formuleerimisel ja see, et väikeste valimite ja väheste gruppide puhul töötab lme4 algoritm palju halvemini, kui bayesi lahendused. Seega kasutame me pigem Stani, kui lme4. Samas, suurepärane pakett nimega brms (https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf) suudab tõlkida lme4 mudeli kirjelduse otse Stani keelde ja seda mudelit seal jooksutada. Brms teeb elu magusaks (vt lisa 2). Suurem sõltuvus valimi suurusest ja erinevatest lisaeeldustest võrreldes Bayesi mudelitega on see hind, mida ad hoc lahendused maksavad oma lihtsuse eest. Enamust klassikalisi teste (t test, chi ruut test, jms) võib vaadelda selliste ad hoc lahendustena, mis sageli lagunevad laiali väikesetel valimitel, samas kui bayes töötab väikeste valimitega hästi – tõsi küll, sõltudes väikeste valimite korral rohkem priorist ja andes seal realistlikult laiad usaldusintervallid. 17.3 Mitmetasemeliste mudelite lisaeeldused Mitmetasandilised mudelid toovad sisse lisaeelduse, et lineaarsuse/normaalsuse jm eeldused kehtivad igal mudeli tasemel. Samuti, et kõik grupid tulevad samast statistilisest populatsioonist, ja vastavalt sellele on nad mudelis koondatud ühise priori alla. Mitmetasemelises mudelis töötab grupi tasemel mudel priorina indiviidi tasemel mudelile. 17.4 Mitmetasemeline mudel töötab korraga mitmel tasmel Mudeli muudab mitmetasemeliseks see, et me määrame veamudelit kasutades mitte ainult indiviidi tasemel koefitsiente (1. tase), vaid anname neile koefitsientidele omakorda veamudeli (2. tase), mis modelleerib koefitsientide varieeruvust gruppide vahel. Selliseid tasemeid võib lisada põhimõtteliselt ükskõik kui palju. Kõrgema taseme mudel, lisaks sellele, et modelleerida gruppide vahelist varieeruvust, töötab ka priorina madalama taseme suhtes. Seega saab mudeli fittimisel 2. tase informatsiooni 1. tasemelt (andmete näol) ja samal ajal annab informatsiooni esimesele tasemele (priori kujul). Mitmetasemelised mudelid modelleerivad eksplitsiitselt varieeruvust klasrtite sees ja klastrite vahel. Nad modelleerivad indiviidi-tasemel regressioonikoefitsientide varieeruvust. Nad võimaldavad paremini määrata indiviidi tasemel regressioonikoefitsiente endid, eriti kui erinevates gruppides on erinev arv indiviide. Shrinkage Oletame, et te plaanite reisi Kopenhaagenisse ja soovite sellega seoses teada, kui kallis on keskeltläbi õlu selle linna kõrtsides. Teile on teada õlle hind kolmes Kopenhaageni kõrtsis, mida ei ole just palju. Aga sellele lisaks on teile teada ka õlle hind 6-s Viini, 4-s Praha ja 5-s Pariisi kõrtsis. Nüüd on teil põhimõtteliselt kolm võimalust, kuidas sellele probleemile läheneda. Te arvestate ainult Kopenhaageni andmeid ja ignoreerite teisi, kui ebarelevantseid. See meetod töötab hästi siis, kui teil on Kopenhaageni kohta palju andmeid (aga teil ei ole). Te arvestate võrdselt kõiki andmeid, mis teil on — ehk te võtate keskmise kõikidest õllehindadest, hoolimata riigist. See töötab parimini siis, kui päriselt pole vahet, millisest riigist te oma õlle ostate, ehk kui õlu maksab igal pool sama palju. Antud juhul pole see ilmselt parim eeldus. Te eeldate, et õlle hinna kujunemisel erinevates riikides on midagi ühist, aga et seal on ka erinevusi. Sellisel juhul tahate te fittida hierarhilise mudeli, kus teie hinnang õlle hinnale Kopenhaagenis sõltuks mingil määral (aga mitte nii suurel määral, kui eelmises punktis) ka teie kogemustest teistes linnades. Sama moodi, teie hinnang õlle hinnale Pariisis, Prahas jne hakkab mingil määral sõltuma kõikide linnade andmetest. Kui teil on olukord, kus te mõõdate erinevaid gruppe, mis küll omavahel erinevad, aga on ka teatud määral sarnased (näiteks testitulemused grupeerituna kooli kaupa), siis on mõistlik kasutada kõikide gruppide andmeid, et adjusteerida iga grupi spetsiifilisi parameetreid. Seda adjusteerimise määra kutsutakse “shrinkage”. Shrinkage toimub parameetri keskväärtuse suunas ja mingi grupi shrinkage on seda suurem, mida vähem on selles grupis liikmeid ja mida kaugemal asub see grupp kõikide gruppide keskväärtusest. See viib shrinkage koefitsientide kallutatusele (bias), aga samas ka suuremale täpsusele (precision). See tähendab, et shrinkage koefitsiendi hinnang on keskeltläbi lähemal tõelisele koefitsiendi väärtusele kui hannang tavalisele ühetasandilise mudeli koefitsiendile. Shrinkage on põhimõtteliselt sama nähtus, mis juba Francis Galtoni poolt avastatud regressioon keskmisele. Regressioon keskmisele on stohhastiline protsess kus, olles sooritanud n mõõtmist ja arvutanud nende tulemuste põhjal efekti suuruse, see valimi ES peegeldab nii tegelikku ES-i kui juhuslikku valimiviga. Kui valimivea osakaal ES-s on suur, siis lisamõõtmised vähendavad keskeltläbi efekti suurust. Shrinkage erineb sellest ainult selle poolest, et lisamõõtmised meenutavad ainult osaliselt algseid mõõtmisi. Kasutades hierarhilisi mudeleid saab võidelda ka valehäirete ehk mitmese testimise probleemiga. See probleem on lihtsalt sõnastatav: kui te sooritate palju võrdluskatseid ja statistilisi teste olukorras, kus tegelik katseefekt on tühine, siis tänu valimiveale annavad osad teie paljudest testidest ülehinnatud efekti. Seega, kui meil on kahtlus, et enamus võrdlusi on “mõttetud” ja me ei oska ette ennustada, millised võrdlused neist (kui üldse mõni) võiks anda tõelise teaduslikult mõtteka efekti, siis on lahendus kõiki saadud efekte kunstlikult pisendada kõikide efektide keskmise suunas. Mudeli kontekstis kutsutakse sellist lähenemist shrinkage-ks. Aga kui suurel määral seda teha? See sõltub nii sellest, kui palju teste me teeme, valimi suurusest, kui ka sellest, kuidas jaotuvad mõõdetud efektisuurused (milline on efektisuuruste varieeruvus testide vahel). Bayesi lahendus on, et me lisame mudelisse veel ühe hierarhilise priori, mis kõrgub üle gruppide-spetsiifilise priori. Seega anname me olemasolevale priorile uue kõrgema taseme meta-priori, mis tagab, et informatsiooni jagatakse gruppide vahel ja samal ajal ka gruppide sees. Sellise lahenduse õigustus on, et me usume, et erinevad alam-grupid pärinevad samast üli-jaotusest ja neil on omavahel midagi ühist (ehkki alam-gruppide vahel võib olla ka reaalseid erinevusi). Näiteks, et kõik klassid saavad oma lapsed samast lastepopulatsioonist, aga siiski, et leidub ka eriklasse eriti andekatele. Selline mudel tagab, et samamoodi nagu mudeli ennustused individaalsete andmepunktide kohta iga alam-grupi sees “liiguvad lähemale” oma alam-grupi keskmisele, samamoodi liiguvad ka alam-gruppide keskmised lähemale üldisele grupi keskmisele. Selle positiivne mõju on valealarmide vähendamine ja oht on, et me kaotame ka tõelisi efekte. Bayesi eelis on, et see oht realiseerub ainult niipalju, kuipalju meie mudel ei kajasta reaalset katse struktuuri. Klassikalises statistikas rakendatavad multiple testingu korrektsioonid (Bonfferroni, ANOVA jt) on kõik teoreetiliselt kehvemad. Lihtsaim shrinkage mudeli tüüp on mudel, kus me laseme vabaks interceptid, aga mitte tõusunurgad. Igale klastrile vastab mudelis oma intercepti parameeter ja oma intercepti prior. Lisaks annab mudel meile fittimise käigus valimi andmete põhjal ise parameetrid kõrgema taseme priorisse, mis on ühine kõikidele interceptidele. Seega me määrame korraga interceptide parameetrid ja kõrgema taseme priori parameetrid, mis tähendab, et informatsioon liigub mudelit fittides mõlemat pidi — mööda hierarhiat alt ülesse ja ülevalt alla. Selline mudel usub, et erinevate koolide keskmine tase erineb (seda näitab iga kooli intercept), aga juhul kui me mõõdame näiteks kalamaksaõli mõju õppeedukusele, siis selle mõju suurus ei erine koolide vahel (kõikide koolide tõusuparameetrid on identsed). Shrinkage kui nähtuse avastas Francis Galton 1870-ndatel aastatel. Galton ja tema sõbrad veetsid nimelt kümme aastat üle Inglismaa taimi kasvatades ja mõõtes erinevate põlvkondade seemnete suurusi. Eesmärk oli luua tühjale kohale uus teadus, pidevate tunnuste geneetika, ja katsete tulemus oli rabav. Nimelt leiti tugev seaduspära, mille kohaselt suurte seemnetega emataimede tütred andsid keskeltläbi väiksemaid seemneid kui nende vanemad ja vastupidi, väikeste seemnetega emade tütred andsid keskeltläbi suuremaid seemneid. Galtoni usk, et ta on avastanud tähtsa bioloogiaseaduse, purunes ca 1885, kui ta pääses analüüsima tuhatkonna inimese pikkusi andmestikus, mis sisaldas vanemate ja täiskasvanud laste pikkusi, ning leidis seal sama nähtuse. Sertifitseeritud geeniusena mõistis Galton, et ta ei olnud avastanud mitte niivõrd geneetikaseaduse, vaid peaaegu, et loogikaseaduse. Tema enda sõnadega: The average regression of the offspring to a constant fraction of their mid-parental deviations, is now shown to be a perfectly reasonable law which might have been deductively foreseen. It is of so simple a character that I have made an arrangement with pulleys and weights by which the probable average height of the children of known parents can be mechanically reckoned. (vt joonis). Sellega avastas Galton regressiooni keskmisele, mis on sisuliselt sama asi, mis shrinkage. Galton nägi, et shrinkagel on järgmised omadused: 1. “The mean filial regression towards mediocrity was directly proportional to the parental deviation from it.” Ehk, mida kaugemal on vanemad keskmisest, seda suurema amplituudiga on nende laste shrinkage 2. “The child inherits partly from his parents, partly from his ancestry. …the further his genealogy goes back, the more numerous and varied will his ancestry become … Their mean stature will then be the same as that of the race; in other words, it will be mediocre.” Ehk, shrinkage toimub alati, kui tunnuse väärtus ei ole deterministlikult määratud (shrinkage taandub korrelatsioonile vanemate ja laste varieeruvuse vahel) 3. “This law tells heavily against the full hereditary transmission of any gift. The more exceptional the amount of the gift, the more exceptional will be the good fortune of a parent who has a son who equals him in that respect.” Ehk, pidevate tunnuste korral on korrelatsioon alati &lt;1 &amp; &gt;-1 4. “The law is even-handed; it levies the same heavy succession-tax on the transmission of badness as well as of goodness. If it discourages the extravagant expectations of gifted parents that their children will inherit all their powers, it no less discountenances extravagant fears that they will inherit all their weaknesses and diseases.” Ehk shrinkage töötab võrdselt mõlemas suunas (ülevalt alla ja alt üles, aga ka vanematelt lastele ja lastelt vanematele). Seega ei ole shrinkage ajas toimuv, põhjuslik, ega isegi mitte füüsikaline protsess, vaid tõenäosusteooriast tulenev loogiline paratamatus. Samamoodi nagu shrinkage esineb vanemate-laste vahel esineb see ka valimi-kordusvalimi vahel kõigi valimite keskmise suunas (valimiefektid taanduvad välja sedamõõda, kuidas valimeid juurde tuleb). Ja samamoodi, kui me võtame valimi testitulemusi mitmest koolist, siis eeldusel, et õpilased on kõikides koolides sarnased (aga mitte identsed), toimub shrinkage kõikide koolide keskmise suunas. Seega, nihutades mingi kooli keskmist testitulemust koolide keskmise suunas, saame parema hinnangu selle kooli õpilaste teadmisetele kui pelgalt selles koolis õpilaste teadmisi mõõtes! ANOVA-laadne mudel Lihtne ANOVA on sageduslik test, mis võrdleb gruppide keskmisi mitmese testimise kontekstis. Siin ehitame selle Bayesi analoogi, mis samuti hindab gruppide keskmisi mitmese testimise kontekstis. Põhiline erinevus seisneb selles, et kui ANOVA punktennustus iga grupi keskväärtusele võrdub valimi keskväärtusega ja ANOVA pelgalt kohandab usaldusintervalle selle keskväärtuse ümber, siis bayesiaanlik mudel püüab ennustada igale grupile selle tegelikku kõige tõenäolisemat keskväärtust arvestades kõigi gruppide andmeid. Shrinkage-i roll on ekstreemseid gruppe “tagasi tõmmates” vähendada ebakindlust iga grupi keskmise ennustuse ümber. Shrinkage käigus tõmmatakse gruppe kõikide gruppide keskmise poole seda tugevamalt, mida kaugemal nad sellest keskmisest on. Sellega kaasneb paratamatult mõningane süstemaatiline viga, kus tõelised efektid tulevad välja väiksematena, kui nad tegelikult on. Kui ilma tegelike efektideta gruppide arv on väga suur võrreldes päris efektidega gruppidega, siis võib shrinkage meie pärisefektid sootuks ära kaotada. Kahjuks on see loogiline paratamatus; alternatiiviks on olukord, kus meie üksikud pärisefektid upuvad sama suurte pseudoefektide merre. Ok, aitab mulast, laadime vajalikud raamatukogud ja andmed ning vaatame mis saab. library(tidyverse) library(stringr) library(rethinking) library(psych) Andmed: The data contain GCSE exam scores on a science subject. Two components of the exam were chosen as outcome variables: written paper and course work. There are 1,905 students from 73 schools in England. Five fields are as follows. School ID Student ID Gender of student 0 = boy 1 = girl Total score of written paper Total score of coursework paper Missing values are coded as -1. schools &lt;- read_csv( &quot;data/schools.csv&quot;) schools &lt;- schools %&gt;% filter(complete.cases(.)) %&gt;% mutate_at(vars(sex, school), as.factor) ## map2stan requires data.frame class(schools) &lt;- &quot;data.frame&quot; Alustuseks mitte-hierarhiline mudel, mis arvutab keskmise score1 igale koolile eraldi. See on intercept-only mudel, mis tähendab, et me hindame testitulemuse keskväärtust kooli kaupa ja igale koolile sõltumatult kõigist teistest koolidest. Me ei püüa siin ennustada testitulemuste väärtusi x-i väärtuste põhjal. Selles mudelis on tavapärased ühetasemelised priorid, ainult mu on ümber nimetatud a_school-iks ja sellele on antud indeks [school], mis tähendab, et mudel arvutab a_school-i, ehk keskmise testitulemuse, igale koolile. Kuna siin puuduvad kõrgema taseme priorid, siis vaatab mudel igat kooli eraldi ja ühegi kooli hinnang ei arvesta ühegi teise kooli andmetega. schoolm2 &lt;- map2stan( alist( score1 ~ dnorm(mu, sigma), mu &lt;- Intercept + v_Intercept[school], Intercept ~ dnorm(0, 50), v_Intercept[school] ~ dnorm(0, 50), sigma ~ dcauchy(0, 2) ), data = schools) Vaata koefitsente. precis(schoolm2, depth = 2) Igale koolile antud hinnang on sõltumatu kõigist teistest koolidest. Ja nüüd hierarhiline mudel, mis teab koolide vahelisest varieeruvusest. Siin leiab a_school-i priorist teise taseme meta-parameetri nimega sigma_school, millele on defineeritud oma meta-prior. schoolm3 &lt;- map2stan(alist( score1 ~ dnorm(mu, sigma), mu &lt;- Intercept + v_Intercept[school], Intercept ~ dnorm(0, 50), v_Intercept[school] ~ dnorm(0, sigma_school), sigma_school ~ dcauchy(0, 2), sigma ~ dcauchy(0, 2) ), data = schools) precis(schoolm3, depth = 2) Nagu näha on sigma_school &lt; sigma, mis tähendab, et koolide vaheline varieeruvus on väiksem kui õpilaste vaheline varieeruvus neis koolides. Seega sõltub testi tulemus rohkem sellest, kes testi teeb kui sellest, mis koolis ta käib. Loogika on siin järgmine: samamoodi nagu testitulemustel on jaotus õpilasekaupa, on neil ka jaotus koolikaupa. Koolikaupa jaotus töötab priorina õpilasekaupa jaotusele. Aga samas vajab kooli kaupa jaotus oma priorit — ehk meta-priorit. Seega saame me samast mudelist hinnangu nii testitulemustele kõikvõimalike õpilaste lõikes, kui ka kõikvõimalike koolide lõikes. Mudel ennustab ka nende koolide ja õpilaste tulemusi, keda tegelikult olemas ei ole, aga kes võiksid kunagi sündida. Ning veel üks hierarhiline mudel, mis teab nii koolide skooride keskmiste varieeruvust kui koolide vahelist varieeruvust. Võrdleme mudeleid. compare(schoolm2, schoolm3) Siit nähtub, et m3 on parim mudel, aga ka m2 omab mingit kaalu. coeftab_plot(coeftab(schoolm2, schoolm3), cex = 0.5) Joonis 4.6: Mudelite koefitsiendid. Siin on hästi näha shrinkage m3 puhul võrreldes m2-ga, mis ei tee multiple testingu korrektsiooni. Nende koolide puhul, kus usaldusintervall on laiem, on ka suurem shrinkage (mudel võtab nende kohta suhteliselt rohkem infot teistest koolidest sest need koolid ise on mingil põhjusel suhteliselt infovaesed). Vabad interceptid klassikalises regressioonimudelis Ennustame score1 sõltuvust sex-ist. Küsimus: kui palju poiste ja tüdrukute matemaatikaoskused erinevad? Fitime mudeli, mis laseb vabaks intercepti. Selle mudeli eeldus on, et igal koolil on oma baastase (oma intercept), aga kõikide koolide efektid (mudeli tõusu-koefitsient) on identsed. describe(schools) #&gt; vars n mean sd median trimmed mad min max range #&gt; school* 1 1523 38.36 19.98 40.0 38.84 23.7 1.00 73 72.0 #&gt; student 2 1523 1016.45 1836.14 129.0 628.65 124.5 1.00 5516 5515.0 #&gt; sex* 3 1523 1.59 0.49 2.0 1.61 0.0 1.00 2 1.0 #&gt; score1 4 1523 46.50 13.48 46.0 46.68 13.3 0.60 90 89.4 #&gt; score2 5 1523 73.38 16.44 75.9 74.65 16.5 9.25 100 90.8 #&gt; skew kurtosis se #&gt; school* -0.18 -1.08 0.51 #&gt; student 1.69 0.98 47.05 #&gt; sex* -0.37 -1.87 0.01 #&gt; score1 -0.12 -0.05 0.35 #&gt; score2 -0.75 0.51 0.42 Me kasutame prediktorina binaarset kategoorilist muutujat. See on analoogiline olukord ANOVA mudelile, mis võtab arvesse multiple testingu olukorra, mis meil siin on. schools_f1 &lt;- glimmer(score1 ~ sex + (1 | school), data = schools) #&gt; alist( #&gt; score1 ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept + #&gt; b_sex1*sex1 + #&gt; v_Intercept[school], #&gt; Intercept ~ dnorm(0,10), #&gt; b_sex1 ~ dnorm(0,10), #&gt; v_Intercept[school] ~ dnorm(0,sigma_school), #&gt; sigma_school ~ dcauchy(0,2), #&gt; sigma ~ dcauchy(0,2) #&gt; ) Kuna glimmeri priorite parametriseeringud on vales skaalas (liiga väikesed), muudame neid nii, et intercept (keskmine testitulemus üle koolide) oleks tsentreeritud 50-le (max testi tulemus on 100) ja standardhälve on 20. Igaks juhuks tõstame veidi ka beta koefitsiendi priori sigmat. v_intercept peaks olema alati nullile tsentreeritud. Glimmeri väljundis on sama palju koolide veerge, kui palju on erinevaid koole, miinus üks. Selline binaarne numbriline väljund on Stani-le vajalik. Seega ei saa me faktortunnuste korral kasutada algset andmetabelit. head(schools_f1$d) schools_m1 &lt;- map2stan(alist( score1 ~ dnorm( mu , sigma ), mu &lt;- Intercept + b_sex1*sex1 + v_Intercept[school], Intercept ~ dnorm(50, 20), b_sex1 ~ dnorm(0, 15), v_Intercept[school] ~ dnorm(0, sigma_school), sigma_school ~ dcauchy(0,2), sigma ~ dcauchy(0,2) ), data = schools_f1$d) # use the data table generated by glimmer() #glimmer converts factors to Stan-eatable form. Siin on v_Intercept kooli-spetsiifiline korrektsioonifaktor, mis tuleb liita üldisele Interceptile. mean(v_Intercept) == 0. Me eeldame, et korrektsioonid on normaaljaotusega. Alternatiivne viis seda mudelit kirjutada oleks mu &lt;- Intercept[school] + b_sex1*sex1 ja see töötab smamoodi (nüüd on iga kooli intercept kohe eraldi). plot(precis(schools_m1, depth = 2), cex = 0.5) Joonis 14.2: Mudeli koefitsiendid precis(schools_m1) #&gt; Mean StdDev lower 0.89 upper 0.89 n_eff Rhat #&gt; Intercept 49.21 0.98 47.89 51.01 227 1 #&gt; b_sex1 -2.44 0.60 -3.37 -1.52 1000 1 #&gt; sigma_school 7.06 0.71 5.87 8.09 1000 1 #&gt; sigma 11.18 0.20 10.85 11.50 1000 1 sex = 1 ehk sex1 on tüdruk. Intercept annab siin sex = 0 (poisid) keskmise skoori kooli kaupa (kui liita üldisele interceptile kooli-spetsiifiline intercept). Kui tahame näiteks hinnangut 2. kooli tüdrukute skoorile (ehk tõelisele matemaatikavõimekusele) siis: Intercept + b_sex1 + intercept[2] annab meile selle posteeriori. Poistele sama 2. kooli kohta: Intercept + intercept[2] Ja poiste-tüdrukute erinevus skooripunktides võrdub b_sex1 Arvutame siis kooli nr 2 tüdrukute keskmise skoori posteeriori. schools_m1_samples &lt;- as.data.frame(schools_m1@stanfit) school_2_girls &lt;- schools_m1_samples$Intercept + schools_m1_samples$b_sex1 + schools_m1_samples$`v_Intercept[2]` ## Plot density histogram of intercepts dens(school_2_girls) Joonis 17.1: Tüdrukute skoori posteerior Ja Poiste oma school_2_boys &lt;- schools_m1_samples$Intercept + schools_m1_samples$`v_Intercept[2]` ## Plot density histogram of intercepts dens(school_2_boys) Joonis 16.3: Poiste skoori posteerior. Siin on eeldus, et kõikides koolides on sama poiste ja tüdrukute vaheline erinevus (b_sex1), kuid erinevad matemaatikateadmiste baastasemed (mudeli intercept on koolide vahel vabaks lastud, kuid tõus mitte). Vabad tõusud ja interceptid Milline näeb välja mudel, kus me laseme vabaks nii intercepti kui tõusu? schools_f2 &lt;- glimmer(score1 ~ sex + (1 + sex | school), data = schools) #&gt; alist( #&gt; score1 ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept + #&gt; b_sex1*sex1 + #&gt; v_Intercept[school] + #&gt; v_sex1[school]*sex1, #&gt; Intercept ~ dnorm(0,10), #&gt; b_sex1 ~ dnorm(0,10), #&gt; c(v_Intercept,v_sex1)[school] ~ dmvnorm2(0,sigma_school,Rho_school), #&gt; sigma_school ~ dcauchy(0,2), #&gt; Rho_school ~ dlkjcorr(2), #&gt; sigma ~ dcauchy(0,2) #&gt; ) nüüd on meil lisaparameetrid v_sex1, mis annab tõusu igale koolile eraldi ning Rho-school, mis annab korrelatsiooni intercepti ja tõusu vahel. Nüüd me jagame informatsiooni erinevat tüüpi parameetrite, nimelt interceptide ja tõusude, vahel. Selleks ongi vaja Rho lisa-parameetrit. Nüüd ei modelleeri me intercepti ja tõusu enam 2 eraldi normaaljaotuste abil vaid ühe 2-dimensionaalse normaaljaotusega (mvnorm2). Prior korrelatsioonile Interceptide ja tõusude vahel on rethinking::lkjcorr(). Selle ainus parameeter on K. Mida suurem K, seda rohkem on prior konsentreeritud 0 korrelatsiooni ümber. K = 1 annab tasase priori. Meie kasutame K = 2, mis töötab laia vahemiku mudelitega. R &lt;- rlkjcorr(1e4, K = 2, eta = 2) dens(R[, 1, 2] , xlab = &quot;correlation&quot;) Joonis 14.4: Korrelatsiooni prior on nõrgalt informatiivne – suunab posteeriori eemale ekstreemsetest korrelatsioonidest. schools_m2 &lt;- map2stan(alist( score1 ~ dnorm( mu , sigma ), mu &lt;- Intercept + b_sex1*sex1 + v_Intercept[school] + v_sex1[school]*sex1, Intercept ~ dnorm(50, 20), b_sex1 ~ dnorm(0, 20), c(v_Intercept,v_sex1)[school] ~ dmvnorm2(0, sigma_school, Rho_school), sigma_school ~ dcauchy(0,2), Rho_school ~ dlkjcorr(2), sigma ~ dcauchy(0,2) ), schools_f2$d) plot(precis(schools_m2, depth = 2), cex = 0.5) Joonis 17.2: Mudeli m2 koefitsiendid. Posteerior korrelatsioonile intercepti ja tõusu vahel: schools_m2_samples &lt;- extract.samples(schools_m2) df1 &lt;- schools_m2_samples$Rho_school %&gt;% as.data.frame() #df1 #corr matrix- we need only the V2 col dens(df1$V2) Joonis 16.4: Posteerior korrelatsioonile intercepti ja tõusu vahel. Meil on negatiivne korrelatsioon intercepti ja tõusu vahel. Seega, mida väiksem on poiste keskmine skoor koolis (=intercept), seda suurem om erinevus poiste ja tüdrukute skooride vahel (= tõus). Nüüd saab 2. kooli skoori tüdrukutele valemiga: Intercept + b_sex1 + v_intercept[2] + v_sex1[2] Sama skoor poistele: Intercept + v_intercept[2] ja tüdrukute ja poiste erinevus 2. koolile: b_sex1 + v_sex1[2] tüdrukute-poiste erinevus üle kõikide koolide: b_sex1 tüdrukute keskmine skoor üle kõikide koolide: Intercept + b_sex1 ja poiste keskmine skoor üle kõikide koolide: Intercept Tõmbame mudelist ennustused 1., 2. ja 37. kooli poiste skooridele järgmisel semestril: d.pred &lt;- list( school = c(1, 2, 37), sex1 = 0 ) schools_sim &lt;- rethinking::sim(schoolm2, data = d.pred) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] pred.p &lt;- apply(schools_sim, 2, mean) pred.p.PI &lt;- apply(schools_sim, 2 , PI) NB! kasutades rethinking::sim() saame me enustused andmepunktide (üksikute poiste tasemel). Antud juhul jääb ennustuse kohaselt esimeses koolis 89% individuaalseid skoore vahemikku 61-132 punkti 200-st võimalikust. Kui meid huvitab hoopis nende koolide keskmine skoor järgmisel semestril, siis kasuta rethinking::sim() asemel rethinking::link() funktsiooni. schools_sim &lt;- link(schools_m2, data = d.pred) #&gt; [ 100 / 1000 ] [ 200 / 1000 ] [ 300 / 1000 ] [ 400 / 1000 ] [ 500 / 1000 ] [ 600 / 1000 ] [ 700 / 1000 ] [ 800 / 1000 ] [ 900 / 1000 ] [ 1000 / 1000 ] pred.p &lt;- apply(schools_sim, 2, mean) pred.p.PI &lt;- apply(schools_sim, 2, PI) pred.p.PI #&gt; [,1] [,2] [,3] #&gt; 5% 32.6 34.4 44.3 #&gt; 94% 46.2 39.9 49.7 Esimeses koolis jääb keskmine poiste skoor 89% tõenäosusega vahemikku 33 kuni 46 punkti. compare(schools_m1, schools_m2) #&gt; WAIC pWAIC dWAIC weight SE dSE #&gt; schools_m1 11734 55.8 0.0 0.7 57.2 NA #&gt; schools_m2 11736 60.8 1.7 0.3 57.2 1.38 Tundub, et tõusude vabakslaskmine oli hea mõte. Ma saan hästi pihta, et erinevad koolid õpetavad matemaatikat erineva kvaliteediga. Aga miks peaks erinevates Inglismaa koolides olema erinev vahe poiste ja tüdrukute matemaatikateadmistel? Kas olukorras kus meil on hea kool, läheb see vahe väiksemaks või suuremaks? Tehke kindlaks!!! võrrelge graafiku slope vs. intercept. Joonis 13.12: mida suurem on koolis poiste skoor, seda väiksem on poiste ja tüdrukute erinevus Tõepoolest: mida suurem on koolis poiste skoor (parem kool), seda väiksem on poiste ja tüdrukute erinevus. Aga seos on kaunis nõrk! Muide sel joonisel tähendavad negatiivsed väärtused alla keskmist väärtust, mitte tingimata negatiivset erinevust või negatiivset skoori. Miks? Arvutage nüüd poiste ja tüdrukute keskmine skoor kooli kaupa ja vaadake uuesti sõltuvust samasse erinevusesse. Mis on õigem viis: kas fittida ilma interceptita mudel (nagu eelmises peatükis) ja kasutada otse selle koefitsiente või kasutada meie m2 mudelit ning arvutada selle mudeli koefitsientide põhjal uus statistik (kaalutud keskmine näiteks)? Miks? Hierarhiline mudel pidevate prediktoritega Siin püüame ennustada score1 mõju score2 väärtusele. plot(schools$score2, schools$score1) abline(lm(score1 ~ score2, data = schools)) Joonis 17.3: score1 vs. score2 Kõigepealt lihtne regressioon lm() funktsiooniga (see ei ole hierarhiline mudel). lm(score1 ~ score2, data = schools) #&gt; #&gt; Call: #&gt; lm(formula = score1 ~ score2, data = schools) #&gt; #&gt; Coefficients: #&gt; (Intercept) score2 #&gt; 17.971 0.389 score2 tõus 1 punkti võrra tõstab score1-e 0.39 punkti võrra. Modelleerime seost üle Bayesi hierarhilise mudeli, kus ainult Intercept on vabaks lastud. glimmer(score1 ~ score2 + (1 | school), data = schools) #&gt; alist( #&gt; score1 ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept + #&gt; b_score2*score2 + #&gt; v_Intercept[school], #&gt; Intercept ~ dnorm(0,10), #&gt; b_score2 ~ dnorm(0,10), #&gt; v_Intercept[school] ~ dnorm(0,sigma_school), #&gt; sigma_school ~ dcauchy(0,2), #&gt; sigma ~ dcauchy(0,2) #&gt; ) schoolm7 &lt;- map2stan(alist( score1 ~ dnorm(mu, sigma), mu &lt;- Intercept + b_score2 * score2 + v_Intercept[school], Intercept ~ dnorm(50, 50), b_score2 ~ dnorm(0, 10), v_Intercept[school] ~ dnorm(0, sigma_school), sigma_school ~ dcauchy(0, 2), sigma ~ dcauchy(0, 2) ), data = schools) Siin ei ole individuaalsed interceptid tõlgenduslikult informatiivsed, aga nende sissepanek parandab mudeli ennustust beta koefitsiendile (beta läheb väiksemaks ja ebakindlus selle hinnangu ümber kasvab). precis(schoolm7, depth = 2) Siin tuleb beta veidi väiksem - 0.36. Kuna sigma_school &lt; sigma, siis tundub, et koolide vaheline varieeruvus on väiksem kui laste vaheline varieeruvus (sigma on üle kõigi koolide). iga kooli baastase tuleb Intercept + v_Intercept[] aga selle mudeli järgi on kõikide koolide score2 ja score1 sõltuvus sama tugevusega. Laseme siis ka tõusud vabaks glimmer(score1 ~ score2 + (1 + score2 | school), data = schools) #&gt; alist( #&gt; score1 ~ dnorm( mu , sigma ), #&gt; mu &lt;- Intercept + #&gt; b_score2*score2 + #&gt; v_Intercept[school] + #&gt; v_score2[school]*score2, #&gt; Intercept ~ dnorm(0,10), #&gt; b_score2 ~ dnorm(0,10), #&gt; c(v_Intercept,v_score2)[school] ~ dmvnorm2(0,sigma_school,Rho_school), #&gt; sigma_school ~ dcauchy(0,2), #&gt; Rho_school ~ dlkjcorr(2), #&gt; sigma ~ dcauchy(0,2) #&gt; ) schoolm5 &lt;- map2stan(alist( score1 ~ dnorm( mu , sigma ), mu &lt;- Intercept + b_score2 * score2 + v_Intercept[school] + v_score2[school] * score2, Intercept ~ dnorm(50, 25), b_score2 ~ dnorm(0, 10), c(v_Intercept, v_score2)[school] ~ dmvnorm2(0, sigma_school, Rho_school), sigma_school ~ dcauchy(0, 2), Rho_school ~ dlkjcorr(2), sigma ~ dcauchy(0, 2) ), data = schools) nüüd saame igale koolile arvutada oma intercepti ja oma tõusu (ikka samamoodi: Intercept + v_intercept[] ja b_score2 + v_score2[]) precis(schoolm5, depth = 2) schoolm6 &lt;- map2stan(alist( score1 ~ dnorm(mu , sigma), mu &lt;- Intercept + b_score2 * score2, Intercept ~ dnorm(50, 50), b_score2 ~ dnorm(0, 10), sigma ~ dcauchy(0, 2) ), data = schools) m2 on selgelt parem mudel, kuigi m3 hinnangud interceptidele on suurema ebakindlusega. beta on nyyd 0.35 compare(schoolm7, schoolm6, schoolm5) #&gt; WAIC pWAIC dWAIC weight SE dSE #&gt; schoolm5 11380 78.4 0.0 1 56.4 NA #&gt; schoolm7 11416 59.5 35.5 0 56.0 11.6 #&gt; schoolm6 11862 3.3 482.0 0 54.6 41.6 0-mudel, mis on kõige kehvem, on kõige suurema betaga ja kõige väiksema ebakindlusega selle ümber. See on tavaline — hierarhiline mudel modelleerib ebakindlust paremini (realistlikumalt) ja vähendab üle-fittimise ohtu (beta tuleb selle võrra väiksem). precis(schoolm7, depth = 2) "],
["brms.html", "18 brms 18.1 brms-i töövoog 18.2 mudeli eelduste kontroll", " 18 brms brms on populaarne pakett, mis võimaldab kirjutada lihtsas ja lühidas keeles ka üsna keerulisi mudeleid ja need Stan-is fittida. Brms on ühe inimese (Paul Bürkner) projekt (https://github.com/paul-buerkner/brms), mis on jõudnud ette Stani meeskonna arendatavast analoogsest paketist rstanarm (https://github.com/stan-dev/rstanarm/blob/master/README.md). Paul Brükner oli mõned aastad tagasi psühholoogia doktorant, kes kirjutas brms-i naljaviluks oma doktoriprojekti kõrvalt, mille tulemusel on ta praegu teatud ringkonnis tuntum kui Lady Gaga. rstanarm, mide me siin ei käsitle, püüab pakkuda tavalisete sageduslikele meetoditele (ANOVA, lineaarne regressioon jne) bayesi analooge, mille mudeli spetsifikatsioon ja väljund erineks võimalikult vähe tavalisest baas-R-i töövoost. Brms on keskendunud mitmetasemelistele mudelitele ja püüab kasutada mudelite keelt, mis on harjumuspärane sageduslike hierarhiliste mudelite fittimise paketi lme4 (https://github.com/lme4/lme4/) kasutajatele. Loomulikult saab brms-is fittida ka lineaarseid ja mitte-lineaareid ühetasemelisi mudeleid, nagu ka imputeerida andmeid ja teha palju muud. library(tidyverse) library(brms) library(broom) library(bayesplot) library(mice) library(pROC) 18.1 brms-i töövoog brms-iga modelleerimisel on mõned asjad, mida tuleks teha sõltumata sellest, millist mudelit te parajasti fitite. Kõigepealt peaksite kontrollima, et mcmc ahelad on korralikult jooksnud (divergent transitions, rhat ja ahelate visuaalne inspekteerimine). Lisaks peaksite tegema posterioorse prediktiivse ploti ja vaatama, kui palju mudeli poolt genereeritud uued valimid meenutavad teie valimit. Samuti peaksite joonisel plottima residuaalid. Kui te inspekteerite fititud parameetrite väärtusi, siis tehke seda posteeriorite tasemel, k.a. koos veapiiridega. Kindlasti tuleks ka plottida mudeli ennustused koos usalduspiiridega. Enne tõsist mudeldamist kiikame irise andmetabelisse summary(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Min. :4.30 Min. :2.00 Min. :1.00 Min. :0.1 #&gt; 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 1st Qu.:0.3 #&gt; Median :5.80 Median :3.00 Median :4.35 Median :1.3 #&gt; Mean :5.84 Mean :3.06 Mean :3.76 Mean :1.2 #&gt; 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 3rd Qu.:1.8 #&gt; Max. :7.90 Max. :4.40 Max. :6.90 Max. :2.5 #&gt; Species #&gt; setosa :50 #&gt; versicolor:50 #&gt; virginica :50 #&gt; #&gt; #&gt; ggplot(iris, aes(Petal.Length, Sepal.Length)) + geom_point(aes(color = Species)) + geom_smooth(method = lm) + geom_smooth( color = &quot;black&quot;, size = 0.5) Loess fit viitab, et 3 liiki ühe sirgega mudeldada pole võib-olla optimaalne lahendus. ggplot(iris, aes(Petal.Length, Sepal.Length, color = Species)) + geom_point() + geom_smooth(method = lm) + geom_smooth(data=iris %&gt;% filter(Species ==&quot;virginica&quot;), se = FALSE, color = &quot;black&quot;, size = 0.5)+ geom_smooth(data=iris %&gt;% filter(Species ==&quot;versicolor&quot;), se = FALSE, color = &quot;black&quot;, size = 0.5)+ geom_smooth(data=iris %&gt;% filter(Species ==&quot;setosa&quot;), se = FALSE, color = &quot;black&quot;, size = 0.5)+ theme_classic() Nüüd on loess ja lm heas kooskõlas - seos y~x vahel oleks nagu enam-vähem lineaarne. Siit tuleb ka välja, et kolme mudeli tõusud on sarnased, interceptid erinevad. 18.1.1 kiire töövoog Minimaalses töövoos anname ette võimalikult vähe parameetreid ja töötame mudeliga nii vähe kui võimalik. See on mõeldud ülevaatena Bayesi mudeli fittimise põhilistest etappidest mudeli fittimine m_kiire &lt;- brm(Sepal.Length~Petal.Length, data= iris) write_rds(m_kiire, path = &quot;m_kiire.fit&quot;) Priorid on brms-i poolt ette antud ja loomulikult ei sisalda mingit teaduslikku informatsiooni. Nad on siiski “nõrgalt informatiivsed” selles mõttes, et kasutavad parametriseeringuid, mis enamasti võimaldavad mcmc ahelatel normaalselt joosta. Järgmises ptk-s õpime ise prioreid määrama. posteeriorid ja mcmc ahelate konvergents plot(m_kiire) Fiti kokkuvõte - koefitsiendid ja nende fittimise edukust hindavad statistikud (Eff.Sample, Rhat) m_kiire %&gt;% tidy #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept 4.307 0.0780 4.179 4.434 #&gt; 2 b_Petal.Length 0.409 0.0187 0.379 0.439 #&gt; 3 sigma 0.411 0.0243 0.373 0.453 #&gt; 4 lp__ -85.338 1.2173 -87.757 -84.015 Eff.Sample näitab efektiivset valimi suurust, mida ahelad on kasutanud. See on suht keeruline mõiste, aga piisab, kui aru saada, et see näitaja ei tohiks olla madalam kui paarkümmend. Rhat on statistik, mis vaatab ahelate konvergentsi. Kui Rhat &gt; 1.1, siis on kuri karjas. Rhat 1.0 ei tähenda paraku, et võiks rahulikult hingata – tegu on statistikuga, mida saab hästi tõlgendada häda kuulutajana, aga liiga sageli mitte vastupidi. Ennustav plot ehk marginal plot - mudeli fit 95% CI-ga. plot(marginal_effects(m_kiire), points=TRUE) 18.1.2 Põhjalikum töövoog Põhiline erinevus eelmisega on suurem tähelepanu prioritele, mudeli fittimise diagnostikale ning tööle fititud mudeliga. 18.1.3 Spetsifitseerime mudeli, vaatame ja muudame vaikeprioreid brms-i default priorid on konstrueeritud olema üsna väheinformatiivsed ja need tuleks enamasti informatiivsematega asendada. Igasse priorisse tuleks panna nii palju informatsiooni, kui teil on vastava parameetri kohta. Kui te mõne parameetri kohta ei oska öelda, milllised oleks selle mõistlikud oodatavad väärtused, siis saab piirduda brms-i antud vaikeväärtustega. Samas, kui keerulisemad mudelid ei taha hästi joosta (mida tuleb ikka ette), siis aitab sageli priorite kitsamaks muutmine. get_prior(Sepal.Length~Petal.Length + (1 | Species), data= iris) #&gt; prior class coef group resp dpar nlpar bound #&gt; 1 b #&gt; 2 b Petal.Length #&gt; 3 student_t(3, 6, 10) Intercept #&gt; 4 student_t(3, 0, 10) sd #&gt; 5 sd Species #&gt; 6 sd Intercept Species #&gt; 7 student_t(3, 0, 10) sigma Me fitime pedagoogilistel kaalutlustel shrinkage mudeli, mis tõmbab 3 liigi intercepte natuke keskmise intercepti suunas. On vaieldav, kas see on irise andmestiku juures mõistlik strateegia, aga me teeme seda siin ikkagi. Mitmetasemeline shrinkage mudel on abinõu ülefittimise vastu. Mudelite võrdlemisel otsitakse kompromissi - ehk mudeli, mille ennustused oleks andmepunktidele võimalikult lähedal ilma,et see mudel oleks liiga keeruliseks aetud (keerulisus on proportsionaalne mudeli parameetrite arvuga). Prioreid muudame nii prior &lt;- c(prior(normal(6, 3), class = &quot;Intercept&quot;), prior(normal(0, 1), class =&quot;b&quot;), prior(student_t(6, 0, 2), class = &quot;sigma&quot;)) Me valime siin nn väheinformariivsed priorid, nii et regressiooni tulemus on suht hästi võrreldav lme4 sagedusliku mudeliga. “b” koefitsiendi priorile (aga mitte “sigma” ega “Intercept”-le) võib anda ka ülemise ja/või alumise piiri [prior(normal(0, 1), class =“b”, lb= -1, ub=10) ütleb, et “b” prior on nullist erinev ainult -1 ja 10 vahel]. sigma priorid on automaatselt lb = 0-ga, sest varieeruvus ei tohi olla negatiivne. Alati tasub prioreid pildil vaadata, et veenduda nende mõistlikuses. x &lt;- seq(0, 10, length.out = 100) y &lt;- brms::dstudent_t(x, df = 6, mu = 0, sigma = 2, log = FALSE) plot(y~x) Sigma prior, mida brms kasutab, on vaikimisi pool sümmeetrilisest jaotusest, mis lõigatakse nulli kohalt pooleks nii, et seal puuduvad &lt; 0 väärtused (seega ei saa varieeruvuse posteerior minna alla nulli). Me võime ka prioreid ilma likelihoodideta (tõepärafunktsioonideta) läbi mudeli lasta, misjärel tõmbame fititud mudelist priorite valimid (neid võiks kutsuda ka “priorite posteerioriteks”) ja plotime kõik priorid koos. Seda pilti saab siis võrrelda koos andmetega fititud mudeli posteerioritega. Selle võimaluse kasutamine on tõusuteel, sest keerulisemate mudelite puhul võib priorite ükshaaval plottimine osutuda eksitavaks. Tekitame priorite valimid, et näha oma priorite mõistlikust (brm() argument on sample_prior = TRUE). Ühtlasi fitime ka oma mudeli koos andmete ja prioritega. m1 &lt;- brm(Sepal.Length~Petal.Length + (1 | Species), data= iris, prior = prior, family = gaussian, warmup = 1000, iter = 2000, chains = 3, cores = 3, sample_prior=TRUE) write_rds(m1, path = &quot;m1.fit&quot;) Me fittisime mudeli m1 kaks korda: nii andmetega (selle juurde jõuame varsti), kui ka ilma andmeteta. Kui panna sisse sample_prior = &quot;only&quot;, siis jookseb mudel ilma andmeteta, ja selle võrra kiiremini. Vaikeväärtus on sample_prior = &quot;no&quot;, mis tähendab, et fititakse ainult üks mudel - koos andmetega. Ilma andmeteta (likelihoodita) fitist saame tõmmata priorite mcmc valimid, mille ka järgmiseks plotime. samples1 &lt;- prior_samples(m1) sa &lt;- samples1 %&gt;% gather() ggplot(sa)+ geom_density(aes(value)) + facet_wrap(~key, scales=&quot;free_x&quot;) Kui kasutame sample_prior = &quot;only&quot; varianti, siis on esimene koodirida erinev: samples1 = as.data.frame(m1$fit). brms-i Intercepti priorite spetsifitseerimisel tasub teada, et brms oma sisemuses tsentreerib kõik prediktorid nullile (x - mean(x)), ja teie poolt ette antud prior peaks vastama neile tsentreeritud prediktoritele, kus kõikide prediktorite keskväärtus on null. Põhjus on, et tsentreeritud parametriseeringuga mudelid jooksevad sageli paremini. Alternatiiv on kasutada mudeli tavapärase süntaksi y ~ 1 + x (või ekvivalentselt y ~ x) asemel süntaksit y ~ 0 + intercept + x. Sellisel juhul saab anda priorid tsentreerimata predikroritele. Lisaks on brms selle süntaksi puhul nõus “b”-le antud prioreid vaikimisi ka intercepti fittimisel kasutama. 18.1.4 brm() funktsiooni argumendid: family - tõepärafunktsiooni tüüp (modelleerib y muutuja jaotust e likelihoodi) warmup - mitu sammu mcmc ahel astub, enne kui ahelat salvestama hakatakse. tavaliselt on 500-1000 sammu piisav, et tagada ahelate konvergents. Kui ei ole, tõstke 2000 sammuni. iter - ahelate sammude arv, mida salvestatakse peale warmup perioodi. Enamasti on 2000 piisav. Kui olete nõus piirduma posteeriori keskväärtuse arvutamisega ja ei soovi täpseid usaldusintervalle, siis võib piisata ka 200 sammust. chains - mitu sõltumatut mcmc ahelat jooksutada. 3 on hea selleks, et näha kas ahelad konvergeeruvad. Kui mitte, tuleks lisada informatiivsemaid prioreid ja/või warmupi pikkust. cores - mitu teie arvuti tuuma ahelaid jooksutama panna. adapt_delta - mida suurem number (max = 1), seda stabiilsemalt, ja aeglasemalt, ahelad jooksevad. thin - kui ahel on autokorreleeritud, st ahela eelmine samm suudab ennustada järgevaid (see on paha), siis saab salvestada näit ahela iga 5. sammu (thin = 5). Aga siis tuleks ka sammude arvu 5 korda tõsta. Vaikeväärtus on thin = 1. Autokorrelatsiooni graafilist määramist näitame allpool Järgmine funktsioon trükib välja Stani koodi, mis spetsifitseerib mudeli, mida tegelikult Stanis fittima hakatakse. See on väga kasulik, aga ainult siis kui tahate õppida otse Stanis mudeleid kirjutama. make_stancode(Sepal.Length~Petal.Length, data= iris, prior = prior) 18.1.5 Fitime mudeleid ja võrdleme fitte. Mudelis m1 ennustame muutuja Sepal.Length väärtusi Petal.Length väärtuste põhjal shrinkage mudelis, kus iga irise liik on oma grupis. Teine mudel, m2, sisaldab veel üht ennustavat muutujat (Sepal.Width). m2 &lt;- brm(Sepal.Length~Petal.Length + Sepal.Width + (1 | Species), data= iris, prior = prior, chains = 3, cores = 4, control = list(adapt_delta = 0.95)) write_rds(m2, path = &quot;m2.fit&quot;) Kolmandaks ühetasemeline mudel, m3, mis vaatab kolme irise liiki eraldi m3 &lt;- brm(Sepal.Length~ Sepal.Width + Petal.Length*Species, data= iris, prior = prior, chains = 3, cores = 3) write_rds(m3, path = &quot;m3.fit&quot;) Ja lõpuks lihtne mudel, m4, mis paneb kõik liigid ühte patta. m4 &lt;- brm(Sepal.Length~Petal.Length + Sepal.Width, data= iris, prior = prior, chains = 3, cores = 4) write_rds(m4, path = &quot;m4.fit&quot;) m2 &lt;- read_rds(&quot;m2.fit&quot;) m3 &lt;- read_rds(&quot;m3.fit&quot;) m4 &lt;- read_rds(&quot;m4.fit&quot;) Siin me võrdleme neid nelja mudelit. Väikseim looic (leave-one-out information criterion) võidab. See on suhteline võrdlus – looic abs väärtus ei mängi mingit rolli. loo(m1, m2, m3, m4) Siin on m1 ja m2/m3 mudeli erinevus 25 ühikut ja selle erinevuse standardviga on 10 ühikut. 2 SE-d annab umbkaudu 95% usaldusintervalli, ja see ei kata antud juhul nulli. Seega järeldame, et m2 ja m3, mis kasutavad ennustamiseks lisamuutujat, on selgelt eelistatud. Samas ei saa me õelda, et hierarhiline mudel m2 oleks parem või halvem kui interaktsioonimudel m3. Ka puudub oluline erinevus m1 ja m4 fiti vahel. Tundub, et selle ennustusjõu, mille me võidame lisaparameetrit mudeldades, kaotame omakorda liike ühte patta pannes (neid mitte osaliselt iseseisvana modelleerides). Alternatiivina kasutame brms::waic kriteeriumit mudelite võrdlemiseks. See töötab kiiremini kui LOO ja tõlgendus on sarnane - väikseim waic võidab ja absolutväärtusi ei saa ükshaaval tõlgendada. brms::waic(m1, m2, m3, m4) #&gt; WAIC SE #&gt; m1 106.40 16.64 #&gt; m2 81.55 16.10 #&gt; m3 79.96 15.76 #&gt; m4 100.53 16.41 #&gt; m1 - m2 24.85 9.63 #&gt; m1 - m3 26.44 10.67 #&gt; m1 - m4 5.87 15.28 #&gt; m2 - m3 1.60 3.34 #&gt; m2 - m4 -18.98 9.90 #&gt; m3 - m4 -20.57 9.95 Nagu näha, annavad LOO ja waic sageli väga sarnaseid tulemusi. Me ei süvene LOOIC ega waic-i statistilisse mõttesse, sest bayesi mudelite võrdlemine on kiiresti arenev ala, kus ühte parimat lahendust pole veel leitud. 18.1.6 vaatame mudelite kokkuvõtet Lihtne tabel mudeli m2 fititud koefitsientidest koos 95% usalduspiiridega tidy(m2) #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept 1.710 1.0270 0.095 3.345 #&gt; 2 b_Petal.Length 0.759 0.0645 0.651 0.866 #&gt; 3 b_Sepal.Width 0.440 0.0839 0.303 0.576 #&gt; 4 sd_Species__Intercept 1.725 1.5315 0.433 5.024 #&gt; 5 sigma 0.313 0.0186 0.284 0.345 #&gt; 6 r_Species[setosa,Intercept] 0.676 0.9925 -0.901 2.250 #&gt; 7 r_Species[versicolor,Intercept] -0.226 0.9842 -1.853 1.253 #&gt; 8 r_Species[virginica,Intercept] -0.642 0.9907 -2.277 0.809 #&gt; 9 lp__ -50.433 2.3680 -54.788 -47.169 r_ prefiks tähendab, et antud koefitsient kuulub mudeli esimesele (madalamale) tasemele (Liigi tase) r- random - tähendab, et iga grupi (liigi) sees arvutatakse oma fit. b_ tähendab mudeli 2. taset (keskmistatud üle kõikide gruppide). 2. tasmel on meil intercept, b1 ja b2 tõusud ning standardhälve y muutuja ennustatud andempunktide tasemel. 1. tasemel on meil 3 liigi interceptide erinevus üldisest b_Intercepti väärtusest. Seega, selleks, et saada setosa liigi intercepti, peame tegema tehte 1.616 + 0.765. tidy funktsiooni tööd saab kontrollida järgmiste parameetrite abil: tidy(x, parameters = NA, par_type = c(&quot;all&quot;, &quot;non-varying&quot;, &quot;varying&quot;, &quot;hierarchical&quot;), robust = FALSE, intervals = TRUE, prob = 0.9, ...) par_type = “hierarchical” kuvab grupi taseme parameetrite sd-d ja korrelatsioonid. “varying” kuvab grupi taseme interceptid ja tõusud (siis kui neid mudeldadakse). “non-varying” kuvab kõrgema taseme (grupi-ülesed) parameetrid. robust = TRUE annab estimate posteeriori mediaanina (vaikeväärtus FALSE annab selle aritmeetilise keskmisena posteeriorist). Nüüd põhjalikum mudeli kokkuvõte: m2 #&gt; Warning: There were 17 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. #&gt; See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: Sepal.Length ~ Petal.Length + Sepal.Width + (1 | Species) #&gt; Data: iris (Number of observations: 150) #&gt; Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup samples = 3000 #&gt; #&gt; Group-Level Effects: #&gt; ~Species (Number of levels: 3) #&gt; Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat #&gt; sd(Intercept) 1.72 1.53 0.36 5.98 503 1.00 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat #&gt; Intercept 1.71 1.03 -0.36 4.08 550 1.01 #&gt; Petal.Length 0.76 0.06 0.63 0.88 1380 1.00 #&gt; Sepal.Width 0.44 0.08 0.27 0.60 1857 1.00 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat #&gt; sigma 0.31 0.02 0.28 0.35 1882 1.00 #&gt; #&gt; Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample #&gt; is a crude measure of effective sample size, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). Siin on eraldi toodud grupi tasemel ja populatsiooni tasemel koefitsiendid ja gruppide vaheline sd (= 1.72). Pane tähele, et üldine varieeruvus sigma = 0.31 on palju väiksem kui gruppide vaheline varieeruvus sd(Intercept) = 1.72. Seega on grupid üksteisest tugevalt erinevad ja neid tuleks võib-olla tõesti eraldi modelleerida. Divergentsed transitsioonid on halvad asjad - ahelad on läinud 17 korda metsa. Viisakas oleks adapt deltat tõsta või kitsamad priorid panna, aga 17 halba andmepunkti paarist tuhandest, mille mcmc ahelad meile tekitasid, pole ka mingi maailmalõpp. Nii et las praegu jääb nagu on. Need divergentsed transitsioonid on kerged tekkima just mitmetasemelistes mudelites. 18.1.7 plotime posteeriorid ja ahelad plot(m2) Siit on näha, et ahelad on ilusti konvergeerunud. Ühtlasi on pildil posterioorsed jaotused fititud koefitsientidele. regular expressioni abil saab plottida mudeli madalama taseme ahelaid &amp; posteerioreid, mida plot() vaikimisi ei näita. #regex works! plot(m2, pars = &quot;r_&quot;, theme = theme_dark()) Vaatame korrelatsioone erinevate parameetrite posterioorsete valimite vahel. (Markovi ahelad jooksevad n-mõõtmelises ruumis, kus n on mudeli parameetrite arv, mille väärtusi hinnatakse.) pairs(m3) teeb pildi ära, aga ilusama pildi saab GGally::ggpairs() abil. pairs(m2, pars=&quot;b_&quot;) library(GGally) posterior_samples(m2) %&gt;% select(contains(&quot;b_&quot;)) %&gt;% ggpairs() Siin on posteeriorite põhjal arvutatud 50% ja 95% CI ja see plotitud. stanplot(m2, pars = &quot;r_&quot;, type = &quot;intervals&quot;) type= argument sisestamine võimaldab plottida erinevaid diagnostilisi näitajaid. Lubatud sisendid on “hist”, “dens”, “hist_by_chain”, “dens_overlay”, “violin”, “intervals”, “areas”, “acf”, “acf_bar”, “trace”, “trace_highlight”, “scatter”, “rhat”, “rhat_hist”, “neff”, “neff_hist” “nuts_acceptance”, “nuts_divergence”, “nuts_stepsize”, “nuts_treedepth” ja “nuts_energy”. stanplot(m2, type=&quot;neff&quot;) Neff on efektiivne valimi suurus ja senikaua kuni Neff/N suhe ei ole &lt; 0.1, pole põhjust selle pärast muretseda. 18.1.8 korjame ahelad andmeraami ja plotime fititud koefitsiendid CI-dega model &lt;- posterior_samples(m1) #model &lt;- m1$fit %&gt;% as.data.frame() ##töötab samamoodi mcmc_intervals() on bayesplot paketi funktsioon. me plotime 50% ja 95% CI-d. pars &lt;- names(model) mcmc_intervals(model, pars=pars[-length(pars)]) #with pars left out the last parameter lp_ Näeme, et sigma hinnang on väga usaldusväärne, samas kui gruppide vahelise sd hinnang ei ole seda mitte (pane tähele posterioorse jaotuse ebasümmeetrilisust). model2 &lt;- m2$fit %&gt;% as.data.frame() pars &lt;- names(model2) mcmc_intervals(model2, pars=pars[-length(pars)]) mcmc_areas(model2, pars=c(&quot;b_Petal.Length&quot;, &quot;b_Sepal.Width&quot;)) 18.1.9 bayesi versioon r-ruudust kui suurt osa koguvarieeruvusest suudavad mudeli prediktorid seletada? bayes_R2(m2) #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; R2 0.86 0.00831 0.84 0.873 bayes_R2(m1) #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; R2 0.833 0.0109 0.807 0.85 https://github.com/jgabry/bayes_R2/blob/master/bayes_R2.pdf Annab põhjenduse sellele statistikule (mille arvutamine erineb tavalisest vähimruutudega arvutatud mudeli r2-st). 18.1.10 plotime mudeli poolt ennustatud valimeid - posterior predictive check Kui mudel suudab genereerida simuleeritud valimeid, mis ei erine väga palju empiirilisest valimist, mille põhjal see mudel fititi, siis võib-olla ei ole see täiesti ebaõnnestunud mudeldamine. See on loogika posterioorse ennustava ploti taga. gridExtra::grid.arrange(pp_check(m1), pp_check(m2), pp_check(m3), nrow = 3) y - tihedusplot empiirilistest andmetest y_rep - plotid mudeli poolt ennustatud iseseisvatest valimitest (igaüks sama suur kui empiiriline valim y) Jooniselt on näha, et m3 ennustused on võrreldes m1 ja m2-ga kõige kaugemal tegelikust valimist. 18.1.11 plotime mudeli ennustusi - marginal effects plots teeme ennustused Kõigepealt ennustame ühe keskmise mudeliga, mis ei arvesta mitmetasemelise mudeli madalamte tasemete koefitsientidega. plot(marginal_effects(m2, effects= &quot;Petal.Length&quot;, method = &quot;predict&quot;, probs=c(0.1, 0.9)), points = TRUE, theme = theme_bw()) marginal_effects(m2, effects= &quot;Petal.Length&quot;, method = &quot;predict&quot;, probs=c(0.1, 0.9)) Ennustus on selles mõttes ok, et vaid väike osa punkte jääb sellest välja, aga laiavõitu teine! Nüüd ennustame sama mudeli põhjal igale liigile eraldi. Seega kasutame mudeli madalama taseme koefitsiente. peame andma lisaparameetri re_formula = NULL, mis tagab, et ennustuse tegemisel kasutatakse ka mudeli madalama taseme koefitsiente. conditions &lt;- data.frame(Species =c(&quot;setosa&quot;, &quot;virginica&quot;, &quot;versicolor&quot;)) plot(marginal_effects(m2, effects= &quot;Petal.Length&quot;, method = &quot;predict&quot;, conditions = conditions, probs=c(0.1, 0.9), re_formula = NULL), points = TRUE, theme = theme_bw()) method = “predict” ennustab, millisesse vahemikku peaks mudeli järgi jääma 90% andmepunkte (k.a. uued andmepunktid, mida pole veel valimisse korjatud). Tõesti, valdav enamus valimi punkte on intervallis sees, mis viitab et mudel töötab hästi. Seal, kus on rohkem punkte, on intervall kitsam (mudel on usaldusväärsem). Järgneval pildil on method = “fitted”. Nüüd on enamus punkte väljaspool usaldusintervalle, mis sellel pildil mõõdavad meie usaldust regressioonijoone vastu. conditions &lt;- data.frame(Species =c(&quot;setosa&quot;, &quot;virginica&quot;, &quot;versicolor&quot;)) plot(marginal_effects(m2, effects= &quot;Petal.Length&quot;, method = &quot;fitted&quot;, conditions = conditions, probs=c(0.1, 0.9), re_formula = NULL), points = TRUE, theme = theme_bw()) method = “fitted” annab CI regressioonijoonele. argumendid: method - predict annab veapiirid (95% CI) mudeli ennustustustele andmepunkti tasemel. fitted annab veapiirid mudeli fitile endale (joonele, mis tähistab keskmist või kõige tõenäolisemat y muutuja väärtust igal x-i väärtusel) conditions - andmeraam, kus on kirjas mudeli nendele ennustavatele (x) muutujatele omistatud väärtused, mida ei joonistata x teljele. Kuna meil on selleks mudeli madalama taseme muutuja Species, siis on lisaks vaja määrata argument re_formula = NULL, mis tagab, et ennustuste tegemisel kasutatakse mudeli kõikide tasemete fititud koefitsiente. re_formula = NA annab seevastu keskmise fiti üle kõigi gruppide (irise liikide) probs annab usaldusintervalli piirid. Pane tähele, et argumendid points ja theme kuuluvad plot(), mitte marginal_effects() funktsioonile. tavaline interaktsioonimudel, aga pidevatele muutujatele. m5 &lt;- brm(Sepal.Length~Petal.Length + Sepal.Width + Petal.Length*Sepal.Width, data= iris, prior = prior, family = gaussian, warmup = 1000, iter = 2000, chains = 3, cores = 4, control = list(adapt_delta = 0.95)) write_rds(m5, path = &quot;m5.fit&quot;) m5 &lt;- read_rds(&quot;m5.fit&quot;) Kõigepealt plotime mudeli ennustused, kuidas Sepal Length sõltub Petal Length-ist kolmel erineval Sepal width väärtusel. Ja siis sümmeetriliselt vastupidi. plot(marginal_effects(m5, effects = &quot;Petal.Length:Sepal.Width&quot;), points = T) plot(marginal_effects(m5, effects = &quot;Sepal.Width:Petal.Length&quot;), points = T) Siin lisame enda soovitud Sepal Width väärtused (5 ja 1.2), mis on väljaspool seda, mida loodus pakub. Pane tähele ennustuse laiu CI-sid. conditions &lt;- data.frame(Sepal.Width = c(5, 1.2)) plot(marginal_effects(m5, effects = &quot;Petal.Length&quot;, conditions = conditions, re_formula = NULL), points= TRUE) 18.1.12 Alternatiivne tee: Teeme tabeli nende väärtustega, millele tahame mudeli ennustusi. Tabelis newx on spetsifitseeritud mudeli kõikide X muutujate väärtused! Me ennustame Y väärtusi paljudel meie poolt võrdse vahemaaga ette antud petal length väärtustel, kusjuures me hoiame sepal width väärtuse alati konstantsena tema valimi keskmisel väärtusel ja vaatame ennustusi eraldi kahele liigile kolmest. Liigid on mudeli madala taseme osad, seega kasutame ennustuste tegemisel mudeli kõikide tasemete koefitsiente. newx &lt;- expand.grid(Petal.Length = seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out = 150), Sepal.Width = mean(iris$Sepal.Width), Species = c(&quot;setosa&quot;, &quot;virginica&quot;) ) expand.grid() lõõb tabeli pikaks nii, et kõik võimalikud kombinatsioonid 3st muutujast on täidetud väärtustega. reformula NULL mudeldab eraldi liigid eraldi mudeli madalama taseme (liikide sees) koefitsiente kasutades predict_interval_brms2 &lt;- predict(m2, newdata = newx, re_formula = NULL) %&gt;% cbind(newx,.) head(predict_interval_brms2) #&gt; Petal.Length Sepal.Width Species Estimate Est.Error Q2.5 Q97.5 #&gt; 1 1.00 3.06 setosa 4.49 0.319 3.86 5.13 #&gt; 2 1.04 3.06 setosa 4.51 0.316 3.88 5.14 #&gt; 3 1.08 3.06 setosa 4.56 0.317 3.94 5.19 #&gt; 4 1.12 3.06 setosa 4.59 0.317 3.95 5.20 #&gt; 5 1.16 3.06 setosa 4.61 0.315 4.01 5.21 #&gt; 6 1.20 3.06 setosa 4.64 0.317 4.01 5.25 predict() ennustab uusi petal length väärtusi (Estimate veerg) koos usaldusinetrvalliga neile väärtustele Siin siis eraldi ennustused kahele liigile kolmest, kaasa arvatud petal length väärtusvahemikule, kus selle liigi isendeid valimis ei ole (ja võib-olla ei saagi olla) iris1 &lt;- iris %&gt;% filter(Species != &quot;versicolor&quot;) ggplot(data = predict_interval_brms2, aes(x = Petal.Length, y = Estimate)) + geom_point(data= iris1, aes(Petal.Length, Sepal.Length, color=Species)) + geom_line(aes(color = Species)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = Species), alpha = .1, colour = NA) + scale_color_brewer(palette = &#39;Set1&#39;) + ggthemes::theme_tufte() Ennustav plot - kuidas lähevad kokku mudeli ennustused reaalsete y-i andmepunktidega pr &lt;- predict(m2) %&gt;% cbind(iris) ggplot(pr, aes(Sepal.Length, Estimate, color=Species))+ geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5), alpha=0.5, size=0.2) + geom_abline(intercept = 0, slope = 1, lty = 2) + coord_cartesian( xlim=c( 4, 8 ), ylim=c( 4, 8 ))+ ggthemes::theme_tufte() Igae andmepunktile - kui palju erineb selle residuaal 0-st - st kui hästi ennustab mudel just seda andmepunkti. Ruumi kokkuhoiuks plotime välja ainult irise esimele 50-le andmepunktile. re &lt;- residuals(m2) %&gt;% cbind(iris) re$indeks &lt;- 1:nrow(re) ggplot(re[1:50,], aes(x = Estimate, y = reorder(indeks, Estimate))) + geom_vline(xintercept = 0, lty = 2) + geom_point(size=1) + geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5), color = &quot;red&quot;, size =0.2) + theme(text = element_text(size = 7), axis.title.y = element_blank())+ xlab(&quot;residuals (95 CI)&quot;) Ok, isendid nr 15 ja 37 paistavad olema palju väiksema Sepal Lengthiga kui mudel ennustab. Võib küsida, miks? Nüüd plotime usaldusintervalli mudeli fitile (keskmisele Y väärtusele igal määratud X-i väärtusel), mitte Y- ennustusele andmepunkti kaupa. Selleks on hea fitted() funktsioon. Me ennustame m2 mudelist vastavalt newdata parameetriväärtustele. Kui me newdata argumendi tühjaks jätame, siis võtab fitted() selleks automaatselt algse iris tabeli (ehk valimi väärtused). predict_interval_brms2f &lt;- fitted(m2, newdata = newx, re_formula = NULL) %&gt;% cbind(newx,.) head(predict_interval_brms2f) #&gt; Petal.Length Sepal.Width Species Estimate Est.Error Q2.5 Q97.5 #&gt; 1 1.00 3.06 setosa 4.49 0.0542 4.38 4.59 #&gt; 2 1.04 3.06 setosa 4.52 0.0535 4.41 4.62 #&gt; 3 1.08 3.06 setosa 4.55 0.0529 4.45 4.65 #&gt; 4 1.12 3.06 setosa 4.58 0.0524 4.48 4.68 #&gt; 5 1.16 3.06 setosa 4.61 0.0520 4.51 4.71 #&gt; 6 1.20 3.06 setosa 4.64 0.0518 4.54 4.74 ggplot(data = predict_interval_brms2f, aes(x = Petal.Length, y = Estimate, color = Species)) + geom_point(data= iris1, aes(Petal.Length, Sepal.Length, color=Species)) + geom_line() + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = Species), alpha = .1, colour = NA) + scale_x_continuous(breaks = 0:10) + theme(panel.grid.minor = element_blank()) + scale_color_brewer(palette = &#39;Set1&#39;)+ ggthemes::theme_tufte() mudeli genereeritud andmed ja valimiandmed mõõtmisobjekti (subjekti e taimeisendi) kaupa. See on sisuliselt posterior predictive plot (vt eespool). predict_subjects_brms &lt;- predict(m2) %&gt;% cbind(iris, .) #ennustame andmeid igale taimele vastavate parameetriväärtustega #ja paneme ennustused kokku algse irise tabeliga predict() arvutab mudeli põhjal uusi Y muutuja andmepunkte. Võib kasutada ka väljamõeldud andmete pealt Y väärtuste ennustamiseks (selleks tuleb anda ette andmeraam kõigi X-muutujate väärtustega, mille pealt tahetakse ennustusi). tugevalt värvitud punktid on ennustused ja läbipastvad punktid on valimiandmed ggplot(data = predict_subjects_brms, aes(Petal.Length, Estimate, color = Species)) + geom_point(aes(Petal.Length, Estimate), alpha = .8)+ geom_point(data = iris, aes(Petal.Length, Sepal.Length), alpha = .3)+ ggthemes::theme_tufte() Alternatiiv - ansambliennustus Kuna meil on 2 mudelit, m2 ja m3, mis on pea võrdselt eelistatud, siis genreerime ennustused mõlemast (mudelite ansamblist) proportsionaalselt nende waic skooridega. See ennustus kajastab meie mudeldamistööd tervikuna, mitte ühte “parimat” mudelit ja seega võib loota, et annab paremini edasi meie mudeldamises peituvat ebakindlust. pp_a &lt;- pp_average(m2, m3, weights = &quot;waic&quot;, method = &quot;predict&quot;) %&gt;% as_tibble() %&gt;% bind_cols(iris) ggplot(data = pp_a, aes(Petal.Length, Estimate, color = Species)) + geom_point(aes(Petal.Length, Estimate), alpha = .8)+ geom_point(data = iris, aes(Petal.Length, Sepal.Length), alpha = .3)+ ggthemes::theme_tufte() 18.2 mudeli eelduste kontroll Pareto k otsib nn mõjukaid (influential) andmepunkte. loo_m2 &lt;- loo(m2) plot(loo_m2) Kui paljud andmepunktid on kahtlaselt mõjukad? loo::pareto_k_table(loo_m2) #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). 18.2.1 plotime residuaalid resid() annab residuaalid vektorina. Kõigepealt plotime residuaalid fititud (keskmiste) Y väärtuste vastu. resid &lt;- resid(m2, type = &quot;pearson&quot;)[, &quot;Estimate&quot;] fit &lt;- fitted(m2)[, &quot;Estimate&quot;] ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + geom_hline(yintercept=0, lty=2) Residuals vs fitted plot testib lineaarsuse eeldust - kui .resid punktid jaotuvad ühtlaselt nulli ümber, siis mudel püüab kinni kogu süstemaatilise varieeruvuse teie andmetest ja see mis üle jääb on juhuslik varieeruvus. vaatame diagnostilist plotti autokorrelatsioonist residuaalide vahel. plot(acf(resid)) Residuaalide autokorrelatsioonid on madalad - seega kõik paistab OK ja andmepunktide sõltumatus on tagatud. siin on residuaalide histogramm ggplot(data = NULL, aes(resid)) + geom_density(fill=&quot;lightgrey&quot;) + geom_vline(xintercept = median(resid), linetype =&quot;dashed&quot;)+ theme_classic() Residuaalid on sümmeetrilise jaotusega ja meedian residuaal on peaaegu null. See on kõik hea. Ja lõpuks plotime residuaalid kõigi x-muutujate vastu: Kõigepealt ühendame resid vektori irise tabeliga, et oleks mugavam plottida, seejärel tekitame uue veeru st_resid e studentiseeritud residuaalid, mis on sd ühikutes. residuaalid standardhälbe ühikutes (nn Studentiseeritud residuaalid) saab ja ka tuleks plottida kõigi x-muutujate suhtes. iris2 &lt;- iris %&gt;% cbind(resid) %&gt;% mutate(st_resid= resid/sd(resid)) ggplot(iris2, aes(Petal.Length, st_resid, color=Species))+ geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + ggthemes::theme_tufte() Pole paha, mudel ennustab hästi, aga mõne punkti jaoks on ennustus 2 sd kaugusel. ggplot(iris2, aes(Sepal.Width, st_resid, color=Species))+ geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + ggthemes::theme_tufte() ggplot(iris2, aes(Species, st_resid)) + geom_boxplot() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_jitter(width = 0.1, size=0.4)+ ggthemes::theme_tufte() "],
["brms-mudelid.html", "19 Brms mudelid 19.1 Robustne lineaarne regressioon 19.2 imputatsioon otse brms-is 19.3 Monotoonilised efektid 19.4 brms mudelite süntaks", " 19 Brms mudelid 19.1 Robustne lineaarne regressioon Kasutame dnorm likelihoodi asemel studenti t jaotust. Selle jaotuse õlad on reguleeritavalt kõrgemad ja nende alla mahuvad paremini outlierid. Õlgade kõrgust reguleerib parameeter nu (1 - Inf), mille väiksemad väärtused (alla 10) annavad laiad õlad ja kaitse outlierite vastu. Me anname nu-le gamma priori. Sellel prioril on omakorda 2 parameetrit, shape ja scale Kui fikseerime shape = 4 ja scale = 1, siis saame kitsa priori, mis eelistab nu väärtusi, mis soosivad laiu õlgu ja robustset regressiooni. x = seq(from = 0, to = 20, by = .1) y = dgamma(x, shape= 4, scale = 1) plot(y~x) get_prior(Sepal.Length~Petal.Length, data= iris, family = &quot;student&quot;) #&gt; prior class coef group resp dpar nlpar bound #&gt; 1 b #&gt; 2 b Petal.Length #&gt; 3 student_t(3, 6, 10) Intercept #&gt; 4 gamma(2, 0.1) nu #&gt; 5 student_t(3, 0, 10) sigma prior &lt;- c(prior(gamma(4,1), class=&quot;nu&quot;)) robust_m1 on studenti likelihoodiga, mille õlad määratakse adaptiivselt andmete poolt. robust_m2-s anname õlgade laiuse ette ja robust_m3 on mitte-robustne kontroll tavalise normaalse likelihoodiga. robust_m1 &lt;- brm(Sepal.Length~Petal.Length, data= iris, family = &quot;student&quot;, prior = prior, chains = 3, cores = 3) robust_m2 &lt;- brm( data= iris, family = student, bf(Sepal.Length~Petal.Length, nu = 4), prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(5, 0, 5), class = sigma)), chains = 3, cores = 3) robust_m3 &lt;- brm(Sepal.Length~Petal.Length, data= iris, family = &quot;gaussian&quot;, chains = 3, cores = 3) write_rds(robust_m1, path = &quot;robust_m1.fit&quot;) write_rds(robust_m2, path = &quot;robust_m2.fit&quot;) write_rds(robust_m3, path = &quot;robust_m3.fit&quot;) b_estimates &lt;- bind_rows(tidy(robust_m1), tidy(robust_m2), tidy(robust_m3), .id= &quot;model_nr&quot;) b1 &lt;- b_estimates %&gt;% filter(str_detect(term, &quot;b_P&quot;) ) %&gt;% ggplot(aes(model_nr, estimate))+ geom_pointrange(aes(ymin=lower, ymax=upper))+ coord_flip()+ labs(x=&quot;Model nr&quot;, title =&quot;slopes&quot;) b2 &lt;- b_estimates %&gt;% filter(str_detect(term, &quot;b_I&quot;) ) %&gt;% ggplot(aes(model_nr, estimate))+ geom_pointrange(aes(ymin=lower, ymax=upper))+ coord_flip()+ labs(x=NULL, title =&quot;intercepts&quot;) gridExtra::grid.arrange(b1, b2, nrow = 1) Kolme mudeli interceptid ja sloped on sisuliselt võrdsed ja sama täpsusega hinnatud. Seega ei tee robustne mudel vähemal halba, kui meil on enam-vähem normaalsed andmed. Proovime ka robuststet versiooni 2 grupi võrdlusest (vastab t testile, kus kahe grupi sd-d hinnatakse eraldi) ir1 &lt;- iris %&gt;% filter(Species != &quot;versicolor&quot;) ir1$Species &lt;- as.factor(ir1$Species) %&gt;% fct_drop() get_prior(bf(Sepal.Length~Species, sigma ~ Species), data = ir1, family = &quot;student&quot;) #&gt; prior class coef group resp dpar nlpar #&gt; 1 b #&gt; 2 b Speciesvirginica #&gt; 3 student_t(3, 6, 10) Intercept #&gt; 4 gamma(2, 0.1) nu #&gt; 5 b sigma #&gt; 6 b Speciesvirginica sigma #&gt; 7 student_t(3, 0, 10) Intercept sigma #&gt; bound #&gt; 1 #&gt; 2 #&gt; 3 #&gt; 4 #&gt; 5 #&gt; 6 #&gt; 7 prior &lt;- c(prior(gamma(4,1), class= &quot;nu&quot;), prior(normal(0, 4), class= &quot;b&quot;)) robust_t_test1 &lt;- brm(bf(Sepal.Length~Species, sigma ~ Species), data = ir1, prior = prior, family = &quot;student&quot;) write_rds(robust_t_test1, path = &quot;robust_t_test1.fit&quot;) tidy(robust_t_test1) #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept 5.002 0.0505 4.916 5.086 #&gt; 2 b_sigma_Intercept -1.175 0.1281 -1.383 -0.970 #&gt; 3 b_Speciesvirginica 1.557 0.1032 1.387 1.726 #&gt; 4 b_sigma_Speciesvirginica 0.577 0.1694 0.298 0.846 #&gt; 5 nu 6.113 2.0334 3.337 9.874 #&gt; 6 lp__ -79.991 1.5766 -82.954 -78.063 b_Intercept on hinnang 1. grupi keskväärtusele (algses skaalas) b_Speciesvirginica on hinnag efekti suurusele, ehk 2. grupi erinevusest esimesest grupist (algses skaalas) b_Intercept + b_Speciesvirginica annab 2. grupi keskväärtuse. b_sigma_Intercept on naturaallogaritm 1. grupi sd-st. b_sigma_Speciesvirginica on logaritm 2. grupi (I. virginica) sd erinevusest esimesest grupist (ehk efekti suurus). Seega saab algses skaalas sd-d nii: exp(b_sigma_Intercept) = 1. grupi sd exp(b_sigma_Intercept) + exp(b_sigma_Speciesvirginica) = 2. grupi sd exp(b_sigma_Speciesvirginica) = sd-de erinevus Nii arvutame 2. grupi keskväärtuse posteeriori r_1_df &lt;- posterior_samples(robust_t_test1) mean_2.gr &lt;- r_1_df$b_Intercept + r_1_df$b_Speciesvirginica ggplot(data = NULL) + geom_density(aes(mean_2.gr)) Nii saab tekitada usaldusinetvalle, mis katavad 90% jaotuse alusest kõrgeimast tihedusest (mis ei ole päris sama, mis kvantiilide meetod) rethinking::HPDI(mean_2.gr, prob = 0.9) #&gt; |0.9 0.9| #&gt; 6.4 6.7 Nii saame teada, milline osa (fraktsioon) posteeriorist on väiksem kui 6.4 mean(mean_2.gr &lt; 6.4) #&gt; [1] 0.0372 Asendades eelnevas koodis 6.4 nulliga saame bayesi versiooni ühepoolsest p väärtusest hüpoteesile, et teise grupi keskväärtus on null. Avaldame posteeriori 2. grupi sd-e sd_2.gr &lt;- exp(r_1_df$b_sigma_Intercept) + exp(r_1_df$b_sigma_Speciesvirginica) ggplot(data = NULL) + geom_density(aes(sd_2.gr)) On tavaline, et sd-de posteeriorid ei ole normaaljaotusega (selle kohta vaata lähemalt Statistical Rethinking raamatust). t.test(Sepal.Length~Species, data=ir1) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: Sepal.Length by Species #&gt; t = -20, df = 80, p-value &lt;2e-16 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.79 -1.38 #&gt; sample estimates: #&gt; mean in group setosa mean in group virginica #&gt; 5.01 6.59 Klassikalise t testi efekti suuruse CI on 1.38 … 1.79 robustse t testi oma on 1.39 … 1.73 Simuleerime siis ühe tõsiste outlieritega andmestiku, et vaadata kas meil õnnestub päästa efekt statistilise mitteolulisuse õnnetust saatusest. Meil on a grupis 30 andmepunkti normaaljaotusest mu = 0, sd = 1 ja b grupis 25 andmepunkti normaaljaotusest mu = 1, sd = 1.5, pluss 5 andmepunkti, mis mängivad outliereid. set.seed(123) df1 &lt;- tibble(a=rnorm(30), b= c(rnorm(25, 1, 1.5), 4.3, 5.3, 7, -8.1, -17)) %&gt;% gather() ggplot(df1, aes(value, fill=key)) + geom_histogram(alpha = 0.7, position = &quot;identity&quot;) robust_t_test2 &lt;- brm(bf(value~key, sigma ~ key), data = df1, family = &quot;student&quot;, prior= prior(gamma(4,1), class= &quot;nu&quot;)) write_rds(robust_t_test2, path = &quot;robust_t_test2.fit&quot;) tidy(robust_t_test2) #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept -0.0961 0.188 -0.410 0.2049 #&gt; 2 b_sigma_Intercept -0.2372 0.192 -0.557 0.0689 #&gt; 3 b_keyb 1.4340 0.405 0.776 2.0979 #&gt; 4 b_sigma_keyb 0.6312 0.287 0.164 1.0979 #&gt; 5 nu 2.8096 0.952 1.585 4.6058 #&gt; 6 lp__ -124.5835 1.718 -128.006 -122.5280 t.test(value~key, data=df1) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: value by key #&gt; t = -1, df = 30, p-value = 0.3 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -2.417 0.777 #&gt; sample estimates: #&gt; mean in group a mean in group b #&gt; -0.0471 0.7729 Nüüd kus meil on outlieritega andmed, annab klassikaline t test efekti suurusele CI -2.41 … 0.78 (p = 0.3), aga robustne t test leiab efekti üles - CI 0.78 … 2.10 [tegelik ES oleks 1, outliereid arvestamata]. Kui tavaline t test annab välja kahe grupi keskmised, usaldusintervalli nende erinevusele (ehk ES-le) ja p väärtuse, siis bayesi variant annab välja 2 grupi keskväärtused, 2 grupi varieeruvused andmepunktide tasemel ning kõik efekti suurused ja hüpoteesitestid, millest te suudate unistada. Selle külluse põhjus on, et hinnang iga parameeteri väärtusele tuleb meile posteeriori ehk tõenäosusjaotuse kujul. Kuna iga posteerior on meil arvutis olemas kui arvuline vektor, ja teatavasti saab vektoritega teha aritmeetilisi tehteid, siis saab ka posteerioreid omavahel liita, lahutada, astendada jms. Teoreetiliselt sisaldab posteerior kogu infot, mis meil vastava parameetri väärtuse kohta on. Me ei vaja midagi enamat, et teha kõiki järeldusi, mida me selle parameetri väärtuse kohta üldse teha saame. Seetõttu on bayesi versioon mitte ainult palju paindlikum kui tavaline t test, vaid selle output on ka hästi palju informatiivsem. Igaks juhuks tuletame meelde, et tavaline t test (küll versioonis, kus võrreldavate gruppide varieeruvused on eeldatud olema identsed) on ekvivalentne lineaarse regressiooniga, mille siin fitime vähimruutude meetodiga. (Väheinformatiivsete prioritega bayesi versioon normaaljaotuse likelihoodiga annaks sellega väga sarnase fiti.) lm1 &lt;- lm(value~key, data=df1) tidy(lm1) #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -0.0471 0.554 -0.0850 0.933 #&gt; 2 keyb 0.820 0.784 1.05 0.300 p = 0.2999252 ongi vastava t testi põhiväljund. 19.1.1 puuduvate andmete imputatsioon Regressioonimudelite fittimisel kasutatakse ainult vaatlusi, kus esinevad väärtused kõigis mudelisse pandud muutujates. Seega, kui meil on palju muutujaid, milles igaühes puuduvad juhuslikult mõned väärtused, siis kaotame kokkuvõttes enamuse oma valimist. Aitab puuduvate andmete imputatsioon, mis tegelikult tähendab, et me fitime iga puuduvaid andmeid sisaldava muutuja eraldi regressioonimudelis kõigi teiste muutujate vastu. Eriti vajalik, kui andmed ei puudu juhuslikult! Viskame irise andmestiku kahest tulbast välja 1/4 andmepunkte, aga mitte juhuslikult vaid kõik madalamad väärtused. Selline suunatud tegevus kallutab (ehk suunab kindlas suunas) oluliselt mudeldamise tulemusi iris_na &lt;- iris quantile(iris_na$Petal.Length) #&gt; 0% 25% 50% 75% 100% #&gt; 1.00 1.60 4.35 5.10 6.90 iris_na$Sepal.Length[iris_na$Sepal.Length &lt; 5] &lt;- NA iris_na$Petal.Length[iris_na$Petal.Length &lt; 1.6] &lt;- NA lm(iris$Petal.Length~iris$Sepal.Length) %&gt;% tidy() #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -7.10 0.507 -14.0 6.13e-29 #&gt; 2 iris$Sepal.Length 1.86 0.0859 21.6 1.04e-47 lm(iris_na$Petal.Length~iris_na$Sepal.Length) %&gt;% tidy() #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) -4.19 0.609 -6.89 4.33e-10 #&gt; 2 iris_na$Sepal.Length 1.43 0.0976 14.6 4.53e-27 imputeerime enne mudeli fittimist kasutades multiple imputation meetodit mice paketist. Siin imputeerime iga puuduva väärtuse kasutades kõigi teiste parameetrite väärtusi, ja me teeme seda 5 korda. library(mice) imp &lt;- mice(iris_na, m = 5, print = FALSE) Meil on nüüd 5 imputeeritud andmesetti. Me saadame need kõik brms-i. Siin kasutame mice() tema vaikeväärtustel, kuid mice pakett on tegelikult vägagi rikkalik imputatsioonimasin, mille helpi ja tutoorialeid tuleks kindlasti enne lugeda, kui oma andmeid imputeerima asuda. Lisaks, see raamat on tervenisti pühendatud imputatsioonile: https://stefvanbuuren.name/fimd/ iris_imp1 &lt;- brm_multiple(Petal.Length~Sepal.Length, data = imp) write_rds(iris_imp1, path = &quot;iris_imp1.fit&quot;) Saame tavalise fitiobjekti, kus on 5 alammudeli posterioorid. Kõik juba koos. tidy(iris_imp1)[1:2,] #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept -7.69 0.5543 -8.6 -6.79 #&gt; 2 b_Sepal.Length 1.95 0.0929 1.8 2.10 Tõepoolest, süstemaatiliselt rikutud andmetest on imutatsiooni abil võimalik täitsa head ennustust tagasi saada!!! 19.2 imputatsioon otse brms-is See töötab küll irise peal halvemini kui mice! bform &lt;- bf(Petal.Length | mi() ~ mi(Sepal.Length)) + bf(Sepal.Length | mi() ~ Sepal.Width + Petal.Width + Species + mi(Petal.Length)) + set_rescor(FALSE) iris_imp2 &lt;- brm(bform, data = iris_na) write_rds(iris_imp2, path = &quot;iris_imp2.fit&quot;) bform &lt;- bf(Petal.Length | mi() ~ mi(Sepal.Length)) + bf(Sepal.Length | mi() ~ Species) + set_rescor(FALSE) iris_imp3 &lt;- brm(bform, data = iris_na) write_rds(iris_imp3, path = &quot;iris_imp3.fit&quot;) tidy(iris_imp2) #&gt; term estimate std.error lower upper #&gt; 1 b_PetalLength_Intercept -5.5690 4.92e-01 -6.204 -4.81e+00 #&gt; 2 b_SepalLength_Intercept 2.6008 3.26e-01 2.115 3.12e+00 #&gt; 3 b_SepalLength_Sepal.Width 0.3074 8.55e-02 0.164 4.30e-01 #&gt; 4 b_SepalLength_Petal.Width 0.0679 1.52e-01 -0.130 3.33e-01 #&gt; 5 b_SepalLength_Speciesversicolor -0.2201 1.59e-01 -0.482 1.72e-02 #&gt; 6 b_SepalLength_Speciesvirginica -0.4502 2.32e-01 -0.856 -9.37e-02 #&gt; 7 bsp_PetalLength_miSepal.Length 1.6355 8.26e-02 1.509 1.74e+00 #&gt; 8 bsp_SepalLength_miPetal.Length 0.6130 3.13e-02 0.576 6.63e-01 #&gt; 9 sigma_PetalLength 0.6527 4.04e-02 0.599 7.22e-01 #&gt; 10 sigma_SepalLength 0.3325 1.35e-02 0.309 3.53e-01 #&gt; 11 Ymi_PetalLength[1] 2.5171 3.47e-01 2.006 3.08e+00 #&gt; 12 Ymi_PetalLength[2] -432.9218 1.39e+03 -3058.912 8.54e+02 #&gt; 13 Ymi_PetalLength[3] -996.6434 1.42e+03 -4056.104 1.16e+02 #&gt; 14 Ymi_PetalLength[4] 1510.4882 1.42e+03 -295.164 3.59e+03 #&gt; 15 Ymi_PetalLength[5] 2.2171 3.65e-01 1.658 2.78e+00 #&gt; 16 Ymi_PetalLength[7] 4136.6840 5.31e+03 -1684.596 1.41e+04 #&gt; 17 Ymi_PetalLength[8] 2.1674 3.67e-01 1.673 2.77e+00 #&gt; 18 Ymi_PetalLength[9] 172.4278 8.99e+02 -963.060 2.21e+03 #&gt; 19 Ymi_PetalLength[10] 1157.3398 9.74e+02 -503.254 2.82e+03 #&gt; 20 Ymi_PetalLength[11] 3.2050 3.12e-01 2.689 3.67e+00 #&gt; 21 Ymi_PetalLength[13] 571.3848 1.11e+03 -644.225 2.81e+03 #&gt; 22 Ymi_PetalLength[14] 265.7601 6.34e+02 -440.222 1.47e+03 #&gt; 23 Ymi_PetalLength[15] 3.3153 3.90e-01 2.681 3.93e+00 #&gt; 24 Ymi_PetalLength[16] 3.0418 4.40e-01 2.216 3.69e+00 #&gt; 25 Ymi_PetalLength[17] 3.0307 3.64e-01 2.566 3.74e+00 #&gt; 26 Ymi_PetalLength[18] 2.5185 3.97e-01 1.782 3.12e+00 #&gt; 27 Ymi_PetalLength[20] 2.2725 4.03e-01 1.530 2.86e+00 #&gt; 28 Ymi_PetalLength[22] 2.4953 3.96e-01 1.890 3.13e+00 #&gt; 29 Ymi_PetalLength[23] -153.9356 2.05e+03 -3859.651 2.09e+03 #&gt; 30 Ymi_PetalLength[28] 2.7134 4.40e-01 2.014 3.39e+00 #&gt; 31 Ymi_PetalLength[29] 2.7003 4.45e-01 1.989 3.48e+00 #&gt; 32 Ymi_PetalLength[32] 2.9400 3.63e-01 2.379 3.58e+00 #&gt; 33 Ymi_PetalLength[33] 2.5132 4.26e-01 1.847 3.28e+00 #&gt; 34 Ymi_PetalLength[34] 2.8653 3.28e-01 2.375 3.41e+00 #&gt; 35 Ymi_PetalLength[35] 315.7272 1.31e+03 -1101.428 2.84e+03 #&gt; 36 Ymi_PetalLength[36] 2.5777 3.37e-01 1.976 3.08e+00 #&gt; 37 Ymi_PetalLength[37] 3.2112 3.88e-01 2.574 3.80e+00 #&gt; 38 Ymi_PetalLength[38] -113.1557 5.45e+02 -1429.359 5.01e+02 #&gt; 39 Ymi_PetalLength[39] -122.0110 1.87e+03 -4121.989 2.67e+03 #&gt; 40 Ymi_PetalLength[40] 2.5490 3.25e-01 2.031 3.05e+00 #&gt; 41 Ymi_PetalLength[41] 2.3217 4.26e-01 1.650 3.08e+00 #&gt; 42 Ymi_PetalLength[42] -10.1840 2.22e+02 -311.454 4.23e+02 #&gt; 43 Ymi_PetalLength[43] -847.8219 1.13e+03 -2960.030 6.47e+02 #&gt; 44 Ymi_PetalLength[46] -163.4421 7.83e+02 -1370.487 9.39e+02 #&gt; 45 Ymi_PetalLength[48] 2292.4104 5.66e+03 -4695.643 1.20e+04 #&gt; 46 Ymi_PetalLength[49] 2.5764 4.47e-01 1.862 3.29e+00 #&gt; 47 Ymi_PetalLength[50] 2.4020 3.81e-01 1.687 2.97e+00 #&gt; 48 Ymi_SepalLength[2] -266.8168 8.70e+02 -1892.662 5.63e+02 #&gt; 49 Ymi_SepalLength[3] -639.3507 9.37e+02 -2636.029 7.14e+01 #&gt; 50 Ymi_SepalLength[4] 964.9306 9.08e+02 -167.148 2.28e+03 #&gt; 51 Ymi_SepalLength[7] 2470.0979 3.13e+03 -1050.894 8.24e+03 #&gt; 52 Ymi_SepalLength[9] 97.2306 5.32e+02 -621.286 1.29e+03 #&gt; 53 Ymi_SepalLength[10] 701.1611 5.83e+02 -313.687 1.65e+03 #&gt; 54 Ymi_SepalLength[12] 4.5950 2.95e-01 4.224 5.17e+00 #&gt; 55 Ymi_SepalLength[13] 366.2949 7.13e+02 -394.819 1.83e+03 #&gt; 56 Ymi_SepalLength[14] 156.5420 3.77e+02 -271.828 8.59e+02 #&gt; 57 Ymi_SepalLength[23] -54.1831 1.24e+03 -2232.912 1.37e+03 #&gt; 58 Ymi_SepalLength[25] 4.7323 1.66e-01 4.474 5.05e+00 #&gt; 59 Ymi_SepalLength[30] 4.5205 2.99e-01 4.040 4.95e+00 #&gt; 60 Ymi_SepalLength[31] 4.5043 2.08e-01 4.157 4.85e+00 #&gt; 61 Ymi_SepalLength[35] 193.4708 7.73e+02 -637.046 1.68e+03 #&gt; 62 Ymi_SepalLength[38] -71.5785 3.47e+02 -922.843 3.01e+02 #&gt; 63 Ymi_SepalLength[39] -55.9220 1.18e+03 -2591.647 1.74e+03 #&gt; 64 Ymi_SepalLength[42] -1.2193 1.37e+02 -182.045 2.66e+02 #&gt; 65 Ymi_SepalLength[43] -523.7231 6.99e+02 -1866.247 3.84e+02 #&gt; 66 Ymi_SepalLength[46] -104.9238 4.79e+02 -856.116 5.47e+02 #&gt; 67 Ymi_SepalLength[48] 1271.7510 3.35e+03 -2938.394 6.94e+03 #&gt; 68 Ymi_SepalLength[58] 5.1607 2.74e-01 4.644 5.60e+00 #&gt; 69 Ymi_SepalLength[107] 6.0507 2.04e-01 5.776 6.40e+00 #&gt; 70 lp__ -213.4054 5.31e+00 -222.784 -2.05e+02 tidy(iris_imp3) #&gt; term estimate std.error lower upper #&gt; 1 b_PetalLength_Intercept -4.601 0.5864 -5.560 -3.650 #&gt; 2 b_SepalLength_Intercept 5.168 0.0915 5.014 5.316 #&gt; 3 b_SepalLength_Speciesversicolor 0.781 0.1157 0.590 0.971 #&gt; 4 b_SepalLength_Speciesvirginica 1.449 0.1155 1.259 1.645 #&gt; 5 bsp_PetalLength_miSepal.Length 1.488 0.0945 1.333 1.642 #&gt; 6 sigma_PetalLength 0.705 0.0494 0.630 0.791 #&gt; 7 sigma_SepalLength 0.503 0.0325 0.453 0.559 #&gt; 8 Ymi_PetalLength[1] 2.998 0.7075 1.887 4.187 #&gt; 9 Ymi_PetalLength[2] 3.074 1.0414 1.344 4.806 #&gt; 10 Ymi_PetalLength[3] 3.110 1.0349 1.401 4.837 #&gt; 11 Ymi_PetalLength[4] 3.108 1.0649 1.343 4.882 #&gt; 12 Ymi_PetalLength[5] 2.833 0.6971 1.690 3.987 #&gt; 13 Ymi_PetalLength[7] 3.101 1.0299 1.433 4.832 #&gt; 14 Ymi_PetalLength[8] 2.830 0.7094 1.668 3.986 #&gt; 15 Ymi_PetalLength[9] 3.106 1.0544 1.335 4.809 #&gt; 16 Ymi_PetalLength[10] 3.120 1.0461 1.402 4.882 #&gt; 17 Ymi_PetalLength[11] 3.452 0.6926 2.341 4.582 #&gt; 18 Ymi_PetalLength[13] 3.098 1.0516 1.346 4.774 #&gt; 19 Ymi_PetalLength[14] 3.079 1.0440 1.420 4.796 #&gt; 20 Ymi_PetalLength[15] 4.028 0.7033 2.873 5.193 #&gt; 21 Ymi_PetalLength[16] 3.876 0.7141 2.693 5.081 #&gt; 22 Ymi_PetalLength[17] 3.439 0.7004 2.269 4.585 #&gt; 23 Ymi_PetalLength[18] 2.977 0.7185 1.778 4.152 #&gt; 24 Ymi_PetalLength[20] 2.972 0.7062 1.804 4.141 #&gt; 25 Ymi_PetalLength[22] 2.980 0.7071 1.853 4.116 #&gt; 26 Ymi_PetalLength[23] 3.083 1.0405 1.385 4.814 #&gt; 27 Ymi_PetalLength[28] 3.150 0.7182 1.970 4.319 #&gt; 28 Ymi_PetalLength[29] 3.137 0.7118 1.955 4.322 #&gt; 29 Ymi_PetalLength[32] 3.438 0.7153 2.250 4.594 #&gt; 30 Ymi_PetalLength[33] 3.135 0.7317 1.949 4.335 #&gt; 31 Ymi_PetalLength[34] 3.585 0.7020 2.410 4.743 #&gt; 32 Ymi_PetalLength[35] 3.095 1.0425 1.354 4.801 #&gt; 33 Ymi_PetalLength[36] 2.843 0.7317 1.667 4.049 #&gt; 34 Ymi_PetalLength[37] 3.582 0.7249 2.382 4.772 #&gt; 35 Ymi_PetalLength[38] 3.078 1.0506 1.311 4.763 #&gt; 36 Ymi_PetalLength[39] 3.088 1.0618 1.333 4.794 #&gt; 37 Ymi_PetalLength[40] 2.989 0.7160 1.784 4.172 #&gt; 38 Ymi_PetalLength[41] 2.852 0.7160 1.662 4.031 #&gt; 39 Ymi_PetalLength[42] 3.092 1.0302 1.385 4.780 #&gt; 40 Ymi_PetalLength[43] 3.102 1.0397 1.401 4.792 #&gt; 41 Ymi_PetalLength[46] 3.126 1.0382 1.416 4.844 #&gt; 42 Ymi_PetalLength[48] 3.128 1.0212 1.465 4.821 #&gt; 43 Ymi_PetalLength[49] 3.278 0.7262 2.072 4.452 #&gt; 44 Ymi_PetalLength[50] 2.825 0.7284 1.598 4.012 #&gt; 45 Ymi_SepalLength[2] 5.169 0.5124 4.342 6.016 #&gt; 46 Ymi_SepalLength[3] 5.175 0.5100 4.306 5.985 #&gt; 47 Ymi_SepalLength[4] 5.181 0.5113 4.354 6.024 #&gt; 48 Ymi_SepalLength[7] 5.172 0.4976 4.360 5.989 #&gt; 49 Ymi_SepalLength[9] 5.175 0.5084 4.340 6.002 #&gt; 50 Ymi_SepalLength[10] 5.182 0.5094 4.337 5.988 #&gt; 51 Ymi_SepalLength[12] 4.643 0.3494 4.053 5.208 #&gt; 52 Ymi_SepalLength[13] 5.164 0.5111 4.325 5.984 #&gt; 53 Ymi_SepalLength[14] 5.163 0.5175 4.324 6.014 #&gt; 54 Ymi_SepalLength[23] 5.158 0.5114 4.329 5.989 #&gt; 55 Ymi_SepalLength[25] 4.741 0.3495 4.161 5.311 #&gt; 56 Ymi_SepalLength[30] 4.643 0.3498 4.080 5.223 #&gt; 57 Ymi_SepalLength[31] 4.642 0.3564 4.056 5.223 #&gt; 58 Ymi_SepalLength[35] 5.170 0.5100 4.336 6.009 #&gt; 59 Ymi_SepalLength[38] 5.177 0.5112 4.328 6.007 #&gt; 60 Ymi_SepalLength[39] 5.167 0.5121 4.335 6.009 #&gt; 61 Ymi_SepalLength[42] 5.171 0.5068 4.328 5.982 #&gt; 62 Ymi_SepalLength[43] 5.179 0.5137 4.311 6.018 #&gt; 63 Ymi_SepalLength[46] 5.174 0.5101 4.339 6.007 #&gt; 64 Ymi_SepalLength[48] 5.178 0.4987 4.349 5.982 #&gt; 65 Ymi_SepalLength[58] 5.615 0.3417 5.058 6.168 #&gt; 66 Ymi_SepalLength[107] 6.357 0.3541 5.779 6.930 #&gt; 67 lp__ -281.863 6.6033 -293.525 -271.709 19.2.1 binoomjaotusega mudelid y ∼ Binomial(n,p) where y is some count variable, n is the number of trials, and p is the probability a given trial was a 1, which is sometimes termed a success. When n = 1, then y is a vector of 0s and 1s. Presuming the logit link, models of this type are commonly termed logistic regression. When n &gt; 1, and still presuming the logit link, we might call our model an aggregated logistic regression model, or more generally an aggregated binomial regression model. 19.2.2 logistic regression library(rethinking) data(chimpanzees) d &lt;- chimpanzees head(d) #&gt; actor recipient condition block trial prosoc_left chose_prosoc #&gt; 1 1 NA 0 1 2 0 1 #&gt; 2 1 NA 0 1 4 0 0 #&gt; 3 1 NA 0 1 6 1 0 #&gt; 4 1 NA 0 1 8 0 1 #&gt; 5 1 NA 0 1 10 1 1 #&gt; 6 1 NA 0 1 12 1 1 #&gt; pulled_left #&gt; 1 0 #&gt; 2 1 #&gt; 3 0 #&gt; 4 0 #&gt; 5 1 #&gt; 6 1 intercept only model m_logreg_1 &lt;- brm(data = d, family = binomial, pulled_left ~ 1, prior(normal(0, 10), class = Intercept)) write_rds(m_logreg_1, path= &quot;m_logreg_1.fit&quot;) m_logreg_1 &lt;- read_rds(&quot;m_logreg_1.fit&quot;) tidy(m_logreg_1) #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept 0.323 0.0882 0.181 0.464 #&gt; 2 lp__ -346.669 0.6652 -347.959 -346.194 Tõenäosus, et ahv “pulled left”: inv_logit_scaled(fixef(m_logreg_1)) #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; Intercept 0.58 0.522 0.539 0.621 Nüüd ehtne ennustav logistiline regressioonimudel m_logreg_2 &lt;- brm(data = d, family = binomial, pulled_left ~ 1 + prosoc_left, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b))) write_rds(m_logreg_2, path= &quot;m_logreg_2.fit&quot;) m_logreg_2 &lt;-read_rds(&quot;m_logreg_2.fit&quot;) tidy(m_logreg_2) #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept 0.0456 0.128 -0.165 0.259 #&gt; 2 b_prosoc_left 0.5659 0.187 0.260 0.879 #&gt; 3 lp__ -345.7102 1.014 -347.639 -344.741 The proportional odds = exp(0.5528), which is the ratio of the probability an event happens to the probability it does not happen (the outcome or y variable). If changing the predictor prosoc_left from 0 to 1 increases the log-odds of pulling the left-hand lever by 0.55, then there is a proportional increase of exp(0.55) = 1.73 in the odds of pulling the left-hand lever. This means that the odds increase by 73%. exp(0.55) #&gt; [1] 1.73 the actual change in probability will also depend upon the intercept, α, as well as any other predictor variables. Logistic regression induce interactions among all variables. You can think of these interactions as resulting from both ceiling and floor effects: If the intercept is large enough to guarantee a pull, then increasing the odds by 73% isn’t going to make it any more guaranteed. Suppose α = 4. Then the probability of a pull, ignoring everything else, would be inv_logit_scaled(4) = 0.98. Adding in an increase of 0.55 (the estimate for beta) changes this to: inv_logit_scaled(4 + 0.55) = 0.99. That’s a difference, on the absolute scale, of 1%, despite being an 73% increase in proportional odds. Likewise, if the intercept is very negative, then the probability of a pull is almost zero. An increase in odds of 73% may not be enough to get the probability up from the floor. inv_logit_scaled(0.04562136 + 0.56590225) #&gt; [1] 0.648 Pr of pulling left, if prosoc_left is 1, is 64%. inv_logit_scaled(0.04562136) #&gt; [1] 0.511 if prosoc_left is 0, it is 51%. This meagre difference is reflected in the roc curve. glm.probs &lt;- predict(m_logreg_2, type= &quot;response&quot;) %&gt;% as.data.frame() #&gt; Warning: Using &#39;binomial&#39; families without specifying &#39;trials&#39; on the left- #&gt; hand side of the model formula is deprecated. glm.probs &lt;- glm.probs[,1] glm.pred &lt;- rep(&quot;pulled_right&quot;,504) glm.pred[glm.probs &gt;.5] &lt;- &quot;pulled_left&quot; table(glm.pred, d$pulled_left) #confusion matrix #&gt; #&gt; glm.pred 0 1 #&gt; pulled_left 201 282 #&gt; pulled_right 11 10 library(pROC) roccurve &lt;- roc(d$pulled_left ~ glm.probs) plot(roccurve, legacy.axes = TRUE, cex.axis=0.7, cex.lab= 0.8) Sarnase mudeli saab fittida ka siis, kui n&gt;1 ja meil on igale ahvile countide suhted nr of pull-left/total pulls. Nüüd on meil vaja lisada trials(), kuhu läheb n kas ühe numbrina või muutujana, mis indekseerib sündmuste arvu ehk n-i. Antud juhul on kõikidel ahvidel katsete arv n 18. d_aggr &lt;- d %&gt;% select(-recipient, -block, -trial, -chose_prosoc) %&gt;% group_by(actor, condition, prosoc_left) %&gt;% summarise(x = sum(pulled_left)) m_logreg_3 &lt;- brm(data = d_aggr, family = binomial, x | trials(18) ~ 1 + prosoc_left) Koefitsendid tulevad samad, mis eelmisel mudelil. näide aggregeeritud binoomsetele andmetele library(rethinking) data(UCBadmit) d &lt;- UCBadmit #teeme dummy variable &quot;male&quot;, kodeeritud kui 1 ja 0 d &lt;- d %&gt;% mutate(male = ifelse(applicant.gender == &quot;male&quot;, 1, 0)) head(d) #&gt; dept applicant.gender admit reject applications male #&gt; 1 A male 512 313 825 1 #&gt; 2 A female 89 19 108 0 #&gt; 3 B male 353 207 560 1 #&gt; 4 B female 17 8 25 0 #&gt; 5 C male 120 205 325 1 #&gt; 6 C female 202 391 593 0 m_ucadmit1 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male , prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2) write_rds(m_ucadmit1, path = &quot;m_ucadmit1.fit&quot;) m_ucadmit1 &lt;- read_rds(&quot;m_ucadmit1.fit&quot;) tidy(m_ucadmit1) #&gt; term estimate std.error lower upper #&gt; 1 b_Intercept -0.831 0.0509 -0.918 -0.748 #&gt; 2 b_male 0.612 0.0626 0.512 0.718 #&gt; 3 lp__ -433.695 0.9484 -435.621 -432.776 exp(0.6102733) #&gt; [1] 1.84 mehed saavad suhtelise 84% eelise ülikooli sissesaamisel. inv_logit_scaled(-0.8311908 + 0.6102733) #&gt; [1] 0.445 Meeskandidaadi tõenäosus sisse saada on 44%. inv_logit_scaled(-0.8311908) #&gt; [1] 0.303 Naiskandidaadi tõenäosus sisse saada on 30%. Kui palju erinevad vastuvõtmise tõenäosused (usaldusintervallidega)? post &lt;- posterior_samples(m_ucadmit1) post %&gt;% mutate(p_admit_male = inv_logit_scaled(b_Intercept + b_male), p_admit_female = inv_logit_scaled(b_Intercept), diff_admit = p_admit_male - p_admit_female) %&gt;% summarise(`2.5%` = quantile(diff_admit, probs = .025), `50%` = median(diff_admit), `97.5%` = quantile(diff_admit, probs = .975)) #&gt; 2.5% 50% 97.5% #&gt; 1 0.115 0.142 0.17 Mudeldame otse küsimust, mis on naiste ja meeste erinevus sissesaamisel. intercepti surume nulli, et saada eraldi hinnang igale departmendile m_ucadmit2 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 0 + dept + male, prior(normal(0, 10), class = b), iter = 2500, warmup = 500, cores = 2, chains = 2) write_rds(m_ucadmit2, path = &quot;m_ucadmit2.fit&quot;) m_ucadmit2 &lt;- read_rds(&quot;m_ucadmit2.fit&quot;) tidy(m_ucadmit2) #&gt; term estimate std.error lower upper #&gt; 1 b_deptA 0.684 0.0968 0.523 0.8426 #&gt; 2 b_deptB 0.641 0.1157 0.453 0.8313 #&gt; 3 b_deptC -0.581 0.0743 -0.705 -0.4593 #&gt; 4 b_deptD -0.613 0.0841 -0.752 -0.4776 #&gt; 5 b_deptE -1.058 0.0974 -1.219 -0.9006 #&gt; 6 b_deptF -2.630 0.1556 -2.896 -2.3911 #&gt; 7 b_male -0.102 0.0807 -0.232 0.0368 #&gt; 8 lp__ -70.630 1.8432 -74.018 -68.2572 d &lt;- d %&gt;% mutate(case = factor(1:12)) d_text &lt;- d %&gt;% group_by(dept) %&gt;% summarise(case = mean(as.numeric(case)), admit = mean(admit / applications) + .05) predict(m_ucadmit2) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications, ymax = Q97.5 / applications), shape = 1, alpha = 1/3) + geom_point() + geom_line(aes(group = dept)) + geom_text(data = d_text, aes(y = admit, label = dept)) + labs(y = &quot;Proportion admitted&quot;, title = &quot;Posterior validation check&quot;) Ohhoo, kui vaadata deparmente eraldi, pole mingit kinnitust, et meestel oleks paremad võimalused ülikooli sisse saada. conditions &lt;- data.frame(male = c(0, 1)) marginal_effects(m_ucadmit2, effects=&quot;dept&quot;, conditions = conditions) 19.2.3 y muutujal 3+ kategoorilist väärtust Building a generalized linear model from a multinomial likelihood is complicated, because as the event types multiply, so too do your modeling choices. And there are two different approaches to constructing the likelihoods, as well. The First is based directly on the multinomial likelihood and uses a generalization of the logit link. When more than two types of unordered events are possible, and the probability of each type of event is constant across trials, then the maximum entropy distribution is the multinomial distribution. The conventional and natural link is this context is the multinomial logit. This link function takes a vector of scores, one for each K event types, and computed the probability of a particular type of event K. Estimate the association between person’s family income and which career (there are 3 choiches) he chooses. library(rethinking) N &lt;- 100 set.seed(2078) # simulate family incomes for each individual family_income &lt;- runif(N) # assign a unique coefficient for each type of event b &lt;- (1:-1) career &lt;- rep(NA, N) # empty vector of choices for each individual for (i in 1:N) { score &lt;- 0.5 * (1:3) + b * family_income[i] p &lt;- softmax(score[1], score[2], score[3]) career[i] &lt;- sample(1:3, size = 1, prob = p) } mult_logistic_m1 &lt;- brm(data = list(career = career, family_income = family_income), family = categorical(link = &quot;logit&quot;), career ~ 1 + family_income) write_rds(mult_logistic_m1, path = &quot;mult_logistic_m1.fit&quot;) Parameetreid ei saa otse tõlgendada. Selle asemel on mõistlik töötada mudeli ennustuste tasemel konverteerides parameetrid 3ks tõenäosusteks, et inimene kindlal perekonna sissetuleku tasemel valib karjääri 1, 2 või 3. pred1 &lt;- predict(mult_logistic_m1) %&gt;% as_tibble() pred1$career &lt;- career pred1$income &lt;- family_income pred1_l &lt;- reshape2::melt(pred1, id.vars = 4:5, measure.vars = 1:3) pred1_l &lt;- pred1_l %&gt;% mutate(variable = case_when(variable == &quot;P(Y = 1)&quot; ~ &quot;career 1&quot;, variable == &quot;P(Y = 2)&quot; ~ &quot;career 2&quot;, variable == &quot;P(Y = 3)&quot; ~ &quot;career 3&quot;)) ggplot(pred1_l, aes(income, value)) + geom_point()+ facet_wrap(~variable)+ ylab(&quot;Pr of career choice at a given income&quot;) 19.2.4 zero inflated mudelid Kasulikud siis, kui teil on Y-s rohkem nulle kui võiks arvata. Näiteks, kui proovida hinnata, mitu suitsu päevas tõmmatakse andmestikust, mis sisaldab mittesuitsetajaid. selle mudeli spetsifitseerimiseks pole vaja teha muud, kui õelda brm() argument family = zero_inflated_poisson(), zero_inflated_beta(), _binomial(), _negbinomial(). zinb data: mitu kala turist püüab. zinb &lt;- read_csv(&quot;http://stats.idre.ucla.edu/stat/data/fish.csv&quot;) zinb brm(count ~ persons + child + camper, data = zinb, family = zero_inflated_poisson()) sellel mudelil on parameeter zi ehk zero inflated probability, mille väärtus annab infleeritud 0-de suhte kõikidesse 0-desse, järgmiseks püüame veel lisaks ennustada zi väärtust lähtuvalt laste arvust (lastega pered võiks vähem kalastada) brm(bf(count ~ persons + child + camper, zi ~ child), data = zinb, family = zero_inflated_poisson()) Nüüd on meil parameetrid zi_Intercept ja zi_child 19.2.5 additiivsed distributsioonilised mudelid inkorporeerime splinid multitaseme mudelisse - siis kui me ei tea y ja x-i suhete kuju. simuleerime andmed dat_smooth &lt;- mgcv::gamSim(eg = 6, n = 200, scale = 2, verbose = FALSE) #&gt; Gu &amp; Wahba 4 term additive model head(dat_smooth[, 1:6]) #&gt; y x0 x1 x2 x3 f #&gt; 1 5.00 0.945 0.192 0.5494 0.246 7.69 #&gt; 2 9.08 0.126 0.194 0.0512 0.963 9.04 #&gt; 3 16.63 0.807 0.508 0.1144 0.396 17.35 #&gt; 4 19.70 0.399 0.823 0.9513 0.748 19.09 #&gt; 5 6.84 0.379 0.396 0.8617 0.660 7.34 #&gt; 6 20.27 0.123 0.826 0.1846 0.197 20.15 x0 kuni x3 on prediktorid, fac on faktor veerg, mis indikeerib nested andmestruktuuri. me ennustame y väärtusi x0 ja x1 järgi, ja lisaks laseme residuaalse sd varieeruma x0 smoothing termi ja fac gruppide interceptide järgi. fit_smooth1 &lt;- brm( bf(y ~ s(x1) + s(x2) + (1|fac), sigma ~ s(x0) + (1|fac)), data = dat_smooth, family = gaussian() ) 19.3 Monotoonilised efektid näit prediktor muutuja: suitsetab palju - vähe - väga vähe. me ei eelda, et 3 taset oleks üksteisest sama kaugel - ei modelleeri seda pideva muutujana. fit1 &lt;- brm(y ~ mo(x), data = d) nüüd tekivad meile Simplex parameetrid. Nende prior on kanooniliselt ühe parameetriga (alpha) Dirichlet prior, mis on beta jaotuse multivariaatne üldistus. siin me eeldame, et kõrvuti kategooriate erinevused on samad - e lähevad sama priori alla. Kui me eeldame, et meil on näit 3 järjestikust monotoonilist taset ja, et madalatel väärtustel (1) on monotoonilise muutuja mõju Y-le suurem, siis anname alphale kõrgema väärtuse. Meie kolemses näites on vaja spetsifitseerida vektor 3 alpha-ga. prior &lt;- prior(dirichlet(c(2, 1, 1)), class = &quot;...&quot;, coef = &quot;...&quot;) fit4 &lt;- brm(y ~ mo(x), data = d, prior = prior, sample_prior = TRUE) monotooniliste muutujate interaktsiooni mudeldamine fit5 &lt;- brm(y ~ mo(x)*age, data = d) marginal_effects(fit5, &quot;x:age&quot;) mitmetasemelised monotoonilised mudelid fit6 &lt;- brm(y ~ mo(x)*age + (mo(x) | city), data = d) 19.3.1 multivariaatsed mudelid mitu y muutujat, millel igaühel on oma prediktorid. Eurasian blue tit: predict the tarsus length as well as the back color of chicks. Half of the brood were put into another fosternest, while the other half stayed in the fosternest of their own dam. This allows to separate genetic from environmental factors. Additionally, we have information about the hatchdate and sex of the chicks (the latter being known for 94% of the animals). data(&quot;BTdata&quot;, package = &quot;MCMCglmm&quot;) head(BTdata) fit1 &lt;- brm( cbind(tarsus, back) ~ sex + hatchdate + (1|p|fosternest) + (1|q|dam), data = BTdata, chains = 2, cores = 2 ) add_ic(fit1) &lt;- &quot;loo&quot; summary(fit1) pp_check(fit1, resp = &quot;tarsus&quot;) pp_check(fit1, resp = &quot;back&quot;) The term (1|p|fosternest) indicates a varying intercept over fosternest. By writing |p| in between we indicate that all varying effects of fosternest should be modeled as correlated. This makes sense since we actually have two model parts, one for tarsus and one for back. The indicator p is arbitrary and can be replaced by other symbols that comes into your mind. Similarily, the term (1|q|dam) indicates correlated varying effects of the genetic mother of the chicks. The summary output of multivariate models closely resembles those of univariate models, except that the parameters now have the corresponding response variable as prefix. Within dams, tarsus length and back color seem to be negatively correlated, while within fosternests the opposite is true. This indicates differential effects of genetic and environmental factors on these two characteristics. Further, the small residual correlation rescor(tarsus, back) on the bottom of the output indicates that there is little unmodeled dependency between tarsus length and back color. Although not necessary at this point, we have already computed and stored the LOO information criterion of fit1, which we will use for model comparisons. Me võime anda ette erinevad valemid kummagile y-muutujale nii bf_tarsus &lt;- bf(tarsus ~ sex + (1|p|fosternest) + (1|q|dam)) bf_back &lt;- bf(back ~ hatchdate + (1|p|fosternest) + (1|q|dam)) fit2 &lt;- brm(bf_tarsus + bf_back, data = BTdata, chains = 2, cores = 2) We change our model in various directions at the same time. Remember the slight left skewness of tarsus, which we will now model by using the skew_normal family instead of the gaussian family. Since we do not have a multivariate normal (or student-t) model, estimating residual correlations is no longer possible. We make this explicit using the set_rescor function. We investigate if the relationship of back and hatchdate is really linear as previously assumed by fitting a non-linear spline of hatchdate. On top of it, we model separate residual variances of tarsus for males and femals chicks. bf_tarsus &lt;- bf(tarsus ~ sex + (1|p|fosternest) + (1|q|dam)) + lf(sigma ~ 0 + sex) + skew_normal() bf_back &lt;- bf(back ~ s(hatchdate) + (1|p|fosternest) + (1|q|dam)) + gaussian() fit3 &lt;- brm(bf_tarsus + bf_back + set_rescor(FALSE), data = BTdata) 19.3.2 mittelineaarsed mudelid b &lt;- c(2, 0.75) x &lt;- rnorm(100) y &lt;- rnorm(100, mean = b[1] * exp(b[2] * x)) dat1 &lt;- data.frame(x, y) prior1 &lt;- prior(normal(1, 2), nlpar = &quot;b1&quot;) + prior(normal(0, 2), nlpar = &quot;b2&quot;) fit1 &lt;- brm(bf(y ~ b1 * exp(b2 * x), b1 + b2 ~ 1, nl = TRUE), data = dat1, prior = prior1) Siin on iga mittelineaarne parameeter (b1 ja b2) eraldi modelleeritud ~1 abil. Argument b1 + b2 ~ 1 (lühivorm: b1 ~ 1, b2 ~ 1) ütleb, mis muutujad valemist on parameetrid, mille väärtust tuleb hinnata. Lisaks spetsifitseerib see igale parameetrile lineaarsed prediktorid. Mitte-lineaarsed parameetrid on tegelikult kohahoidjad lineaarsetele prediktor-termidele. Kuna me siin ei ennusta b1 ja b2 lisaparameetrite kaudu, on meil intercept-only mudel kummagile. Priors on population-level parameters (i.e., ‘fixed effects’) are often mandatory to identify a non-linear model. Thus, brms requires the user to explicitely specify these priors. In the present example, we used a normal(1, 2) prior on (the population-level intercept of) b1, while we used a normal(0, 2) prior on (the population-level intercept of) b2. Setting priors is a non-trivial task in all kinds of models, especially in non-linear models, so you should always invest some time to think of appropriate priors. Quite often, you may be forced to change your priors after fitting a non-linear model for the first time, when you observe different MCMC chains converging to different posterior regions. This is a clear sign of an idenfication problem and one solution is to set stronger (i.e., more narrow) priors. 19.4 brms mudelite süntaks üldine vorm: response ~ pterms + (gterms | group) kasuta g1:g2 või g1/g2, kui nii g1 kui g2 on sobilikud grupeerivad faktorid. operaator loob uue grupeeriva faktori, mis kombineerib g1 ja g2 tasemed. / operaator viitab nested struktuurile (kool - koolitüüp) (1 | g1/g2), tähendab tegelikult (1 | g1) + (1 | g1:g2). (1 | g1 + g2) on sama, mis (1 | g1) + (1 | g2). || kasutades (x || g1) ei modelleeri me grupi-taseme korrelatsioone. See on hea, kui mudeli fittimine muidu ei tööta. kuidas mudeldada sama grupeeriva faktori korrelatsioone üle mitme regressioonivõrrandi? Selleks laiendame | operaatori ||, kus on suvaline väärtus. Näiteks kui termid (x1|ID|g1) and (x2|ID|g1) esinevad kuskil samas või erinevates võrrandites, modelleeritakse need kui korreleeritud. alternatiivseid grupeeivaid struktuure saab väljendada nii: (gterms | fun(group)). Hetkel on meil 2 sellist fun-i: gr() annab default käitumise ja mm() annab multi-membership termid. Näiteks brm(y ~ 1 + (1 | mm(s1, s2)) modelleerib seda, kuidas lapsed võivad õppida kahes koolis (s1 ja s2) eri aegadel gr() lisatakse muidu automaatselt, aga seda spetsifitseerides saab kirjutada y ~ x + (1|gr(g1, by = g2)), mis tähendab, et grupeeriva muutja g1 sees lahutatakse veel gruppidesse g2 muutuja tasemete järgi - ja iga g2 grupp modelleeritakse iseseisvalt (ilma shrinkageta) Mittelineaarne mudel: y = b1(1 − exp(−(x/b2)**b3 ) y ja x seos parameetritega b1..b3 Oletame, et tahame kõik parameetrid fittida grupeeriva muutuja g tasemete järgi ja et grupi tasmel efektid oleks omavahel korreleeritud. Lisaks ennustame me b1-e kovariaat z järgi. See kõik läheb järgmisesse võrrandisüsteemi kus, lisaks mitte-lineaarsele võrrandile, igale parameetrile b1…b3 vastab oma lineaarne võrrand: y ~ b1 * (1 - exp(-(x / b2) ^ b3) b1 ~ z + (1|ID|g) b2 ~ (1|ID|g) b3 ~ (1|ID|g) lisaks on mudeli keeles silumistermid s() ehk spline ja t2() ehk bivariate tensor spline, mis tulevad mgcv paketist. Näiteks rentsqm ~ t2(area, yearc) + (1|district) category specific effects cs, monotonic effects mo, noise-free effects me, or Gaussian process terms gp. additional information on the response variable may be specified via response | aterms ~ &lt;predictor terms&gt;. The aterms part may contain multiple terms of the form fun() separated by + each providing special information on the response variable. This allows among others to weight observations, provide known standard errors for meta-analysis, or model censored or truncated data. "],
["bayesi-ja-sagedusliku-statistika-vordlus.html", "A Bayesi ja sagedusliku statistika võrdlus Kaks statistikat: ajaloost ja tõenäosusest Poleemika: kumbki tõenäosus pole päris see, mida üldiselt arvatakse Võrdlev näide: kahe grupi võrdlus Kahe paradigma erinevused Statistiline ennustus kui mitmetasandiline protsess", " A Bayesi ja sagedusliku statistika võrdlus library(tidyverse) library(rethinking) library(ggthemes) library(brms) Kaks statistikat: ajaloost ja tõenäosusest Bayesiaanlik ja sageduslik statistika leiutati üksteise järel Pierre-Simon Laplace poolt, kes arendas välja kõigepealt bayesiaanliku statistika alused ning seejärel sagedusliku statistika omad (ca. 1774 - 1814). Sagedusliku statistika õitsengu põhjusteks 20. sajandil olid arvutuslik lihtsus ning tõenäosuse sagedusliku tõlgenduse sobivus 20 saj esimeses pooles käibinud teadusfilosoofiatega - eeskätt loogilise postivismiga. 1930-1980-ndatel valitses akadeemiliste statistikute seas seisukoht, et Bayesiaanlik statistika on surnud ja maha maetud, ning selle arendamisega tegelesid vaid üksikud inimesed, kes sageli olid füüsikaliste teaduste taustaga (Jeffreys, Jaynes). Alates 1960-e keskpaigast arendati bayesiaanlust USA sõjaväe egiidi all, kuna seal oli piisav juurdepääs arvutivõimsusele, kuid seda tehti paljuski salastatult. Bayesi meetoditega ei olnud võimalik korralikult tsiviilteadust teha enne 1990-ndaid aastaid, mil personaalarvutite levik algatas buumi nende meetodite arendamises. Praegu on maailmas bayesiaanlikku ja sageduslikku statistikat umbes pooleks (vähemalt uute meetodite arendustöö poole pealt). Eestis bayesiaanlik statistika 2017 aasta seisuga peaaegu, et puudub. 1930ndatel kodifitseeris Andrei Kolmogorov tõenäosusteooria aksioomid (3 aksioomi), mis ütlevad lühidalt, et tõenäosused jäävad 0 ja 1 vahele ning, et üksteist välistavate ja hüpoteesiruumi ammendavate hüpoteeside tõenäosused summeeruvad ühele. Selgus, et Bayesi teoreem on lihtsa aritmeetika abil tuletatav Kolmogorovi aksioomidest. Tagantjärele saame öelda, et bayesiaanlik statistika on mitte ainult tõenäosusteooriaga kooskõlas vaid ka, et Bayesi teoreem on parim võimalik viis sellist kooskõla saavutada (see on 1950ndate tarkus - Coxi teoreem). On ka teada, et kui tõenäosused on fikseeritud nulli ja ühega, siis taandub Bayesi teoreem klassikalisele lausearvutuslikule loogikale. See tähendab, et klassikaline loogika on bayesiaanluse erijuht. Seevastu sageduslik statistika püüab saavutada mõistlikke lahendusi arvutuslikult lihtsamate meetoditega, mille hinnaks on formaalse kooskõla puudumine tõenäosusteooriaga. Seega kujutab sageduslik statistika endast kogumit ad hoc meetodeid, mis ei tähenda muidugi, et sellest kasu ei võiks olla. Küll aga tähendab see, et kuigi sageduslike mudeleid on lihtsam arvutada, on neid raskem ehitada ja mõista ning, et sageduslike testide, milliseid on viimase saja aasta jooksul loodud 10 000 ringis, tulemusi on raskem tõlgendada. Kahe statistika põhiline erinevus ei tulene tõenäosusteooria matemaatikast, vaid erinevatest tõenäosuse tõlgendusest. Bayesi tõlgenduses on tõenäosus teadlase usu määr mingi hüpoteesi kehtimisse. Hüpotees võib näiteks olla, et järgmise juulikuu sademete hulk Vilsandil jääb vahemikku 22 kuni 34 mm. Kui Bayesi arvutus annab selle hüpoteesi tõenäosuseks 0.57, siis oleme me selle teadmise najal nõus maksma mitte rohkem kui 57 senti kihlveo eest, mille alusel makstakse juhul, kui see hüpotees tõeseks osutub, välja 1 EUR (ja me saame vähemalt 43 senti kasumit). Sageduslikud teoreetikud usuvad, et selline tõenäosuse tõlgendus on ebateaduslik, kuna see on “subjektiivne”. Nimelt on võimalik, et n teadlast arvutavad korrektselt samade andmete põhjal n erinevat tõenäosust ja usuvad seega samade tõendite põhjal erinevaid asju. Kui nad lähtuvad väga erinevatest taustauskumustest oma hüpoteeside kehtimise kohta, võivad nad lõpuks uskuda väga erinevaid asju. Sellise olukorra analoog poliitikas on elanikkond, kus üks pool kannavad konservatiivseid väärtusi (perekond, rahvusühtsus) ja teine pool liberaalseid (multikultuursus, üldinimlikud väärtused). Seega priorid on erinevad. Niikaua, kui mõlemad pooled saavad oma uudised samast usaldusväärsest allikast (sama tõepära), ei ole demokraatia siiski ohus. Aga siis, kui konservatiivid ja liberaalid hakkavad erinevatest allikatest hankima erinevaid fakte, mis kummagi ideoloogiat kinnitavad, toimub arvamuste polariseerumine ning demokraatia sattub ohtu. Seega, kui te usute, et teie taustateadmised ei tohi mõjutada järeldusi, mis te oma andmete põhjal teete, siis te ei ole bayesiaan. Siinkohal pakub alternatiivi tõenäosuse sageduslik tõlgendus. Sageduslik tõenäosus on defineeritud kui teatud tüüpi andmete esinemise pikaajaline suhteline sagedus. Näiteks, kui me viskame münti palju kordi, siis peaks kullide (või kirjade) suhteline sagedus meile andma selle mündi tõenäosuse langeda kiri üleval. Selline tõenäosus on omistatav ainult sellistele sündmustele, mille esinemisel on sagedus. Kuna teaduslik teooria ei ole selline sündmus, ei ole sageduslikus statistikas võimalik rääkida ka hüpoteesi kehtimise tõenäosusest. Sageduslik lahendus on selle asemel, et rääkida meie hüpoteesi tõenäosusest meie andmete korral, rääkida andmete, mis sarnanevad meie andmetega, esinemise tõenäosusest null-hüpoteesi (mis ei ole meie hüpotees) kehtimise korral. Seega omistatakse sagedus ehk tõenäosus andmetele, mitte hüpoteesile. Poleemika: kumbki tõenäosus pole päris see, mida üldiselt arvatakse Bayesi tõenäosus ei anna tegelikult seda tõenäosusnumbrit, mida me reaalselt peaksime kihlveokontoris kasutama. Ta annab numbri, millest me lähtuksime juhul, kui me usuksime, et selle numbri arvutamisel kasutatud statistilised mudelid kirjeldavad täpselt maailma. Paraku, kuna mudeldamine on oma olemuselt kompromiss mudeli lihtsuse ja ennustusvõime vahel, ei ole meil põhjust sellist asja uskuda. Seega ei peaks me bayesi tõenäosusi otse maailma üle kandma, vähemasti mitte automaatselt. Bayes ei ütle meile, mida me reaalselt usume. Ta ei ütle, mida me peaksime uskuma. Ta ütleb, mida me peaksime uskuma tingimuslikult. Sageduslik tõenäosus on hoopis teine asi. Seda on võimalik vaadelda kahel viisil: imaginaarsete andmete esinemissagedus nullhüpoteesi all; reaalsete sündmuste esinemise sagedus. Teise vaate kohaselt on sageduslik tõenäosus päriselt olemas. See on samasugune füüsikaline nähtus nagu näiteks auto kiirus, mõõdetuna liiklusmiilitsa poolt. Kui kaks politseinikku mõõdavad sama auto kiirust ja 1. saab tulemuseks 81 km/h ning 2. saab 83 km/h, siis meie parim ennustus auto kiiruse kohta on 82 km/h. Kui aga 1. mõõtmistulemus on 80 km/h ja teine 120 km/h, siis meie parim hinnang ei ole 100 km/h. Enne sellise hinnangu andmist peame tegema lisatööd ja otsustama, kumb miilits oma mõõtmise kihva keeras. Ja me ei otsusta seda mitte oodatavast trahvist lähtuvalt, vaid neutraalseid objektiivseid asjaolusid vaagides. Seda sellepärast, et autol on päriselt kiirus olemas ja meil on hea põhjus, miks me tahame seda piisava täpsusega teada. Sagedusliku statistiku mõõteriist on statistiline mudel ja mõõtmistulemus on tõenäosus, mis jääb 0 ja 1 vahele. Õpikunäidetes on sündmusteks, mille esinemise sagedust tõenäosuse abil mõõdetakse, enamasti täringuvisked, ehk katsesüsteemi reaalne füüsikaline funktsioneerimine. Pane tähele, et need on inimtekkelised sündmused (loodus ei viska täringuid). Teaduses on sündmused, millele tõenäosusi omistatakse, samuti inimtekkelised: selleks sündmuseks on teadlase otsus H0 ümberlükkamise kohta, mille tegemisel ta lähtub p (või q) väärtusest ja usaldusnivoost. Siin vastab auto kiirusele tüüp 1 vigade tegemise sagedus. See sagedus on inimtekkeline, aga sellest hoolimata päriselt olemas ja objektiivselt mõõdetav. Kui 2 teadlast mõõdavad seda paraleelselt ja saavad piisavalt erineva tulemuse (näiteks väga erineva FDR-i), võib olla kindel, et vähemalt üks neist eksib, ning peaks olema võimalik ausalt otsustada, kumb. Sageduslikku tõenäosust on võimalik mõõta siis, kui sündmused, mille sagedust mõõdetakse (ümber lükatud null-hüpoteesid) on üksteisest sõltumatud. Tavapärane sageduslik statistika annab mitte lihtsalt valesid, vaid absurdselt valesid mõõtmistulemusi alati, kui mõõdetavad sündmused sõltuvad tugevalt üksteisest (teades ühe sündmuse esinemise fakti, saab suure tõenäosusega ennustada teise esinemist). Näiteks, me mõõdame mass-spektroskoopiaga 2000 valgu tasemed katse-kontroll süsteemis ja lükkame neist kahest tuhandest 30 H0-i ümber, kui statistiliselt olulised. Me teeme seda lähtuvalt FDR (false discovery rate) kriteeriumist, mis tähendab, et me oleme mõõtnud sagedust, millega meie poolt ümber lükatud H0-d on tegelikult tõesed. Nüüd me avastame, et pooled ümber lükatud H0-d tähistavad valke, mis kõik kuuluvad samasse reguloni. Sellest teeme igati mõistliku järelduse, et meie katsetingimusel on see regulon inaktiveeritud. Paraku, see tähendab ühtlasi, et meie FDR on valesti mõõdetud, kusjuures see ülehindab väga tugevalt FDR-i reguloni kuuluvate valkude osas ja ilmselt alahindab FDR-i reguloni mittekuuluvate valkude osas. Seega, me oleme asjatult analüüsist välja jätnud teised selle reguloni valgud, mille q väärtus valele poole usaldusnivood jättis; ja samal ajal kulutame asjatult oma teadlaseajusid selleks, et välja mõelda seletusi, miks üks või teine reguloni mittekuuluv valk meie katses siiski oluline on. Me oleme politseinäite juures tagasi, aga seekord teame, et politsei ei saa enda käsutuses oleva aparatuuriga piisavalt täpselt kiirust mõõta, et trahvid kohtus püsima jääksid. Bayesiaanile ei ole see näide probleem. Ta inkorporeerib informatsiooni regulonide kohta oma mudelisse ja juhul kui regulonid on valkude tasemete muutuste seisukohast olulised, ei juhtu midagi muud, kui et tema mudeli võime ennustada tegelikke muutusi valkude tasemetes paraneb oluliselt. Me teame (avaldamata andmed), et kui bayesi mudeli struktuuri inkorporeerida info valkude kuuluvusest operonidesse, siis mudeli ennustusvõime kasvab dramaatiliselt. See on loogiline, sest sama operoni valke toodetakse enamasti samalt mRNAlt ja mRNA tase määrab oluliselt valgu taseme. Aga see tähendab ka, et suure tõenäosusega on FDR-i mõõtmine igas seda tüüpi katses ebatäpne (kuigi me ei tea, millisel määral), sest sageduslikud mudelid ei talu sõltuvaid sündmusi (milleks operonidesse koondunud valgud ilmselt on). Võrdlev näide: kahe grupi võrdlus Järgnevalt toome näite, kuidas bayesiaan ja sageduslik statistik lahendavad sama ülesande. Meil on 2 gruppi, katse ja kontroll, millest kummagis 30 mõõtmist ja me soovime teada, kui palju katsetingimus mõjutab mõõtmistulemust. Meie andmed on normaaljaotusega ja andmepunktid, mida me analüüsime, on efektisuurused (katse1 - kontroll1 = es1 jne). Bayesiaan Statistiline küsimus on Bayesiaanil ja sageduslikul statistikul sama: kas ja kui palju erinevad kahe grupi keskväärtused? Bayesiaan alustab sellest, et ehitab kaks mudelit: andmete tõepäramudel ja taustateadmiste mudel ehk prior. Kui andmed on normaaljaotusega, siis on ka tõepäramudel normaaljaotus. Alustame sellest, et fitime oma valimiandmed (üksikud efekti suurused) normaaljaotuse mudelisse. Joonis A.1: Paariviisiline katse - kontroll disain. Katset on korratud 30 korda. X-teljel on efektisuurused (ES). 30 üksikut efektisuurust on näidatud punktidena. Must joon näitab keskmist efektisuurust. Andmed on mudeldatud normaaljaotusena. See ei ole veel tõepäramudel, sest me tahame hinnangut ES keskväärtuse kõige tõenäolisemale väärtusele, ja lisaks veel hinnangut ebakindlusele selle punkt-hinnangu ümber (usalduslpiire). Seega tuleb eelmine jaotus kitsamaks tõmmata, et ta kajastaks meie teadmisi ES-ide keskväärtuste, mitte individuaalsete ES-de, kohta. Uue jaotusmudeli sd = eelmise jaotuse sd/sqrt(30). Joonis A.2: See jaotus iseloomustab keskmise ES paiknemist puhtalt meie andmete põhjal. Täpsemalt, selle joonise põhjal võib arvutada, milline on meie valimi keskväärtuse kohtamise tõenäosus igal võimalikul tõelisel ES-i väärtusel. Kõige tõenäolisemad on andmed siis, kui tegelik ES = andmete keskväärtusega (seda kohta näitab must joon). Kui me jagame musta joone pikkuse punase kurvi all läbi katkendjoone pikkusega sama kurvi all, saame teada, mitu korda on meie andmed tõenäolisemad siis, kui tegelik ES = mean(valimi ES), võrreldes olukorraga, kus tegelik ES = 0. Loomulikult võime sama näitaja arvutada ükskõik millise hüpoteeside paari kohta (näiteks, andmed on miljon korda tõenäolisemad hüpoteesi ES = 0.02 all kui hüpoteesi ES = -1 all; mis aga ei tähenda, et andmed oleksid väga tõenäolised kummagi võrreldud hüpoteesi all). Aga see ei ole veel Bayes. Lisame andmemudelile taustateadmiste mudeli. Sellega tühistame me väga olulise eelduse, mis ripub veskikivina sagedusliku statistika kaelas. Nimelt, et valimi andmed peavad olema esinduslikud populatsiooni suhtes. Me võime olla üsna kindlad, et väikeste valimite korral see eeldus ei kehti ja sellega seoses ei tööta ka sageduslik statistika viisil, milleks R.A. Fisher selle kunagi lõi. Taustateadmiste mudeli peamine, kuigi mitte ainus, roll on mõjutada meie hinnangut õiges suunas vähendades halbade andmete võimet meile kahju teha. Kui sul on väike valim, siis sinu andmed vajavad sellist kantseldamist. Olgu meie taustateadmise mudel normaaljaotus keskväärtusega 0 ja standardhälbega 1. Joonis A.3: Taustateadmiste mudel ehk prior on normaaljaotus (must joon), mille ülesanne on veidi vähendada ekstreemsete valimite kahjulikku mõju. Taustateadmiste mudel on sageli normaaljaotus. Kui meil on palju taustateadmisi, siis on see jaotus kõrge ja kitsas, kui meil on vähe taustateadmisi, siis on see madal ja lai. Mida teha, kui sa ei taha, et taustateadmiste mudel sinu posteeriori kuju mõjutab? Sellisel juhul kasutatakse nõrgalt informatiivseid prioreid, mis tähendab, et priori jaotus on palju laiem kui tõepäramudeli laius. Miks mitte kasutada mitte-informatiivseid tasaseid prioreid? Põhjused on arvutuslikud, seega tehnilist laadi. Igal juhul järgmise sammuna korrutab bayesiaan selle jaotuse andmejaotusega, saades tulemuseks kolmanda normaaljaotuse, mille ta seejärel normaliseerib nii, et jaotuse alune pindala = 1. See kolmas jaotus on posterioorne tõenäosusjaotus, mis sisaldab kogu infot, millest saab arvutada kõige tõenäolisema katseefekti suuruse koos ebakindluse määraga selle ümber (mida rohkem andmeid, seda väiksem ebakindlus) ja tõenäosused, et tegelik katseefekt jääb ükskõik milllisesse meid huvitavasse vahemikku. Nüüd ei ole siis muud kui bayesi mudel läbi arvutada. dfa &lt;- data.frame(a) m99 &lt;- map2stan( alist( a ~ dnorm(mean = mu, sd = sigma), mu ~ dnorm(0, 1), sigma ~ dcauchy(0, 1)), data = dfa) Joonis A.4: Triplot. Bayesi väljund on posterioorne tõenäosusjaotus (roheline). Nagu näha, ei ole selle jaotuse tipp täpselt samas kohas kui andmejaotuse tipp ehk keskväärtus. Prior tõmbab seda veidi nulli suunas. Lisaks on posteerior veidi kitsam kui andmemudel, mis tähendab, et hinnang ES-le tuleb väiksema ebakindluse määraga. Posteerior sisaldab endas kogu infot, mis meil ES-i tõelise väärtuse kohta on. Siit saame arvutada: parima hinnangu ES-i punktväärtusele, usaldusintervalli, ehk millisest ES-ide vahemikust loodame leida tõelise ES-i näit 90% tõenäosusega, iga mõeldava ES-i väärtuste vahemiku kohta tõenäosuse, millega tõeline ES jääb sellesse vahemikku. saame ES-i põhjal arvutada mõne muu statistiku, näiteks ES1 = log(ES), kasutades selleks ES-i posterioorset jaotust. Sel viisil kanname oma ES-i hinnangus peituva ebakindluse üle ES1-le, millele saame samuti rakendada punkte 1-3 (sest ES1 on posterioorne jaotus). uute andmete lisandumisel saame kasutada ES-i posteeriorit uue priorina ja arvutada uue täiendatud posteeriori. Põhimõtteliselt võime seda teha pärast iga üksiku andmepunkti lisandumist. See avab ka head võimalused metaanalüüsiks. lisaks saame oma algsest mudelist ka posteeriori andmepunkti tasemel varieeruvusele (pole näidatud). Seda kasutame uute andmete simuleerimiseks (meie näites üksikud ES-d). Sageduslik statistik Sageduslik lähenemine sisaldab ainult ühte mudelit, mida võrreldakse valimi andmetega. Sageduslik statistik alustab selles lihtsas näites täpselt samamoodi nagu bayesiaan, tekitades eelmisega identse andmemudeli, mis on keskendatud valimi keskväärtusele A.2. Seejärel nihutab ta oma andmemudelit niipalju, et normaaljaotuse tipp ei ole enam valimi keskväärtuse kohal vaid hoopis 0-efekti kohal. Jaotuse laius nihutamisel ei muutu. Joonis A.5: Nullhüpotees (must kõver) ja tõepärafunktsioon (punane kõver). Seda nullile tsentreeritud mudelit kutsutakse null-hüpoteesiks (H0). Nüüd võrdleb ta oma valimi keskväärtust (must joon) H0 jaotusega. Kui valimi keskväärtuse kohal on H0 jaotus kõrge, siis on andmete tõenäosus H0 kehtimise korral suur. Ja vastupidi, kui valimi keskväärtuse kohal on H0 madal, siis on andmete esinemise tõenäosus H0 all madal. Seda tõenäosust kutsutakse p väärtuseks. Mida väiksem on p, seda vähem tõenäolised on teie andmed juhul, kui H0 on tõene ja katseefekt võrdub nulliga. P on defineeritud kui “teie andmete või 0-st veel kaugemal asuvate andmete esinemise pikaajaline suhteline sagedus tingimusel, et H0 kehtib”. Tulemuste tõlgendamine Kui sageduslik statistik kirjutab, et tema “efekti suurus on statistiliselt oluline 0.05 olulisusnivool”, siis ta ütleb sellega, et tema poolt arvutatud p &lt; 0.05. Selle väite korrektne tõlgendus on, et juhul kui statistik pika aja jooksul võtab omaks “statistiliselt olulistena” kõik tulemused, millega kaasnev p &lt; 0.05 ja lükkab tagasi kõik tulemused, mille p &gt; 0.05, siis sooritab ta 5% sagedusega tüüp 1 vigu. See tähendab, et igast sajast tõesest H0-st, mida ta testib, võtab ta keskeltläbi 5 vastu, kui statistiliselt olulised. Sageduslik statistika on parim viis tüüp 1 vigade sageduse pikaajaliseks fikseerimiseks. Paraku ei tea me ühegi üksiku testi kohta ette, kas see testib kehtivat või mittekehtivat H0-i, mis teeb raskeks katseseeriate ühekaupa tõlgendamise. Tuletame meelde, et sageduslikus statistikas ei saa rääkida H0 kehtimise tõenäosusest vaid peab rääkima andmete tõenäosusest (ehk andmete esinemise sagedusest) tingimusel, et H0 kehtib. Kas ühte p väärtust saab tõlgendada kui hinnangut tõendusmaterjali hulgale, mida teie valim pakub H0 vastu? Selle üle on vaieldud juba üle 80 aasta, kuid tundub, et ainus viis seda kas või umbkaudu teha on bayesiaanlik. Igal juhul, p väärtust, mis on defineeritud pikaajalise sagedusena, on raske rakendada üksiksündmusele. Bayesiaanliku p väärtuste tõlgendamiskalkulaatori leiate aadressilt http://www.graphpad.com/quickcalcs/interpretPValue1/. Kujutle mass spektroskoopia katset, kus mõõdame 2000 valgu tasemeid katse-kontroll skeemis ja katset korratakse n korda. Sageduslik statistik kasutab adjusteeritud p väärtusi või q väärtusi, et tõmmata piir, millest ühele poole jäävad statistiliselt olulised ES-d ja teisele poole mitteolulised null-efektid. Edasi tõlgendab ta mitteolulisi efekte kui ebaolulisi ja diskuteerib vaid “olulisi” efekte. Paraku, p väärtuste arvutamine ja adjusteerimine saab toimuda mitmel erineval moel ja usalduspiiri panekule just 95-le protsendile, mitte näiteks 89% või 99.2%-le, pole ühtegi ratsionaalset põhjendust. Seega tõmbab ta sisuliselt juhuslikus kohas joone läbi efektide, misjärel ignoreerib kõiki sellest joonest valele poole jäänud efekte. Meetod, mis väga hästi töötab pikaajalises kvaliteedikontrollis, ei ole kahjuks kuigi mõistlik katse tulemuste ükshaaval tõlgendamises. Mis juhtub, kui oleme kavalad ja proovime mitmeid erinevaid p väärtustega töötamise meetodeid, et valida välja see usalduspiir, millest õigele poole jäävaid andmeid on teaduslikult kõige parem tõlgendada? Ehkki ükshaaval võisid kõik meie poolt läbi arvutatud meetodid olla lubatud (ja isegi võrdselt head), ei fikseeri p nüüd enam tüüp 1 vigade sagedust. See tähendab, et p on kaotanud definitsioonijärgse tähenduse ja te oleksite võinud olulisuspiiri sama hästi tõmmata tunde järgi. Tüüpiline tulemuse kirjeldus artiklis: sageduslik: the effect is statistically significant (p &lt; 0.01). bayesiaanlik: the most likely effect size is x (90% CI = x-low, x-high) and the probability that the true effect is &lt; 0 is z percent. 90% CI — credible interval — tähendab, et me oleme 90% kindlad, et tegelik efekti suurus asub vahemikus x-low … x-high. Kahe paradigma erinevused sageduslikus statistikas võrdub punkt-hinnang tegelikule efekti suurusele valimi keskmise ES-ga. Bayesi statistikas see sageli nii ei ole, sest taustateadmiste mudel mõjutab seda hinnangut. Paljud mudelid püüavad ekstreemseid valimeid taustateadmiste abil veidi mõistlikus suunas nihutada, niiviisi vähendades ülepaisutatud efektide avaldamise ohtu. sageduslik statistika töötab tänu sellele, et uurija võtab vastu pluss-miinus otsuseid: iga H0 kas lükatakse ümber või jäetakse kehtima. Seevastu bayesiaan mõtleb halli varjundites: sissetulevad andmed kas suurendavad või vähendavad hüpoteeside tõenäosusi (mis jäävad aga alati &gt; 0 ja &lt; 1). p väärtused kontrollivad tüüp 1 vigade sagedust ainult siis, kui katse disaini ja hilisema tulemuste analüüsi detailid on enne katse sooritamist jäigalt fikseeritud (või eelnevalt on täpselt paika pandud lubatud variatsioonid katse- ja analüüsi protokollis). Eelkõige tähendab see, et valimi suurus ja kasutatavad statistilinsed testid peavad olema eelnevalt fikseeritud. Tüüpiliselt saame p väärtuse arvutada vaid üks kord ja kui p = 0.051, siis oleme sunnitud H0 paika jätma ning efekti deklareerimisest loobuma. Me ei saa lihtsalt katset juurde teha, et vaadata, mis juhtub. Bayesiaan seevastu võib oma posterioorse tõenäosuse arvutada kasvõi pärast iga katsepunkti kogumist ning katse peatada kohe (või alles siis), kui ta leiab, et tema posterioorne jaotus on piisavalt kitsas, et teaduslikku huvi pakkuda. Sageduslikus statistikas sõltub tulemus sellest, kas hüpotees, mida testitakse oli defineeritud enne andmete kogumist või mitte. Selle pärast tuleks seal testitavad hüpoteesid eel-registreerida. Bayesi statistikas pole aga vahet, kas hüpotees on formuleeritud enne või pärast andmete nägemist – seal loevad tõepärafunktiooni loomisel ainult olemasolevad andmed (mitte andmed, mis oleksid võinud olla aga ei ole). sagedusliku statistika pluss-miinus iseloom tingib selle, et kui tegelik efekti suurus on liiga väike, et sattuda õigele poole olulisusnivood, siis annavad statistiliselt olulisi tulemusi ülepaisutatud efektid, mida tekib tänu valimiveale. Nii saab süstemaatiliselt kallutatud teaduse. Bayesi statistikas seda probleemi ei esine, kuna otsused ei ole pluss-miinus tüüpi. bayesi statistika ei fikseeri tüüp 1 vigade sagedust. See-eest võitleb see nn valehäirete vastu, milleks kaasajal kasutatakse enim hierarhilisi shrinkage mudeleid. See on bayesi vaste sageduslikus statistikas kasutatavatele multiple testingu korrektsioonidele. Kui sageduslik statistik võitleb valehäiretega p väärtusi adjusteerides ja selle läbi olulisusnivood nihutades, siis bayesiaan kasutab shrinkage mudelit, et parandada hinnanguid üksikute efektide keskväärtustele ja nende sd-le, kasutades paindlikult kogu andmesetis leiduvat infot. A.0.1 Sageduslik ja teaduslik hüpoteesitestimine. Teaduslik lähenemine tõendusmaterjalile on sarnane kohtuliku uurimisega: me kogume tõendusmaterjali senikaua, kuni oleme veendunud, et saame selle põhjal eelistada ühte hüpoteesi kõikide teiste arvelt. Seega, kui faktid meile piisavat survet avaldavad, võtame vastu pluss-miinus otsuse, et kohtualune süüdi või teaduslik hüpotees õigeks mõista. See otsus on meie tegevuse eesmärk ja meie tegevus oli suunatud selle eesmärgi täitmisele. Sageduslik statistika võtab samuti vastu dihhotoomseid otsuseid, aga hoopis teisel moel. Seal ei ole otsus eesmärk, vaid vahend. Kui me lükkame tagasi teatud null hüpoteesid, aga mitte teised, saame me sellisel viisil fikseerida pikaajalise 1. tüüpi vigade sageduse. Me võtame vastu otsuseid, eesmärgiga tagada katsesüsteemi pikaajaline kvaliteet. Need otsused ei eelda, et me usuksime, et mõni konkreetne null hüpotees on tõene või väär ning matemaatilised protseduurid, mille alusel me neid otsuseid langetame, ei püüa määrata individuaalse null hüpoteesi tõelähedust või tõenäosust, et see H0 ei kehti. Seega ei tähenda fakt, et me lükkasime ümber konkreetse nullhüpoteesi seda, et me usume, et see null hüpotees on väär. H0-i ümberlükkamisel põhineva statistikaga kaasnevad järelmid, millest ehk olulisim on vajadus fikseerida andmete kogumise ja analüüsi meetodid enne, kui andmed on kogutud. See on nii tehnilistel põhjustel, mis on seotud puhtalt meie sooviga fikseerida 1. tüüpi vigade sagedus. Sellise veasageduste fikseerimise hind on, et andmeanalüüsi valikud ei saa sõltuda tegelikest andmetest (nende kvaliteedist, jaotusest jms), mida analüüsitakse. Siinkohal tuleb eraldi rõhutada, et tegemist ei ole üldise teadusliku meetodi omadusega. Teaduses (ja bayesi statistikas) on mitte ainult täiesti normaalne vaid lausa vajalik vaadata andmeid kriitilise pilguga ja kujundada oma analüüs vastavalt andmete kvaliteedile. Ning kui andmed ei paku piisavalt tõendusmaterjali, et me saaksime otsustada oma hüpoteesi kasuks või kahjuks, siis on igati mõistlik andmeid juurde korjata senikaua, kuni oleme veendunud ühte või teistpidi. Oluline erinevus sagedusliku ja bayesi statistika vahel on, et kui sageduslik meetod fikseerib pikaajalise veasageduse aga ei arvuta üksikute hüpoteeside tõenäosust, siis bayesi meetod vastupidi arvutab üksikute hüpoteeside tõenäosused, aga ei fikseeri pikaajalisi veasagedusi. Kui meid ikkagi huvitavad veasagedused ja statistiline võimsus, saab neid ka bayesiaanlikult leida, arvutades oma mudeleid simuleeritud andmetega. Statistiline ennustus kui mitmetasandiline protsess Me võime vaadelda ennustavat statistikat mitmetasemelise protsessina, kus alumisel tasemel on punkthinnang parameetri väärtusele, selle peal oleval tasemel on hinnang ebakindlusele selle punkthinnangu ümber, ning 3. tasemel on omakorda hinnang ebakindlusele 2. taseme hinnangu ümber. Ja nii edasi lõpmatusse. Bayes erineb klassikalisest statistikast selle poolest, et kui Bayes ehitab 2. taseme hinnangu tõepära ja priori põhjal, siis klassikaline statistika kasutab selleks pelgalt tõepära (konverteerituna null hüpoteesiks). See on tähtis, kuna tõepära modelleerib ainult seda osa juhuslikust varieeruvusest punkthinnangu ümber, mida kutsutakse valimiveaks. Prior on võimeline arvesse võtma ka teise osa juhuslikust varieeruvusest punktväärtuse ümber, mida võime kutsuda andmete esinduslikuseks. Juhuslik varieeruvus tähendab siin, et viga ehk erinevus tegelikust väärtusest on jaotunud sümmeetriliselt tegeliku väärtuse ümber. Andmete esinduslikus on määr, millega meie andmete jaotus sarnaneb populatsiooni jaotusele, kust need andmed on korjatud. Kui esinduslikus kehtib konkreetselt meie andmete kohta, siis valimiviga on modelleeritud funktsioonina, mis kirjeldab kõikvõimalike hüpoteetiliste andmete kohtamise tõenäosust igal mõeldaval parameetriväärtusel. Paraku, kuna valimivea mudel fititakse andmete peal, siis eeldab see meie konkreetsete andmete esinduslikkust. Kuna klassikalises statistikas ei ole formaalset priori mudelit, ei hinda klassikalised usaldusintervallid (2. tase) ebakindlust punktväärtuse ümber. Seda teevad bayesiaanlikud kredibiilsusintervallid, aga ainult siis, kui priorite koostamisse on tõsiselt suhtutud. I Punkthinnang – enamasti aritmeetiline keskmine –- modelleerib andmejaotuse tüüpilist elementi. Eeldus: me teame, milline on andmete jaotus. II tõepärafunktsioon hindab ebakindlust punkthinnangu ümber. Modelleerib valimiviga, mis on seda suurem, mida vähem on teil andmeid. Eeldus 1: andmed on esinduslikud (andmejaotus = populatsiooni jaotus) Eeldus 2: mudel kirjeldab andmeid genereerivat mehhanismi (siit tulevad sageli lisaeeldused, nagu populatsiooni normaaljaotus, lineaarsus, sõltumatud sündmused valimi koostamisel, vigade sõltumatus, homoskedastilisus jms) III prior kohendab tõepärafunktsiooni hinnangut Modelleerib (1) andmete esinduslikkust, mis on seda väiksem, mida väiksem on valim; ja (2) süstemaatilist viga. Eeldus: meil on andemtest sõltumatuid teadmisi populatsiooni jaotuse kohta Valimiviga ja andmete esinduslikus on erinevad ja üksteisest sõltumatud pseudo-protsessid, ehkki mõlemad on juhuslikud protsessid, mille tõenäosus muutub proportsionaalselt andmete hulga kasvuga (täpsemalt sqrt(N)-ga). Kui valim on piisavalt suur, siis võime olla piisavalt kindlad, et andmed on esinduslikud ning klassikalise statistika hinnangud ebakindlusele punktväärtuse ümber muutuvad selle võrra usutavamaks. Samas, sedamõõda kui valimi suurus kasvab, muutub tõepärafunktsioon üha kitsamaks, mis tõstab omakorda tõenäosust, et tegelik parameetri väärtus jääb tõepärafunktsiooni kõrgema osa alt välja tingituna süstemmatilisest veast, mille suurus ei sõltu valimi suurusest. Seega töötab klassikaline statistika parimini keskmiselt suurte valimite (ja keskmiselt suure andmete varieeruvuse) korral. A.0.2 Ajaloolist juttu: normaaljaotus, Bayes ja sageduslik statistika (Anders Hald; A History of Parametric Statistical Inference from Bernoulli to Fisher, 1713-1935, Springer 2000) Laplace sõnastas 1774. aastal statistiku tööpõllu järgmiselt: kirjeldamaks andmeid (vigade jaotust) tõenäosusfunktsioonina leia selline matemaatiline mudel, millel oleks lõplik arv parameetreid. Seejärel leia algoritm, mille kasutamine minimeeriks vead meie hinnagutele nende parameetrite väärtuste kohta. Seega oli eesmärk konverteerida algoritmiliselt andmete (mõõtmisvigade) jaotus posterioorseks tõenäosusjaotuseks, mille pealt saaks omakorda arvutada usaldusintervallid meie hinnagu täpsusele. Laplace, kes tegeles palju astronoomilste mõõtmiste analüüsiga, lootis näidata, et mõõtmisandmete aritmeetiline keskmine on parim viis arvutada sellise posterioorse jaotuse kõige tõenäolisemat väärtust (lokatsiooniparameetrit). Parim viis selles mõttes, et teoreetiliselt parima lokalisatsiooniparameetri ümber on võimalik arvutada kitsaimad veapiirid. Aritmeetilise keskmise selle pärast, et selle kasutamine oli laialt levinud, tundus intuitiivselt mõistlik ning oli arvutuslikult lihtne. Laplace probleemi lahendas Gauss ca. 1809, võttes kasutusele nii uue tõenäosusjaotuse – normaaljaotuse – kui ka uue lokatsiooniparameetri arvutusmeetodi – vähimruutude meetodi. Tema küsitud küsimus oli: millist jaotust ja hindamismeetodit oleks vaja kasutada, et lokatsiooniparameeter tuleks just aritmeetiline keskmine? Rõhutades normaaljaotuse tähtsust, leidis Laplace 1812. aastal, et kõikidest sümmetrilistest andmejaotustest viib ainult normaaljaotus olukorrani, kus aritmeetiline keskmine kattub posterioorse jaotuse tipuga. Kuna Laplace ei teadnud midagi tegelike veajaotuste kohta astronoomilistel mõõtmistel, oli tal raskusi andmejaotuse spetsifitseerimisega, mille pealt Bayesi teoreemiga posterioorne hinnanguvigade jaotus arvutada. Mõõtmisvigu tavatseti mudeldada nelinurksete, kolmnurksete või kvadraat-jaotustega ja polnud ühtki teaduslikku põhjust eelistada üht jaotust teistele! See oli üks põhjuseid, miks Laplace hakkas arendama sageduslikku statistikat ja kasutas alates 1811 üha vähem Bayesi teoreemi. (Alles 1818 näitas Bessel empiiriliselt, et astronoomilised mõõtmised on normaaljaotusega.) Teine põhjus hüljata Bayesi statistika oli seotud tolle aja tehniliste raskustega priorite mudeldamisel, mistõttu Laplace oli sunnitud kasutama tasaseid prioreid, mis omistasid igale sündmusele/hüpoteesile võrdse eeltõenäosuse – ja oma ilmses absurdsuses tegid elu lihtsaks tema kriitikutele. Sageduslik statistika kasutab oma alusena keskset piirteoreemi (Laplace 1810, 1812), mille kohaselt on paljude andmevalimite aritmeetilised keskmised normaaljaotusega, ja seda hoolimata andmete tegelikust jaotusest (eeldusel, et valimid on piisavalt suured; vt 6. ptk). Seega, senikaua kui me ei modelleeri mitte ühe valimi empiirilist andemte jaotust (mida vajab Bayesi teoreem), vaid hoopis paljude virtuaalsete valimite pealt arvutatud keskväärtuste jaotust, ei pea me teadma, milline on andmete tegelik jaotus. Sellelt pinnalt ongi välja töötatud nullhüpoteesi testimine, millel põhineb suur osa 20. sajandi statistikast. 1908 näitas Edgeworth, et suurte valimite ja mitteinformatiivsete priorite korral annavad mõlemad meetodid (Bayes ja sageduslik) sama hinnangu parameetriväärtusele ja numbriliselt sama usaldusintervalli vahemiku. Sagedusliku statistika põhiprintsiibid: Fisher 1922 Statistilisete meetodite eesmärk on andmete redutseerimine. Selleks on vaja vaadelda andmeid juhuvalimina hüpoteetilisest lõpmata suurest populatsioonist, mille jaotust saab kirjeldada suhteliselt väheste parameetritega mudeli abil. (Valimit iseloomustab “statistik”, populatsiooni aga “parameeter”) Statistik puutub kokku 3 sorti probleemidega: Spetsifikatsiooni probleemid kerkivad esile seoses populatsiooni jaotusmudeli spetsifitseerimisega. Estimatsiooni probleemid kerkivad esile seoses algoritmidega, mille abil määratakse hüpoteetilise populatsioonimudeli parameetrite väärtused. Jaotuse probleemid kerkivad üles seoses valimite põhjal arvutatud statistikute jaotuste matemaatilise kirjeldamisega. Heal estimatsioonil on omakorda 3 kriteeriumit: Konsistentsus — statistik on kosnsistentne siis kui arvutatuna lõpmata suurest valimist, mis võrdub populatsiooniga, tuleb selle statistiku väärtus täpselt õige. Efektiivsus — statistiku efektiivsus on suhe (protsent) selle sisemisest täpsusest võrrelduna teoreetiliselt efektiivseima statistikuga. Efektiivsus väljendab, kui suurt osa kogu kättesaadavast informatsioonist meie statistik kasutab. Efektiivsuse kriteerium: statistik, arvutatuna suurest valimist, annab vähima võimaliku standardhälbega normaaljaotuse. Piisavus (sufficiency) — kriteerium: statistik on piisav kui ükski teine statistik, mida saab samast valimist arvutada, ei lisa informatsiooni hinnatava parameetri väärtuse kohta. Fisheri poolt juurutatud sageduslik terminoloogia: parameeter, statistik, tõepära, dispersioon (variance; ANOVA), konsistentsus, efektiivsus, piisavus, informatsioon, null hüpotees, statistiline olulisus, p väärtus, olulisuse test, protsendipunkt, randomiseerimine, interaktsioon, faktoriaalne disain. Tänu kesksele piirteoreemile on sageduslik statistika arvutuslikult palju lihtsam kui Bayesi statistika (arvutused saab teha paberi ja pliiatsiga), aga kuna me ei saa enam rääkida tegelike empiiriliste andmete jaotusest, vaid peame selle asemel kasutama lõpmata hulka virtuaalseid valimeid, ei saa me enam Bayesi teoreemi abil tõenäosusi pöörata (konverteerida meie andmete tõenäosus parameetriväärtuse x kehtimise korral, parameetriväärtuse x kehtimise tõenäosuseks meie andmete korral). Veelgi enam, me ei saa isegi rääkida meie andmete tõenäosusest parameetriväärtuse x kehtimise korral, vaid oleme sunnitud oma keelt ja meelt murdes rääkima “meie andmete või neist ekstreemsemate andmete pikajalisest suhtelisest sagedusest nullhüpoteesi (aga kahjuks mitte ühegi teise parameetriväärtuse) kehtimise korral” (Fisher ca. 1920). Nagu näha, on siin arvutuslikul lihtsusel kõrge kontseptuaalne hind. Bayesi ja sagedusliku statistika võrdlust vt lisa 1. "],
["sonastik.html", "B Sõnastik", " B Sõnastik Statistiline populatsioon (statistical population) – objektide kogum, millele soovime teha statistilist üldistust. Näiteks hinnata keskmist ravimi mõju patsiendipopulatsioonis. Või alkoholi dehüdrogenaasi keskmist Kcat-i. Valim (sample) – need objektid (patsiendid, ensüümiprepid), mida me reaalselt mõõdame. Juhuvalim (random sample) – valim, mille liikmed on populatsioonist valitud juhuslikult ja iseseisvalt. See tähendab, et kõigil populatsiooni liikmetel (kõikidel patsientidel või kõikidel võimalikel ensüümipreparatsioonidel) on võrdne võimalus sattuda valimisse JA, et valimisse juba sattunud liikme(te) põhjal ei ole võimalik ennustada järgmisena valimisse sattuvat liiget. Juhuvalim muudab lihtsamaks normaaljaotuse mudeli kasutamise bayesiaanlikes arvutustes, aga ta ei ole seal selleks absoluutselt vajalik. Seevastu pea kogu sageduslik statistika põhineb juhuvalimitel. Esinduslik valim (representative sample) – Valim on esinduslik, kui ta peegeldab hästi statistilist populatsiooni. Ka juhuvalim ei pruugi olla esinduslik (juhuslikult). valimiviga (sampling error, sampling effect) - määr, millega juhuvalimi põhjal arvutatud statistiku väärtus (näit keskväärtus) erineb populatsiooni parameetri väärtusest. valimiviga kutsutakse sageli ka juhuslikuks müraks. kallutatus e süstemaatiline viga (bias) - see osa statistiku väärtuse erinevusest katsetingimuse ja kontrolltingimuse vahel, mis on põhjustatud millegi muu poolt, kui deklareeritud katse-interventsioon. Statistik (statistic) – midagi, mis on täpselt arvutatud valimi põhjal (näiteks pikkuste keskmine) Parameeter (parameter) – teadmata suurus populatsiooni tasemel, mille täpset väärtust me saame umbkaudu ennustada, aga mitte kunagi täpselt teada. Näiteks mudeli intercept, populatsiooni keskmine pikkus. Efekti suurus (effect size) - siin võrdub katsegrupi keskmine – kontrollgrupi keskmine. Leidub ka teistsuguseid es mõõte, millest levinuim on coheni d. standardhälve mad variatsiooni koefitsient Statistiline mudel (statistical model) – matemaatiline formaliseering, mis koosneb 2st osast: deterministlik protsessi-mudel pluss juhuslik vea/varieeruvuse-mudel. Protsessi-mudeli näiteks kujutle, et mõõdad mitme inimese pikkust (x muutuja) ja kaalu (y muutuja). Sirge võrrandiga y = a + bx (kaal = a + b * pikkus) saab anda determinismliku lineaarse ennustuse kaalu kohta: kui x (pikkus) muutub ühe ühiku (cm) võrra, siis muutub y (kaal) väärtus keskmiselt b ühiku (kg) võrra. Seevastu varieeruvuse-mudel on tõenäosusjaotus (näit normaaljaotus). Selle abil modelleeritakse y-suunalist andmete varieeruvust igal x väärtusel (näiteks, milline on 182 cm pikkuste inimeste oodatav kaalujaotus). Mudel on seega tõenäosuslik: me saame näiteks küsida: millise tõenäosusega kaalub 182 cm pikkune inimene üle 100 kilo. Mida laiem on varieeruvuse mudeli y-i suunaline jaotus igal x-i väärtusel, seda kehvemini ennustab mudel, millist y väärtust võime konkreetselt oodata mingi x-i väärtuse korral. Lineaarsete mudelite eesmärk ei ole siiski mitte niivõrd uute andmete ennustamine (seda teevad paremini keerulised mudelid), vaid mudeli struktuurist lähtuvalt põhjuslike hüpoteeside püstitamine/kontrollimine (kas inimese pikkus võiks otseselt reguleerida/kontrollida tema kaalu?). Kuna selline viis teadust teha töötab üksnes lihtsate mudelite korral, on enamkasutatud statistilised mudelid taotluslikult lihtsustavad ja ei pretendeeri tõelähedusele. tõepära (likelihood) prior e eeljaotus posteerior e järeljaotus (posterior) Tehniline replikatsioon (technical replication) – sama proovi (patsienti, ensüümipreparatsiooni, hiire pesakonna liiget) mõõdetakse mitu korda. Mõõdab tehnilist varieeruvust ehk mõõtmisviga. Seda püüame kontrollida parandades mõõtmisaparatuuri või protokolle. Bioloogiline replikatsioon (biological replication) – erinevaid patsiente, ensüümipreppe, erinevate hiirepesakondade liikmeid mõõdetakse, igaüht üks kord. Eesmärk on mõõta bioloogilist varieeruvust, mis tuleneb mõõteobjektide reaalsetest erinevustest: iga patsient ja iga ensüümimolekul on erinev kõigist teistest omasugustest. Bioloogiline varieeruvus on teaduslikult huvitav ja seda saab visualiseerida algandmete tasemel (mitte keskväärtuse tasemel) näiteks histogrammina. Teaduslikke järeldusi tehakse bioloogiliste replikaatide põhjal. Tehnilised replikaadid seevastu kalibreerivad mõõtesüsteemi täpsust. Kui te uurite soolekepikest E. coli, ei saa te teha formaalset järeldust kõigi bakterite kohta. Samamoodi, kui te uurite vaid ühe hiirepesakonna/puuri liikmeid, ei saa te teha järeldusi kõikide hiirte kohta. Kui teie katseskeem sisaldab nii tehnilisi kui bioloogilisi replikaate on lihtsaim viis neid andmeid analüüsida kõigepealt keskmistada üle tehniliste replikaatide ning seejärel kasutada saadud keskmisi edasistes arvutustes üle bioloogiliste replikaatide (näiteks arvutada nende pealt uue keskmise, standardhälve ja/või usaldusintervalli). Selline kahe-etapiline arvutuskäik ei ole siiski optimaalne. Optimaalne, kuid keerukam, on panna mõlemat tüüpi andmed ühte hierarhilisse mudelisse. Tõenäosuse (P) reeglid on ühised kogu statistikale: P jääb 0 ja 1 vahele; P(A) = 1 tähendab, et sündmus A toimub kindlasti. kui sündmused A ja B on üksteist välistavad, siis tõenäosus, et toimub sündmus A või sündmus B on nende kahe sündmuse tõenäosuste summa — P(A v B) = P(A) + P(B). Kui A ja B ei ole üksteist välistavad, siis P(A v B) = P(A) + P(B) – P(A &amp; B). kui A ja B on üksteisest sõltumatud (A toimumise järgi ei saa ennustada B toimumist ja vastupidi) siis tõenäosus, et toimuvad mõlemad sündmused on nende sündmuste tõenäosuste korrutis –– P(A &amp; B) = P(A) x P(B). Kui B on loogiliselt A alamosa, siis P(B) &lt; P(A) P(A | B) –– tinglik tõenäosus. Sündmuse A tõenäosus, juhul kui peaks toimuma sündmus B. P(vihm | pilves ilm) ei ole sama, mis P(pilves ilm | vihm). Juhul kui P(B)&gt;0, siis P(A | B) = P(A &amp; B)/P(B) ehk P(A | B) = P(A) x P(B | A)/P(B) –– Bayesi teoreem. Kuigi kõik statistikud lähtuvad tõenäosustega töötamisel täpselt samadest matemaatilistest reeglitest, tõlgendavad erinevad koolkonnad saadud numbreid erinevalt. Kaks põhilist koolkonda on sageduslikud statistikud ja Bayesiaanid. Tõenäosus, Bayesi tõlgendus (Bayesian probability) – usu määr mingisse hüpoteesi. Näiteks 62% tõenäosus (et populatsiooni keskmine pikkus &lt; 180 cm) tähendab, et sa oled ratsionaalse olendina nõus kulutama mitte rohkem kui 62 senti kihlveo peale, mis võidu korral toob sulle sisse 1 EUR (ja 38 senti kasumit). Bayesi tõenäosus omistatakse statistilisele hüpoteesile (näiteks, et ravimiefekti suurus jääb vahemikku a kuni b), tingimusel, et sul on täpselt need andmed, mis sul on; ehk P(hüpotees | andmed). Tõenäosus, sageduslik tõlgendus (Frequentist probability) – pikaajaline sündmuste suhteline sagedus. Näiteks 6-te sagedus paljudel täringuvisetel. Sageduslik tõenäosus on teatud tüüpi andmete sagedus, tingimusel et nullhüpotees (H0) kehtib; ehk P(andmed | H0). Nullhüpotees ütleb enamasti, et uuritava parameetri (näiteks ravimiefekti suurus) väärtus on null. Seega, kui P on väike, ei ole seda tüüpi andmed kooskõlas arvamusega, et parameetri väärtus on null (mis aga ei tähenda automaatselt, et sa peaksid uskuma, et parameetri väärtus ei ole null). library(tidyverse) library(brms) library(bayesplot) library(broom) library(rethinking) schools &lt;- read_csv( &quot;data/schools.csv&quot;) schools &lt;- schools %&gt;% drop_na() %&gt;% mutate_at(vars(sex, school), as.factor) head(schools) #&gt; # A tibble: 6 x 5 #&gt; school student sex score1 score2 #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 20920 27 1 39 76.8 #&gt; 2 20920 31 1 36 87.9 #&gt; 3 20920 42 0 16 44.4 #&gt; 4 20920 101 1 49 89.8 #&gt; 5 20920 113 0 25 17.5 #&gt; 6 22520 1 1 48 84.2 (0 - poiss, 1 - tüdruk) skimr::skim(schools) #&gt; Skim summary statistics #&gt; n obs: 1523 #&gt; n variables: 5 #&gt; #&gt; ── Variable type:factor ─────────────────────────────────────────────────── #&gt; variable missing complete n n_unique #&gt; school 0 1523 1523 73 #&gt; sex 0 1523 1523 2 #&gt; top_counts ordered #&gt; 681: 83, 684: 65, 688: 61, 225: 56 FALSE #&gt; 1: 899, 0: 624, NA: 0 FALSE #&gt; #&gt; ── Variable type:integer ────────────────────────────────────────────────── #&gt; variable missing complete n mean sd p0 p25 p50 p75 p100 #&gt; student 0 1523 1523 1016.45 1836.14 1 62.5 129 445 5516 #&gt; hist #&gt; ▇▁▁▁▁▁▁▂ #&gt; #&gt; ── Variable type:numeric ────────────────────────────────────────────────── #&gt; variable missing complete n mean sd p0 p25 p50 p75 p100 #&gt; score1 0 1523 1523 46.5 13.48 0.6 38 46 56 90 #&gt; score2 0 1523 1523 73.38 16.44 9.25 62.9 75.9 86.1 100 #&gt; hist #&gt; ▁▁▃▇▇▅▁▁ #&gt; ▁▁▁▂▅▇▇▆ Excersize: how do these models differ? score1~ sex lm(score1~sex, data = schools) %&gt;% tidy() #&gt; # A tibble: 2 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 48.5 0.536 90.5 0 #&gt; 2 sex1 -3.34 0.697 -4.80 0.00000178 score1~ score2 score1~score2 + sex score1~score2*sex score1~score2:sex lm(score1~score2:sex, data = schools) %&gt;% tidy() #&gt; # A tibble: 3 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 16.1 1.37 11.7 2.72e- 30 #&gt; 2 score2:sex0 0.460 0.0197 23.4 2.62e-103 #&gt; 3 score2:sex1 0.386 0.0180 21.4 5.80e- 89 score1~sex*school lm(score1~school:sex, data = schools) %&gt;% tidy() #&gt; # A tibble: 143 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 60.8 3.70 16.4 1.95e-55 #&gt; 2 school20920:sex0 -40.3 8.68 -4.64 3.85e- 6 #&gt; 3 school22520:sex0 -26.4 4.46 -5.93 3.92e- 9 #&gt; 4 school22710:sex0 -23.9 5.86 -4.09 4.57e- 5 #&gt; 5 school22738:sex0 -5.98 6.20 -0.965 3.35e- 1 #&gt; 6 school22908:sex0 0.222 11.7 0.0190 9.85e- 1 #&gt; # ... with 137 more rows score1~sex + (1 | school) score1~sex + (sex | school) score1~score2*sex on sama, mis score1~score2 + sex + score2:sex lm_m1 &lt;- lm(score1~score2 + sex, data = schools) lm_m2 &lt;- lm(score1~score2*sex, data = schools) lm_m3 &lt;- lm(score1~score2:sex, data = schools) rethinking::AIC(lm_m1, lm_m2, lm_m3) #&gt; df AIC #&gt; lm_m1 4 11771 #&gt; lm_m2 5 11768 #&gt; lm_m3 4 11785 tidy(lm_m1) #&gt; # A tibble: 3 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 19.0 1.36 14.1 2.73e- 42 #&gt; 2 score2 0.422 0.0183 23.1 2.08e-101 #&gt; 3 sex1 -5.98 0.611 -9.79 5.54e- 22 tidy(lm_m2) #&gt; # A tibble: 4 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 22.2 1.97 11.3 1.77e-28 #&gt; 2 score2 0.377 0.0274 13.7 1.66e-40 #&gt; 3 sex1 -11.9 2.73 -4.36 1.40e- 5 #&gt; 4 score2:sex1 0.0818 0.0368 2.23 2.62e- 2 score1~score2 + sex + score2:sex score2 &lt;- 50 sex &lt;- 1 a &lt;- 22.22521711 b1 &lt;- 0.37664046 b2 &lt;- -11.90657014 b3 &lt;- 0.08180509 a + b1*score2 + b2*sex + b3*score2*sex #&gt; [1] 33.2 score2 &lt;- 50 sex &lt;- 0 a &lt;- 22.22521711 b1 &lt;- 0.37664046 b2 &lt;- -11.90657014 b3 &lt;- 0.08180509 a + b1*score2 + b2*sex + b3*score2*sex #&gt; [1] 41.1 a + b1*score2 #&gt; [1] 41.1 tidy(lm_m3) #&gt; # A tibble: 3 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 16.1 1.37 11.7 2.72e- 30 #&gt; 2 score2:sex0 0.460 0.0197 23.4 2.62e-103 #&gt; 3 score2:sex1 0.386 0.0180 21.4 5.80e- 89 score1~ a + b1* score2_fem + b2* score2_male Homework: please get from the fitted lm_m1 the predictions for the score 1 for boys and girls, if score 2 is 50. ennustame keskmist skoori nii üle kõikide koolide kui ka kooli tasemel eraldi poistele ja tüdrukutele. get_prior(score1~sex + (sex | school), data= schools) #&gt; prior class coef group resp dpar nlpar bound #&gt; 1 b #&gt; 2 b sex1 #&gt; 3 lkj(1) cor #&gt; 4 cor school #&gt; 5 student_t(3, 46, 13) Intercept #&gt; 6 student_t(3, 0, 13) sd #&gt; 7 sd school #&gt; 8 sd Intercept school #&gt; 9 sd sex1 school #&gt; 10 student_t(3, 0, 13) sigma prior &lt;- c(prior(normal(50, 30), class=&quot;b&quot;), prior(normal(0, 20), class =&quot;b&quot;, coef = &quot;sex1&quot;), prior(lkj(3), class = &quot;cor&quot;)) sch_m1 &lt;- brm(score1~sex + (sex | school), data= schools, prior = prior, chains = 3, cores = 3) write_rds(sch_m1, path = &quot;sch_m1.fit&quot;) sch_m1 &lt;- read_rds(&quot;sch_m1.fit&quot;) b_Intercept annab ennustuse sex = 0 keskmisele testitulemusele üle kõikide koolide ja b_sex1 annab sex = 1 tulemuse. Kooli tasemele minnes tuleb teha tehted: b_Intercept + r_school[xxx,Intercept] ja b_sex1 + r_school[xxx,sex1], et saada sellele koolile ennustatud poiste ja tüdrukute skoor. broom::tidyMCMC(sch_m1$fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;, rhat = TRUE) #&gt; # A tibble: 152 x 6 #&gt; term estimate std.error conf.low conf.high rhat #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 b_Intercept 49.2 0.993 47.3 51.1 1.00 #&gt; 2 b_sex1 -2.50 0.663 -3.75 -1.16 1.000 #&gt; 3 sd_school__Intercept 7.24 0.798 5.71 8.75 1.00 #&gt; 4 sd_school__sex1 1.53 1.01 0.00555 3.36 1.00 #&gt; 5 cor_school__Intercept__sex1 -0.0924 0.315 -0.651 0.570 1.000 #&gt; 6 sigma 11.2 0.208 10.7 11.6 0.999 #&gt; # ... with 146 more rows Kui suure tõenäosusega on sex1 koef suurem kui -2.3? sch_df &lt;- as.data.frame(sch_m1$fit) mean(sch_df$b_sex1 &gt; -2.3) #&gt; [1] 0.38 Vastus - 38% tõenäosusega. Posteerior kahe kooli keskmise tulemuse erinevusele “22520”, “30474” a &lt;- ((sch_df$`r_school[22520,Intercept]` - sch_df$`r_school[22520,sex1]`)/2) - ((sch_df$`r_school[30474,Intercept]` - sch_df$`r_school[30474,sex1]`)/2) ggplot(data=NULL, aes(a)) + geom_density() 90% CI rethinking::HPDI(a, prob = 0.9) #&gt; |0.9 0.9| #&gt; -14.68 -7.12 Juhhei! keskmine tulemus erineb kuskil 7 - 15 punkti võrra ja posteerior ei kata üldse nulli. mean(sch_df$b_sex1 &gt; 0) #&gt; [1] 0 plot(sch_m1, pars = &quot;b_&quot;) stanplot(sch_m1, pars = &quot;cept]$&quot;) bayes_R2(sch_m1) #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; R2 0.316 0.018 0.28 0.35 pp_check(sch_m1) poiste ja tüdrukute ennustatud keskmine tulemus 90% CI-ga (0 - poiss, 1 - tüdruk) marginal_effects(sch_m1, method = &quot;fitted&quot;, probs=c(0.1, 0.9)) siin ennustab mudel kooli kaupa, kuhu piirkonda võiksid tulla just selle kooli tulemused kordustestis. Näidatud on ka algse valimi andmepunktid. NB! kui minna mudeli alumisele tasemele, siis tuleb sisse panna argument re_formula = NULL (muidu ei arvesta ennustus mudeli alumise taseme koefitsiente) conditions &lt;- data.frame(school =c(&quot;20920&quot;, &quot;22520&quot;, &quot;30474&quot;)) plot(marginal_effects(sch_m1, method = &quot;predict&quot;, re_formula = NULL, conditions = conditions, probs=c(0.1, 0.9)), points = TRUE, theme = theme_bw()) ja kooli tasemel ennustused keskmisele skoorile. Kool 2, kus on rohkem andmeid kui kool 1-l, saab ka kitsamad usalduspiirid. plot(marginal_effects(sch_m1, method = &quot;fitted&quot;, conditions = conditions, re_formula = NULL, probs=c(0.1, 0.9)), theme = theme_bw()) "]
]
