
```{r}
library(tidyverse)
library(mice)
```


# Puuduvad andmed

Järgnev koodirida tagab, et lm() funktsioon teeb automaatselt drop_na():
`options(na.action = na.omit)`  

`(na.action(fit))` annab lm() mudeli objekti pealt tabelist välja visatud ridade numbrid

`naprint(na.action(fit))` annab selliste ridade arvu

`colSums(is.na(airquality))` annab veeru kaupa NA-de arvu

Andmete puudumise mehhanismid:

1. missing completely at random (MCAR) - sisuliselt on NA-d jaotunud juhuvalimina. NA-dega ridade eemaldamine on õigustatud ainult selles kategoorias, igal pool mujal kallutab see pahasti teie poolt arvutatud statistikute väärtusi. 

2. missing at random (MAR) - Andmed on jagatavad gruppidesse ja iga grupi sees on NA-d juhuvalimina. Kui meil on NA-d muutujas A, siis andmete puudumine ei sõltu A väärtusest, aga ta võib sõltuda muutujate B, C, jne väärtustest (see ei tähenda, et me tingimata teaksime muutujate B, C jne olemasolust). Kaasaegsed imputatsioonimeetodid kasutavad enamasti MAR-eeldust. 

3. missing not at random (MNAR) - NA-d muutujas A sõltuvad A väärtusest. Üldiset, kui andmed ei ole MCAR ega MAR, siis me eeldame, et nad on MNAR. MNAR imputatsioon vajab eraldi mudelit, mis arvestaks NA-sid genereeriva mehhanismiga --- see on raske asi.

Seda, millisesse kategooriasse mingi andmeset kuulub, ei saa üldjuhus öelda andmete endi põhjal. Selleks tuleb tunda andmeid genreerivat mehhanismi. Seega on oma andmete paigutamine ühte kolmest kategooriast sageli üsna ebakindel.

## ühekaupa imputatsiooni meetodid

Siin käsitleme meetodeid, mis annavad tulemuseks ühe imputeeritud tabeli, kus iga NA asemel on üks ja ainult üks number. Neid meetodeid on lihtne mõista, aga nende kasutamist tuleks niipalju vältida, kui võimalik. 

### NA-dega ridade eemaldamine (drop_na)

Pealiskaudsel vaatlusel võib tunduda, et see on üks hea ja mittekallutatud meetod, eriti siis kui meil on andmetes vaid väike NA-de osakaal, aga tegelikult ei ole see nii.
NA-dega ridade eemaldamist (drop_na) tuleks kasutada kitsalt, vaid kolmel erijuhul: 

1. MCAR juhul, ei kalluta see keskmist, regressioonikoefitsiente ega korrelatsiooni, ehkki ülehindab SEM-i. 

2. Kui meil on NA-d ainult Y-muutujas, siis on NA-dega ridade eemaldamine sama hea meetod kui mitmene imputatsioon ka MAR juhul (eeldades, et mitmene imputatsioon kasutab samu x-muutujaid, mis imputeeritud andmetga regressioongi).

3. NA-dega ridade väljaviskamine on eelistatud meetod logistiline analüüsimudeli korral, juhul kui NA-d esinevad ainult dihhotoomses Y või dihhotoomses X muutujas (mitte mõlemas) ja tõenäosus kohata NA-d sõltub ainult Y-väärtusest (mitte X-st).

Siiski, kui meil on mitmene regressioon ja igas X-muutujas on ka väike osa NA-sid, siis nende kombinatsioonid tagavad ikkagi paljude ridade eemaldamise ja seega kalli informatsiooni ära viskamise.

### Paariviisiline deleteerimine

Kui muutujad on multivariaatselt normaalsed, nende vahel ei ole korrelatsioone ja meil on MCAR olukord, siis on hea kasutada *pairwise deletion* e *available case* meetodit, mis arvutab igale muutujale olemasolevate andmete pealt keskmise ja sem-i ja kasutab korrelatsioonide jms jaoks kõiki ridu, kus andmepaarid on täielikud. Siinkohal tuleb küll mainida, et sellist olukorda praktikas naljalt ette ei tule. 

Regressioonil tuleb meil lm() asemel kasutada lavaan raamatukogu. 

```{r eval=FALSE}
data <- airquality[, c("Ozone", "Solar.R", "Wind")]
mu <- colMeans(data, na.rm = TRUE)
cv <- cov(data, use = "pairwise")

library(lavaan)
fit <- lavaan("Ozone ~ 1 + Wind + Solar.R
              Ozone ~~ Ozone",
             sample.mean = mu, sample.cov = cv,
             sample.nobs = sum(complete.cases(data)))
```

### Keskmise imputatsioon ja regressiooniga imputatsioon

Kasutame mice raamatukogu.

```{r eval=FALSE}
imp <- mice(airquality, method = "mean", m = 1, maxit = 1)
```

See on kindel viis varieeruvust alahinnata ja tekitada kummalisi andmejaotusi. Kui meil on MCAR andmed, siis tuleb vähemalt keskmise hinnang nihketa, aga pea kõik teised statistikud lähevad puusse. Seda meetodit tuleks vältida.

Regressiooniga imputatsioonil fitime mudeli olemasolevatel andmetel ja seejärel ennustame seda kasutades puuduvad andmepunktid

```{r eval=FALSE}
fit <- lm(Ozone ~ Solar.R, data = airquality)
pred <- predict(fit, newdata = ic(airquality))
```

See on jällegi hea viis, kuidas kunstlikult andmete varieervust vähendada ja nende jaotust muuta. Ühtlasi tõstab see meetod kunstlikult muutujate vahelist korrelastsiooni. Enam halvemaks ei saa minna!

### Stohhastiline regressiooniga imputatsioon

Lisab eelnevasse müra ja töötab ka MAR andmete peal. Kõigepealt fitib lm()-ga mudeli koefitsiendid, siis ennustab nende pealt puuduvad väärtused ja lõpuks liidab igale neist juhusliku residuaali algsest mudelist.

```{r eval=FALSE}
data <- airquality[, c("Ozone", "Solar.R")]
imp <- mice(data, method = "norm.nob", m = 1, maxit = 1,
            seed = 1, print = FALSE)
```

Eeldus, mis kunagi ei kehti, on et mudeli koefitsiendid vastavad tegelikkusele. Seega puudub siin koefitsientide määramise ebakindlus (seda saab arvutada kas Bayesi meetoditega tõmmates parameetriväärtuste ebakindluse otse posteerioritest, või bootstrappides).

Stohhastiline regressiooniga imputatsioon võib anda negatiivseid väärtusi muidu positiivsetesse muutujatesse, ekstreemsed väärtused pole hästi mudeliga kaetud ja ennustused/imputatsioonid vastavad regressioonieeldustele (homoskedastilisus jne) ka siis, kui andmed nendele ei vasta. Siiski, kuna see pigem algeline meetod säilitab korrelatsioonid muutujate vahel ja ei kalluta regressioonikoefitsiente, ei saa see liiga halb olla. Sellelt põhjalt on ehitatud paljud kaasaegsed imputatsioonomeetodid.

### Eelmise vaatluse kopeerimine (LOCF).

See on *ad hoc* meetod aegridadele, mida rakendab `tydyr::fill()`. Seda on hea kasutada siis, kui muutuja väärtus kirjutatakse üles ainult siis, kui see muutub. Muidu, LOCF võib olla kallutatud isegi MCAR juhul.  

## Mitmene imputatsioon

See meetod loodi Donald Rubini poolt 1970ndatel ja üldskeem on järgmine:

1. Tekita mitu imputeeritud andmeraami (igaüks on ilma NA-deta), mis erinevad ainult imputeeritud väärtuste poolest. Iga NA asemele imputeeritakse arvud omast jaotusest, mille laius on kommentaar meie ebakindlusele selle konkreetse imputatsiooni kohta. 

2. Analüüsi igat andmeraami eraldi, aga tavapärasel viisil

3. kombineeri tulemused lõplikuks keskmiseks ja sem-iks.

Näide: mitmene imputatsioon lm()-ga ja CI-d koefitsientidele

```{r eval=FALSE}
imp <- mice(nhanes, print = FALSE, m = 10, seed = 24415)
fit <- with(imp, lm(bmi ~ age))
est <- pool(fit)
summary(est, conf.int = TRUE)
```

m=10 loob 10 iseseisvat andmeraami. 

> Kaasaegne soovitus on, kui võimalik, sättida m kuhugi 20-100 vahele.

Kogu järgnev imputatsioonijutt käib vaikimisi mitmese imputatsiooni kohta.

Imputatsiooni mudel peaks

* arvestama protsessiga, mis genereeris NAd (NMAR juhul) 

* säilitama andmetes leiduvad omavahelised suhted 

* säilitama nende suhete ümber laiuva ebakindluse määra 

Selle nimel peame tegema 7 valikut selles järjekorras:

1. **Kas me loeme MAR eelduse kehtivaks?** MNAR imputatsioon toob sisse lisaeeldused. MNAR juhul on kaks võimalust: toome sisse uued muutujad, mis enustaks NAde paiknemist lootuses, et läheneme MAR-olukorrale, või kasutame lisamudelit, mis arvestab protsessiga, mis genereeris NAd.  

2. **Milline imputatsioonimudel (nii mudeli struktuurne osa kui eeldatud vigade jaotus)?** *Fully conditional specification* (FCS) imputeerib multivariiatseid NA-sid muutuja haaval. Seega peab mudeli kuju spetsifitseerima igale NA-dega muutujale eraldi, arvestades ka muutujate vahelisi seoseid. 

```{r echo=FALSE}
 tribble(~Method, ~Description, ~`Scale Type`,
        "pmm", "Predictive mean matching", "Any*",
        "midastouch", "Weighted predictive mean matching", "Any",
        "sample", "Random sample from observed values", "Any",
        "cart", "Classification and regression trees", "Any",
        "rf", "Random forest imputation", "Any",
        "norm", "Bayesian linear regression", "numeric",
        "norm.boot", "Normal imputation with bootstrap", "Numeric",
        "quadratic", "Imputation of quadratic terms", "Numeric",
        "ri", "Random indicator for nonignorable data", "Numeric",
        "logreg", "Logistic regression", "Binary*",
        "logreg.boot", "Logistic regression with bootstrap", "Binary",
        "polr", "Proportional odds model", "ordinal*",
        "polyreg", "Polytomous logistic regression", "Nominal*") %>% knitr::kable()
```

* - vaikemudel antud andmetüübile

Normaalsetele muutujatele on norm efektiivsem, kui pmm. norm.boot on kiirem mitte-bayesiaanlik alternatiiv. Kui pmm töötab halvasti (näit ei ole piisavalt ennustuse lähedasi doonorarve), kasuta norm meetodeid. Harvade kategooriliste andmepunktide korral eelista pmm-i, pigem kui logreg, polr või polyreg. sample on kiirmeetod imputeerimiseks ilma kovariaatideta. 

3. **Milliseid muutujaid kasutada imputatsioonimudelis prediktoritena?** Üldine soovitus on lisada kõik relevantsed muutujad ja nende relevantsed interaktsioonid. 

predictorMatrix argument on maatriks suurusega ncol(data), mis sisaldab 0/1. Iga rida ütleb, milliseid prediktoreid kasutatkse muutujale, mille nimi on reanimes. 

```{r}
imp <- mice(nhanes, print = FALSE)
imp$predictorMatrix
```
Kõikide muutujate kasutamine on ok, kui meil on < 20-30 muutuja, mille vahel ei ole keerulisi interakstsioone. Rohkem muutujaid muudab ka MAR eelduse täitmise tõenäolisemaks. Muidu võta sisse (i) kogu hiljem kasutatava regressioonimudeli, (2) lisa muutujad, mis võiksid ennustada NA-de teket (mille jaotused erinevad andmete ja NA-de korral imputeeritavas muutujas), (3) lisa muutujad, mis on korreleeritud imputeeritava muutujaga. quickpred() annab kiire viisi prediktormaatriksi defineerimiseks. 

4. **Kas imputeerida muutujaid, mis on teiste NA-dega muutujate funktsioonid (suhted jms)?** Lihtsaim vastus on "ei". 
```{r}
data <- boys[, c("age", "hgt", "wgt", "hc", "reg")]
imp <- mice(data, print = FALSE, seed = 71712)
long <- mice::complete(imp, "long", include = TRUE)
long$whr <- with(long, 100 * wgt / hgt)
imp.itt <- as.mids(long)
```
See ei ole täiuslik meetod, aga suhetega töötamine teistel meetoditel ei ole lihtne.

5. **Mis järjekorras muutujaid imputeerida?** 

6. **Mitu iteratsiooni teha?**  

7. **Mitu imputatsiooni teha?** Liiga suur m viib suurele simulatsiooniveale ja statistilisele ebaefektiivsusele  


## Predictive mean matching

See on enimkasutatud mitmese imputatsiooni meetod:

1. ennusta NA asemele parim oodatud väärtus regressioonimudelist

2. leia mingi arv (3, 5 v 10) oodatud väärtusega sarnaseid kandidaatväärtusi (ehk doonorväärtusi d=) olemasolevatest andmetest

3. võta nende seast üks juhuslik väärtus ja imputeeri see.

See põhineb eeldusel, et kandidaatväärtused on sama jaotusega, kui puuduvad väärtused. Meetod on robustne andmete tranformatsioonidele ja töötab üldiselt hästi pidevatel muutujatel, juhul kui valim on suur. Kui N>100, kasuta d=5, väikeste valimite korral aga d=1. Kui N on ligikaudu 10, siis on eelis adaptatiivsel meetodil `midastouch`.

*Predictive mean matching* d=5 on mice vaikemudel pidevatele muutujatele.
Vaikemudel on lineaarne additiivne mudel üle kõikide prediktorite. Kui päris regressioonimudel on vaikemudelist erinev (interaktsioonid!), siis tuleks seda kasutada ka imputatsioonil. Meetod võib töötada halvasti väikesel valimitel, seda ei saa kasutada andmetest välja ekstrapoleerimiseks ja ka andmete sees, juhul kui andmete tihedus on madal.

## Imputeerimine mitte-normaalsete jaotuste korral

Kui n > 400, pole enamasti probleemi imputeeritud andmete pealt keskmise arvutamisega (isegi kui muutujas on 75% NA-sid), kuid varieeruvusnäitajad võivad siiski olla problemaatilised. Üks võimalus on transformeerida andmed nii, et nad oleksid normaalsed, siis imputeerida ja seejärel andmed uuesti tagasi transformeerida. Siiski ei ole garantiid, et selline trikitamine asja hoopis hullemaks ei tee. Teine võimalus on imputeerida, kastudades mitte-normaalseid jaotusi. Näiteks saab kasutada studenti t jaotust, et saada imputatsioonil robustsemaid tulemusi, juhul kui andmetes on outlierid, mis kajastavad tegelikke olusid.

```{r eval=FALSE}
library("ImputeRobust")
library("gamlss")
imp <- mice(data2, m = 1, meth = "gamlssTF", print = FALSE)
```

## Kategoorilised muutujad

Binaarse muutuja korral kasutame logistilist mudelit, rohkem kui kahe tasemega mittejärjestatud juhul multinominaalset logistilist mudelit ja järjestatud tasemetega juhul *ordered logit* mudelit või *proportional odds* mudelit. 

Kategooriliste muutujate imputeerimine on raske. Logistilises regressioonis vajame me reeglina vähemalt 10 sündmust prediktori kohta. Seega, kui me imputeerime 10 binaarset NAd, vajame me 100 sündmust ja kui sündmuse tõenäosus on 0.1, siis üle 1000 katse. Kui muutuja tasemete arv kasvab, siis need numbrid kasvavad väga kiiresti. 

Vaikimisi töötavad logreg, polyreg ja polr on tavaliselt ok. Kui suhe sündmuste arv/fititud parameetrite arv langeb alla 10, siis võiks kasutada robustsemaid meetodeid: pmm, cart või rf.

## countid (sisaldavad nulle) {-}

Meetodid:

1. Predictive mean matching.

2. Ordered categorical imputation.

3. (Zero-inflated) Poisson regression.

4. (Zero-inflated) negative binomial regression.

Poisson mudeldab haruldaste sündmuste summat, negatiivne binoommudel on paindlikum versioon Poissonist, mida saab kasutada siis, kui meil on andmetes üle-dispersioon (poisson eeldab kindlat dispersioonimäära). Zero-inflated versioonid mõlemast töötavad siis, kui meil on andmetes oodatust rohkem nulle. 

ImputeRobust raamatukogu kasutab mice meetodeid: `gamlssPO` (Poisson), `gamlssZIBI` (zero-inflated binomial) ja `gamlssZIP` (zero-inflated Poisson). 

## Mitme prediktoriga imputatsioon

### monotooniline muster

NA-de muster on monotooniline siis, kui muutujaid saab järjestada nõnda, et kui esimeses muutujas on NA, siis ka kõikides järgnevates muutujates on selles reas NA. (Näiteks, kui aegreas mõni subjekt välja kukub.)

Näiteks siin on meil 3 monotoonilist muutujat
```{r}
library(mice)
data <- nhanes2[, 1:3]
md.pattern(data, plot = FALSE)
```

Meil on 16 ilma NA-deta rida, 1 rida, kus puudub bmi üksi ja 8 rida, kus puuduvad bmi ja hyp. Paremas servas on veerg rea kaupa NA-de arvuga igale kombinatsioonile ja alumine rida annab NA-de arvu igale veerule ja totaalse NAde arvu (17).

```{r eval=FALSE}
imp <- mice(data, visit = "monotone", maxit = 1, m = 2,
            print = FALSE)
```

Argument visit = monotone määrab monotoonilise imputatsiooni, ehk kõigepealt imputeeritakse need muutujad, milles on vähem NA-sid (enne hyp ja siis bmi). Siin imputeeritakse hyp logistilise regressiooniga ja bmi prediktive matching-uga, mis on vaikesettingud vastavalt binaarsetele ja pidevatele muutujatele.

> Monotooniline imputatsioon eeldab, et NA-de muster on monotooniline ja et muutujad ei sõltu üksteisest (muutujate distinktsust).












